stage: 
task: Prediction
data: Model construction

prompt:
Write a Python function `get_embeddings` for the class `BayesianTabMlp`. The function should extract and concatenate embeddings for both categorical and continuous features from the input tensor `X`. The class structure is as follows:

```python
class BayesianTabMlp(BaseBayesianModel):
    def __init__(
        self,
        column_idx: Dict[str, int],
        *,
        cat_embed_input: Optional[List[Tuple[str, int, int]]] = None,
        cat_embed_activation: Optional[str] = None,
        continuous_cols: Optional[List[str]] = None,
        embed_continuous: Optional[bool] = None,
        cont_embed_dim: Optional[int] = None,
        cont_embed_dropout: Optional[float] = None,
        cont_embed_activation: Optional[str] = None,
        use_cont_bias: Optional[bool] = None,
        cont_norm_layer: Optional[Literal["batchnorm", "layernorm"]] = None,
        mlp_hidden_dims: List[int] = [200, 100],
        mlp_activation: str = "leaky_relu",
        prior_sigma_1: float = 1,
        prior_sigma_2: float = 0.002,
        prior_pi: float = 0.8,
        posterior_mu_init: float = 0.0,
        posterior_rho_init: float = -7.0,
        pred_dim=1,
    ):
        super(BayesianTabMlp, self).__init__()
        # Initialize the layers and attributes as shown above

    def _get_embeddings(self, X: Tensor) -> Tensor:
        # your code here
```

In `_get_embeddings`, if `cat_embed_input` is not `None`, extract and append categorical embeddings using `self.cat_embed`. If `continuous_cols` is not `None`, normalize the continuous features and optionally embed them using `self.cont_norm` and `self.cont_embed`, then append to the list. Finally, concatenate all tensors along the second dimension and return the result.

 ground Truth:pytorch_widedeep/bayesian_models/tabular/bayesian_mlp/bayesian_tab_mlp.py

 repo:pytorch-widedeep

 function:get_emeddings
 
 test_cases:tests/test_bayesian_models/test_bayes_model_components/test_mc_bayes_tabmlp.py
 
 class:BayesianTabMlp
