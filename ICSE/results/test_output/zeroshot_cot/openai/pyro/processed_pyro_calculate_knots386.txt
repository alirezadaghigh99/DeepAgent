output file:
processed_pyro_calculate_knots386.json
function:
_calculate_knots
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape0]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape0]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape0]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape0]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape0] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape0] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape0] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape0]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape0]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape0]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape0] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape0] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape0] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape1] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape1]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape0]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape0]'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/pyro/pyro/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/pyro/pyro
configfile: setup.cfg
plugins: typeguard-4.4.1, jaxtyping-0.2.19
collecting ... collected 54 items

../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape0] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape1] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape2] FAILED

=================================== FAILURES ===================================
________ test_conditional_compose_transform_module[0-2-2-batch_shape0] _________

batch_shape = (), input_dim = 2, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([-0.0597,  2.8833])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-2-2-batch_shape1] _________

batch_shape = (7,), input_dim = 2, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 3.5128,  2.8923],
        [-1.3505,  0.0261],
        [ 0.4328,  2.3464],
        [-1.6268,  5.5873],
        [ 1.0144,  3.8157],
        [ 1.9761, -0.2626],
        [ 0.8780, -1.4050]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-2-2-batch_shape2] _________

batch_shape = (6, 7), input_dim = 2, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[ 2.9123, -0.7835],
         [-0.5558,  3.5854],
         [ 4.2942, -3.8764],
         [ 3.4627,  0.7682],
  ...,
         [ 3.4372,  2.2615],
         [-2.1417,  0.1393],
         [ 0.6619,  1.2885],
         [ 5.9075,  2.9047]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-2-3-batch_shape0] _________

batch_shape = (), input_dim = 3, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([0.7703, 1.6659, 1.7279])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-2-3-batch_shape1] _________

batch_shape = (7,), input_dim = 3, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 2.2367, -2.2377,  1.2097],
        [ 1.5590,  2.8948, -1.8677],
        [ 4.6149,  0.1517, -0.7177],
       ....0096],
        [-1.2934,  3.1482,  1.4730],
        [ 2.1175,  3.0730,  0.5784],
        [ 5.2138,  1.0840,  1.5774]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-2-3-batch_shape2] _________

batch_shape = (6, 7), input_dim = 3, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-1.3919e+00,  2.0417e+00,  6.6637e-01],
         [ 6.3200e-01,  2.0410e+00, -5.4628e-01],
         [-7.2870e...01,  2.5238e+00],
         [-6.4661e-01,  1.1251e+00, -7.6052e-02],
         [ 1.0282e+00, -4.2935e-03,  6.3217e-01]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-2-5-batch_shape0] _________

batch_shape = (), input_dim = 5, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([-2.1918,  3.1805,  2.0902,  2.1673, -0.7671])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-2-5-batch_shape1] _________

batch_shape = (7,), input_dim = 5, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 1.8819, -2.2211,  2.1498,  0.2742,  0.8808],
        [ 1.6778,  3.5871,  0.7147,  0.2204,  4.9038],
        ...1.2904],
        [-1.8709, -2.0854,  4.3945, -1.2298,  1.1942],
        [ 1.9541,  2.9692, -1.9514,  2.1259, -0.9328]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-2-5-batch_shape2] _________

batch_shape = (6, 7), input_dim = 5, context_dim = 2, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-0.3774,  2.0865, -0.2881,  0.9407, -0.8434],
         [-0.6301,  5.5833,  0.4184, -0.4205,  0.8772],
      ...853],
         [ 2.3318,  0.4002, -1.0918,  0.5769,  5.4386],
         [ 3.0689,  0.7234, -0.2156,  1.2564,  1.6857]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-2-batch_shape0] _________

batch_shape = (), input_dim = 2, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([3.5128, 2.8923])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-2-batch_shape1] _________

batch_shape = (7,), input_dim = 2, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 1.0595,  3.8264],
        [-0.8156,  1.2849],
        [-0.0120, -0.7252],
        [ 0.7079,  2.8726],
        [ 2.5421,  1.2498],
        [ 1.4334,  3.1884],
        [-0.1521, -0.6062]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-2-batch_shape2] _________

batch_shape = (6, 7), input_dim = 2, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-0.0097, -0.3857],
         [ 1.5584,  2.0801],
         [-1.1928,  1.0388],
         [ 1.3170, -1.7806],
  ...,
         [ 0.0472, -0.4858],
         [-0.8876,  4.8732],
         [ 3.8540,  1.9258],
         [-4.6834,  0.2064]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-3-batch_shape0] _________

batch_shape = (), input_dim = 3, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([-3.7674, -2.0194,  0.2300])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-3-batch_shape1] _________

batch_shape = (7,), input_dim = 3, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 4.3802, -0.1814, -2.1359],
        [-1.8677,  4.6149,  2.1055],
        [-4.2883, -2.9810,  4.4246],
       ....3628],
        [-0.4774,  1.2455,  0.4940],
        [ 0.0548,  1.0970, -0.1385],
        [ 3.9252,  2.3032,  1.6356]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-3-batch_shape2] _________

batch_shape = (6, 7), input_dim = 3, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-1.2750,  1.4520, -0.2876],
         [-0.4729,  0.9680,  4.9818],
         [-0.4900,  2.9428,  3.3539],
    ...0],
         [ 2.2426,  3.7113,  2.7685],
         [-1.6702,  1.7489,  3.1133],
         [-2.4134, -2.3130,  3.2094]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-5-batch_shape0] _________

batch_shape = (), input_dim = 5, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([ 3.7381,  5.2565,  0.2417,  1.1361, -0.2494])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-5-batch_shape1] _________

batch_shape = (7,), input_dim = 5, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 1.5597,  2.6733,  0.2002, -3.2349,  1.0128],
        [ 1.3896, -3.1726, -0.4357,  1.7251,  2.7963],
        ...0.1925],
        [-0.3412, -0.4059,  1.0739,  2.2097,  2.0039],
        [ 0.3222,  1.0419,  1.8315, -1.7733,  1.0556]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-3-5-batch_shape2] _________

batch_shape = (6, 7), input_dim = 5, context_dim = 3, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[ 6.2545,  2.4265,  3.2120,  2.0765, -2.1689],
         [ 1.0250, -0.0851,  2.0971, -1.2509, -0.1569],
      ...648],
         [ 4.4714,  4.3984,  2.9687,  2.0300, -1.7596],
         [ 0.5692,  1.6710, -0.0169, -0.0116,  4.1145]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-2-batch_shape0] _________

batch_shape = (), input_dim = 2, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([0.7079, 2.8726])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-2-batch_shape1] _________

batch_shape = (7,), input_dim = 2, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 0.7148, -0.8715],
        [-1.7878,  1.8082],
        [ 0.3580,  4.4512],
        [-0.3196, -4.1606],
        [ 3.0989,  2.0292],
        [ 0.8924, -0.0522],
        [-1.0620, -1.1482]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-2-batch_shape2] _________

batch_shape = (6, 7), input_dim = 2, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[ 0.6139, -1.1211],
         [-0.2181,  1.7876],
         [ 1.7575,  1.3978],
         [-0.6034, -1.8955],
  ...,
         [ 2.0107,  0.3603],
         [ 1.1017,  0.3055],
         [ 0.0608,  3.9224],
         [ 2.4421, -1.6607]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-3-batch_shape0] _________

batch_shape = (), input_dim = 3, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([-0.6007, -0.1005,  2.7774])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-3-batch_shape1] _________

batch_shape = (7,), input_dim = 3, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[-0.8941,  2.1281,  0.2717],
        [ 3.8358,  2.4950, -0.5463],
        [-0.7287,  5.6590, -0.6868],
       ....2509],
        [-2.5871, -1.0162, -1.1602],
        [-1.6377, -0.2106,  2.5688],
        [ 2.5847, -0.2854,  0.7139]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-3-batch_shape2] _________

batch_shape = (6, 7), input_dim = 3, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-0.3424,  3.5294,  1.1635],
         [ 1.6511, -1.4298,  0.1330],
         [ 1.4793,  4.3570, -0.7245],
    ...6],
         [-1.2925,  0.9886, -2.0243],
         [ 1.2457, -0.2866,  3.9737],
         [ 4.8773, -0.3325,  3.6534]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-5-batch_shape0] _________

batch_shape = (), input_dim = 5, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([ 2.4987,  0.0645,  1.6466,  1.8378, -0.6068])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-5-batch_shape1] _________

batch_shape = (7,), input_dim = 5, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[-2.4729, -0.9453,  4.1726,  2.3259,  0.1925],
        [-0.3412, -0.4059,  0.7750,  2.2097,  2.0039],
        ...0.9484],
        [ 1.4364, -0.2075,  1.7087,  0.8524,  3.7891],
        [ 0.8963,  2.0353,  1.6798, -3.6889,  2.0553]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[0-5-5-batch_shape2] _________

batch_shape = (6, 7), input_dim = 5, context_dim = 5, cache_size = 0

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-2.2591,  2.8525, -0.0081,  2.9570,  2.2127],
         [-0.3372,  3.3954, -2.3350,  1.2561,  1.9545],
      ...897],
         [ 0.1906,  2.5282, -1.1486,  1.6333, -0.5956],
         [ 2.4706, -2.3448,  1.9704,  2.1091, -0.3391]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-2-batch_shape0] _________

batch_shape = (), input_dim = 2, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([-0.0597,  2.8833])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-2-batch_shape1] _________

batch_shape = (7,), input_dim = 2, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 3.5128,  2.8923],
        [-1.3505,  0.0261],
        [ 0.4328,  2.3464],
        [-1.6268,  5.5873],
        [ 1.0144,  3.8157],
        [ 1.9761, -0.2626],
        [ 0.8780, -1.4050]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-2-batch_shape2] _________

batch_shape = (6, 7), input_dim = 2, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[ 2.9123, -0.7835],
         [-0.5558,  3.5854],
         [ 4.2942, -3.8764],
         [ 3.4627,  0.7682],
  ...,
         [ 3.4372,  2.2615],
         [-2.1417,  0.1393],
         [ 0.6619,  1.2885],
         [ 5.9075,  2.9047]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-3-batch_shape0] _________

batch_shape = (), input_dim = 3, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([0.7703, 1.6659, 1.7279])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-3-batch_shape1] _________

batch_shape = (7,), input_dim = 3, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 2.2367, -2.2377,  1.2097],
        [ 1.5590,  2.8948, -1.8677],
        [ 4.6149,  0.1517, -0.7177],
       ....0096],
        [-1.2934,  3.1482,  1.4730],
        [ 2.1175,  3.0730,  0.5784],
        [ 5.2138,  1.0840,  1.5774]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-3-batch_shape2] _________

batch_shape = (6, 7), input_dim = 3, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-1.3919e+00,  2.0417e+00,  6.6637e-01],
         [ 6.3200e-01,  2.0410e+00, -5.4628e-01],
         [-7.2870e...01,  2.5238e+00],
         [-6.4661e-01,  1.1251e+00, -7.6052e-02],
         [ 1.0282e+00, -4.2935e-03,  6.3217e-01]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-5-batch_shape0] _________

batch_shape = (), input_dim = 5, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([-2.1918,  3.1805,  2.0902,  2.1673, -0.7671])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-5-batch_shape1] _________

batch_shape = (7,), input_dim = 5, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 1.8819, -2.2211,  2.1498,  0.2742,  0.8808],
        [ 1.6778,  3.5871,  0.7147,  0.2204,  4.9038],
        ...1.2904],
        [-1.8709, -2.0854,  4.3945, -1.2298,  1.1942],
        [ 1.9541,  2.9692, -1.9514,  2.1259, -0.9328]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-2-5-batch_shape2] _________

batch_shape = (6, 7), input_dim = 5, context_dim = 2, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-0.3774,  2.0865, -0.2881,  0.9407, -0.8434],
         [-0.6301,  5.5833,  0.4184, -0.4205,  0.8772],
      ...853],
         [ 2.3318,  0.4002, -1.0918,  0.5769,  5.4386],
         [ 3.0689,  0.7234, -0.2156,  1.2564,  1.6857]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-2-batch_shape0] _________

batch_shape = (), input_dim = 2, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([3.5128, 2.8923])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-2-batch_shape1] _________

batch_shape = (7,), input_dim = 2, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 1.0595,  3.8264],
        [-0.8156,  1.2849],
        [-0.0120, -0.7252],
        [ 0.7079,  2.8726],
        [ 2.5421,  1.2498],
        [ 1.4334,  3.1884],
        [-0.1521, -0.6062]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-2-batch_shape2] _________

batch_shape = (6, 7), input_dim = 2, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-0.0097, -0.3857],
         [ 1.5584,  2.0801],
         [-1.1928,  1.0388],
         [ 1.3170, -1.7806],
  ...,
         [ 0.0472, -0.4858],
         [-0.8876,  4.8732],
         [ 3.8540,  1.9258],
         [-4.6834,  0.2064]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-3-batch_shape0] _________

batch_shape = (), input_dim = 3, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([-3.7674, -2.0194,  0.2300])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-3-batch_shape1] _________

batch_shape = (7,), input_dim = 3, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 4.3802, -0.1814, -2.1359],
        [-1.8677,  4.6149,  2.1055],
        [-4.2883, -2.9810,  4.4246],
       ....3628],
        [-0.4774,  1.2455,  0.4940],
        [ 0.0548,  1.0970, -0.1385],
        [ 3.9252,  2.3032,  1.6356]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-3-batch_shape2] _________

batch_shape = (6, 7), input_dim = 3, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-1.2750,  1.4520, -0.2876],
         [-0.4729,  0.9680,  4.9818],
         [-0.4900,  2.9428,  3.3539],
    ...0],
         [ 2.2426,  3.7113,  2.7685],
         [-1.6702,  1.7489,  3.1133],
         [-2.4134, -2.3130,  3.2094]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-5-batch_shape0] _________

batch_shape = (), input_dim = 5, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([ 3.7381,  5.2565,  0.2417,  1.1361, -0.2494])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-5-batch_shape1] _________

batch_shape = (7,), input_dim = 5, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 1.5597,  2.6733,  0.2002, -3.2349,  1.0128],
        [ 1.3896, -3.1726, -0.4357,  1.7251,  2.7963],
        ...0.1925],
        [-0.3412, -0.4059,  1.0739,  2.2097,  2.0039],
        [ 0.3222,  1.0419,  1.8315, -1.7733,  1.0556]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-3-5-batch_shape2] _________

batch_shape = (6, 7), input_dim = 5, context_dim = 3, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[ 6.2545,  2.4265,  3.2120,  2.0765, -2.1689],
         [ 1.0250, -0.0851,  2.0971, -1.2509, -0.1569],
      ...648],
         [ 4.4714,  4.3984,  2.9687,  2.0300, -1.7596],
         [ 0.5692,  1.6710, -0.0169, -0.0116,  4.1145]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-2-batch_shape0] _________

batch_shape = (), input_dim = 2, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([0.7079, 2.8726])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-2-batch_shape1] _________

batch_shape = (7,), input_dim = 2, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[ 0.7148, -0.8715],
        [-1.7878,  1.8082],
        [ 0.3580,  4.4512],
        [-0.3196, -4.1606],
        [ 3.0989,  2.0292],
        [ 0.8924, -0.0522],
        [-1.0620, -1.1482]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-2-batch_shape2] _________

batch_shape = (6, 7), input_dim = 2, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[ 0.6139, -1.1211],
         [-0.2181,  1.7876],
         [ 1.7575,  1.3978],
         [-0.6034, -1.8955],
  ...,
         [ 2.0107,  0.3603],
         [ 1.1017,  0.3055],
         [ 0.0608,  3.9224],
         [ 2.4421, -1.6607]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1923, 0.2183, 0.2382, 0.1246]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0282, 0.0580, 0.4551, 0.0669, 0.0593, 0.1376, 0.0451, 0.1497],
        [0.0455, 0.1313, 0.0533, 0.0918, 0.0759, 0.3519, 0.0289, 0.2215]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[0.5508, 0.8023, 1.2893, 1.1080, 1.1237, 1.5244, 0.2930],
        [0.2414, 1.3754, 0.2246, 0.9160, 0.2588, 1.1054, 2.3186]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5632, 0.6050, 0.7121, 0.5518, 0.5119, 0.5856, 0.5168, 0.5743],
        [0.7272, 0.6983, 0.5154, 0.5754, 0.7182, 0.5854, 0.6802, 0.7307]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-3-batch_shape0] _________

batch_shape = (), input_dim = 3, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([-0.6007, -0.1005,  2.7774])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-3-batch_shape1] _________

batch_shape = (7,), input_dim = 3, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[-0.8941,  2.1281,  0.2717],
        [ 3.8358,  2.4950, -0.5463],
        [-0.7287,  5.6590, -0.6868],
       ....2509],
        [-2.5871, -1.0162, -1.1602],
        [-1.6377, -0.2106,  2.5688],
        [ 2.5847, -0.2854,  0.7139]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-3-batch_shape2] _________

batch_shape = (6, 7), input_dim = 3, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-0.3424,  3.5294,  1.1635],
         [ 1.6511, -1.4298,  0.1330],
         [ 1.4793,  4.3570, -0.7245],
    ...6],
         [-1.2925,  0.9886, -2.0243],
         [ 1.2457, -0.2866,  3.9737],
         [ 4.8773, -0.3325,  3.6534]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0250, 0.1829, 0.4864, 0.0233, 0.1...295, 0.0749],
        [0.1581, 0.1192, 0.3179, 0.0723, 0.2241, 0.0422, 0.0270, 0.0392]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.2682, 0.0837, 0.2921, 0.0262, 0.0974, 0.0268, 0.1128, 0.0927],
        [0.1219, 0.0723, 0.2217, 0.1488, 0.1...482, 0.1060],
        [0.0629, 0.0608, 0.0890, 0.0986, 0.4355, 0.0632, 0.0865, 0.1034]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.2881, 0.4464, 0.7117, 0.2421, 1.5244, 0.9449, 0.9285],
        [0.5220, 0.3923, 2.1753, 0.2701, 0.6894, 1.7585, 1.0407],
        [0.1158, 0.6537, 1.1250, 0.8523, 0.3629, 0.9043, 0.1849]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.5249, 0.6237, 0.5328, 0.5348, 0.5593, 0.5046, 0.7273, 0.5286],
        [0.6168, 0.6097, 0.6915, 0.6619, 0.6... 0.5574],
        [0.5721, 0.5627, 0.6321, 0.5635, 0.7117, 0.7117, 0.6942, 0.5332]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-5-batch_shape0] _________

batch_shape = (), input_dim = 5, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([ 2.4987,  0.0645,  1.6466,  1.8378, -0.6068])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-5-batch_shape1] _________

batch_shape = (7,), input_dim = 5, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[-2.4729, -0.9453,  4.1726,  2.3259,  0.1925],
        [-0.3412, -0.4059,  0.7750,  2.2097,  2.0039],
        ...0.9484],
        [ 1.4364, -0.2075,  1.7087,  0.8524,  3.7891],
        [ 0.8963,  2.0353,  1.6798, -3.6889,  2.0553]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
________ test_conditional_compose_transform_module[1-5-5-batch_shape2] _________

batch_shape = (6, 7), input_dim = 5, context_dim = 5, cache_size = 1

    @pytest.mark.parametrize("batch_shape", [(), (7,), (6, 7)])
    @pytest.mark.parametrize("input_dim", [2, 3, 5])
    @pytest.mark.parametrize("context_dim", [2, 3, 5])
    @pytest.mark.parametrize("cache_size", [0, 1])
    def test_conditional_compose_transform_module(
        batch_shape, input_dim, context_dim, cache_size
    ):
        conditional_transforms = [
            T.AffineTransform(1.0, 2.0),
            T.Spline(input_dim),
            T.conditional_spline(input_dim, context_dim, [5]),
            T.SoftplusTransform(),
            T.conditional_spline(input_dim, context_dim, [6]),
        ]
        cond_transform = dist.conditional.ConditionalComposeTransformModule(
            conditional_transforms, cache_size=cache_size
        )
    
        base_dist = dist.Normal(0, 1).expand(batch_shape + (input_dim,)).to_event(1)
        cond_dist = dist.ConditionalTransformedDistribution(base_dist, [cond_transform])
    
        context = torch.rand(batch_shape + (context_dim,))
        d = cond_dist.condition(context)
        transform = d.transforms[0]
        assert isinstance(transform, T.ComposeTransformModule)
    
>       data = d.rsample()

/local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py:560: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transformed_distribution.py:155: in rsample
    x = transform(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:358: in __call__
    x = part(x)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/distributions/transforms.py:162: in __call__
    y = self._call(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:147: in _call
    y, log_detJ = self.spline_op(x)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:174: in spline_op
    y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

inputs = tensor([[[-2.2591,  2.8525, -0.0081,  2.9570,  2.2127],
         [-0.3372,  3.3954, -2.3350,  1.2561,  1.9545],
      ...897],
         [ 0.1906,  2.5282, -1.1486,  1.6333, -0.5956],
         [ 2.4706, -2.3448,  1.9704,  2.1091, -0.3391]]])
widths = tensor([[0.2129, 0.1005, 0.1619, 0.1241, 0.1253, 0.1079, 0.1368, 0.0306],
        [0.0495, 0.0707, 0.0213, 0.0851, 0.1...128, 0.0927],
        [0.0159, 0.2453, 0.0770, 0.1305, 0.0599, 0.4102, 0.0418, 0.0194]],
       grad_fn=<AddBackward0>)
heights = tensor([[0.0538, 0.0225, 0.4134, 0.1013, 0.0503, 0.0036, 0.2434, 0.1116],
        [0.0635, 0.0879, 0.0693, 0.0590, 0.0...837, 0.0380],
        [0.2460, 0.0333, 0.0727, 0.2090, 0.0496, 0.2471, 0.0174, 0.1250]],
       grad_fn=<AddBackward0>)
derivatives = tensor([[1.0519, 0.7021, 1.5743, 1.1446, 1.7264, 0.4930, 0.2837],
        [0.7921, 1.1352, 0.8067, 0.7747, 0.6491, 1.5...073, 0.3112, 1.2080],
        [0.3520, 1.7020, 1.0004, 0.2856, 0.9326, 0.3965, 0.9376]],
       grad_fn=<AddBackward0>)
lambdas = tensor([[0.7076, 0.5770, 0.6900, 0.5774, 0.6630, 0.6150, 0.6643, 0.5907],
        [0.5242, 0.5072, 0.6084, 0.6028, 0.6... 0.5117],
        [0.5803, 0.7194, 0.5771, 0.6285, 0.5873, 0.5789, 0.6801, 0.5228]],
       grad_fn=<SigmoidBackward0>)
inverse = False, bound = 3.0, min_bin_width = 0.001, min_bin_height = 0.001
min_derivative = 0.001, min_lambda = 0.025, eps = 1e-06

    def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
        """
        Calculating a monotonic rational spline (linear or quadratic) or its inverse,
        plus the log(abs(detJ)) required for normalizing flows.
        NOTE: I omit the docstring with parameter descriptions for this method since it
        is not considered "public" yet!
        """
        assert bound > 0.0
        num_bins = widths.shape[-1]
        if min_bin_width * num_bins > 1.0:
            raise ValueError('Minimal bin width too large for the number of bins')
        if min_bin_height * num_bins > 1.0:
            raise ValueError('Minimal bin height too large for the number of bins')
        left, right = (-bound, bound)
        bottom, top = (-bound, bound)
        inside_interval_mask = (inputs >= left) & (inputs <= right)
        outside_interval_mask = ~inside_interval_mask
        outputs = torch.zeros_like(inputs)
        logabsdet = torch.zeros_like(inputs)
        widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
        heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
        derivatives = min_derivative + derivatives
>       widths, cumwidths = _calculate_knots(widths, left, right)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/pyro/pyro/pyro/distributions/transforms/spline.py:70: TypeError
=========================== short test summary info ============================
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape0]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape1]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape2]
============================== 54 failed in 2.59s ==============================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/pyro/pyro/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/pyro/pyro
configfile: setup.cfg
plugins: typeguard-4.4.1, jaxtyping-0.2.19
collecting ... collected 54 items

../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape2] PASSED

============================== 54 passed in 1.79s ==============================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/pyro/pyro/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/pyro/pyro
configfile: setup.cfg
plugins: typeguard-4.4.1, jaxtyping-0.2.19
collecting ... collected 54 items

../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-2-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-3-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[0-5-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-2-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-3-5-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-2-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-3-batch_shape2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape0] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape1] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/distributions/test_transforms.py::test_conditional_compose_transform_module[1-5-5-batch_shape2] PASSED

============================== 54 passed in 1.81s ==============================
