output file:
processed_pyroconvolve380.json
function:
convolve
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-3]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-3] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-5]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-5] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-10] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-10]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-6]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-4] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-2] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-4]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-2]', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-10]'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/pyro/pyro/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/pyro/pyro
configfile: setup.cfg
plugins: typeguard-4.4.1, jaxtyping-0.2.19
collecting ... collected 324 items

../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-10] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-2] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-3] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-4] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-5] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-6] FAILED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-10] PASSED

=================================== FAILURES ===================================
__________________________ test_convolve[full-()-2-3] __________________________

batch_shape = (), m = 3, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-2-4] __________________________

batch_shape = (), m = 4, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-2-5] __________________________

batch_shape = (), m = 5, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-2-6] __________________________

batch_shape = (), m = 6, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-2-10] __________________________

batch_shape = (), m = 10, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-3-2] __________________________

batch_shape = (), m = 2, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-3-4] __________________________

batch_shape = (), m = 4, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-3-5] __________________________

batch_shape = (), m = 5, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-3-6] __________________________

batch_shape = (), m = 6, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-3-10] __________________________

batch_shape = (), m = 10, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-4-2] __________________________

batch_shape = (), m = 2, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-4-3] __________________________

batch_shape = (), m = 3, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-4-5] __________________________

batch_shape = (), m = 5, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-4-6] __________________________

batch_shape = (), m = 6, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-4-10] __________________________

batch_shape = (), m = 10, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-5-2] __________________________

batch_shape = (), m = 2, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-5-3] __________________________

batch_shape = (), m = 3, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-5-4] __________________________

batch_shape = (), m = 4, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-5-6] __________________________

batch_shape = (), m = 6, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085,  0.2103]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-5-10] __________________________

batch_shape = (), m = 10, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653,  0.3528]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-6-2] __________________________

batch_shape = (), m = 2, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-6-3] __________________________

batch_shape = (), m = 3, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-6-4] __________________________

batch_shape = (), m = 4, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[full-()-6-5] __________________________

batch_shape = (), m = 5, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-6-10] __________________________

batch_shape = (), m = 10, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653,  0.3528,  0.9728]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-10-2] __________________________

batch_shape = (), m = 2, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,
         0.2103, -0.3908]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-10-3] __________________________

batch_shape = (), m = 3, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103,
        -0.3908,  0.2350]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-10-4] __________________________

batch_shape = (), m = 4, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,
         0.2350,  0.6653]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-10-5] __________________________

batch_shape = (), m = 5, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,  0.2350,
         0.6653,  0.3528]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-()-10-6] __________________________

batch_shape = (), m = 6, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,  0.2350,  0.6653,
         0.3528,  0.9728]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-2-3] _________________________

batch_shape = (4,), m = 3, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...8]]), tensor([[ 0.2350,  0.6653],
        [ 0.3528,  0.9728],
        [-0.0386, -0.8861],
        [-0.4709, -0.4269]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-2-4] _________________________

batch_shape = (4,), m = 4, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...4]]), tensor([[-0.0386, -0.8861],
        [-0.4709, -0.4269],
        [-0.0283,  1.4220],
        [-0.3886, -0.8903]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-2-5] _________________________

batch_shape = (4,), m = 5, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...1]]), tensor([[ 0.7290,  1.2775],
        [-1.0815, -1.3027],
        [ 1.0827, -1.3841],
        [ 0.4033, -1.2239]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-2-6] _________________________

batch_shape = (4,), m = 6, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...1]]), tensor([[ 1.0827, -1.3841],
        [ 0.4033, -1.2239],
        [ 0.7017,  2.2139],
        [-0.0276,  1.0541]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-2-10] _________________________

batch_shape = (4,), m = 10, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...5]]), tensor([[ 0.7885,  0.3208],
        [ 0.8456, -0.3621],
        [ 0.1027, -3.5310],
        [ 0.5485, -1.6063]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-3-2] _________________________

batch_shape = (4,), m = 2, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...2103],
        [-0.3908,  0.2350,  0.6653],
        [ 0.3528,  0.9728, -0.0386],
        [-0.8861, -0.4709, -0.4269]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-3-4] _________________________

batch_shape = (4,), m = 4, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...4709],
        [-0.4269, -0.0283,  1.4220],
        [-0.3886, -0.8903, -0.9601],
        [-0.4087,  1.0764, -0.4015]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-3-5] _________________________

batch_shape = (4,), m = 5, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0815],
        [-1.3027,  1.0827, -1.3841],
        [ 0.4033, -1.2239,  0.7017],
        [ 2.2139, -0.0276,  1.0541]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-3-6] _________________________

batch_shape = (4,), m = 6, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...4033],
        [-1.2239,  0.7017,  2.2139],
        [-0.0276,  1.0541,  0.5661],
        [-0.3820,  0.8807,  0.2710]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-3-10] _________________________

batch_shape = (4,), m = 10, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...8456],
        [-0.3621,  0.1027, -3.5310],
        [ 0.5485, -1.6063,  0.7281],
        [ 0.6609,  0.2391,  0.0340]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-4-2] _________________________

batch_shape = (4,), m = 2, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten....4622, -0.2632, -0.7370],
        [-2.4337,  0.3042, -1.2614,  0.4050],
        [ 0.6603, -0.5331, -0.6030,  0.7264]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-4-3] _________________________

batch_shape = (4,), m = 3, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ....7768,  1.2984, -0.6320],
        [ 0.6603, -0.5331, -0.6030,  0.7264],
        [-0.6457,  0.4280, -0.4844,  0.0673]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-4-5] _________________________

batch_shape = (4,), m = 5, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ....4585,  1.7166, -0.7307],
        [ 0.5772, -0.6045,  1.4284,  0.7550],
        [-1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-4-6] _________________________

batch_shape = (4,), m = 6, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,....7076,  0.7600,  0.5615],
        [-1.1005,  1.6964,  0.5282,  1.0614],
        [ 0.2744,  2.2121, -0.0944, -0.8925]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-4-10] _________________________

batch_shape = (4,), m = 10, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ....7479,  1.7876,  1.0033],
        [ 0.2395,  0.5696,  0.3293,  0.1652],
        [-1.6648,  2.2198,  0.2171,  1.3690]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-5-2] _________________________

batch_shape = (4,), m = 2, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten....0847],
        [ 0.1440, -1.1005,  1.0618, -0.6267, -1.0863],
        [-0.7031, -0.8052, -0.3629, -0.4370, -0.4687]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-5-3] _________________________

batch_shape = (4,), m = 3, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ....3409],
        [-0.0374, -1.5155, -0.8052, -0.3629, -0.4370],
        [-0.4687,  0.5772, -0.6045,  1.4284,  0.7550]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-5-4] _________________________

batch_shape = (4,), m = 4, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572....4585],
        [ 1.7166, -0.7307,  0.5772, -0.6045,  1.4284],
        [ 0.7550, -1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-5-6] _________________________

batch_shape = (4,), m = 6, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,....9216],
        [ 0.2086, -0.1922, -1.6648,  2.2198,  0.2171],
        [ 1.3690, -0.3084, -0.3429,  0.0434,  0.1462]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-5-10] _________________________

batch_shape = (4,), m = 10, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ....5779],
        [ 0.0347, -1.2997, -0.7377, -1.2602, -0.3562],
        [ 1.0706,  1.6303,  0.0573, -0.9721,  0.0475]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-6-2] _________________________

batch_shape = (4,), m = 2, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...0.1829,  1.3409, -0.0374, -1.5155, -0.8052, -0.3629],
        [-0.4370, -0.4687,  0.5772, -0.6045,  1.4284,  0.7550]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-6-3] _________________________

batch_shape = (4,), m = 3, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...1.6309,  0.4585,  1.7166, -0.7307,  0.5772, -0.6045],
        [ 1.4284,  0.7550, -1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-6-4] _________________________

batch_shape = (4,), m = 4, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...0.6114, -0.7076,  0.7600,  0.5615, -1.1005,  1.6964],
        [ 0.5282,  1.0614,  0.2744,  2.2121, -0.0944, -0.8925]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[full-(4,)-6-5] _________________________

batch_shape = (4,), m = 5, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0.3930, -0.9216,  0.2086, -0.1922, -1.6648,  2.2198],
        [ 0.2171,  1.3690, -0.3084, -0.3429,  0.0434,  0.1462]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-6-10] _________________________

batch_shape = (4,), m = 10, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...1.2775, -0.5276,  1.1303, -0.6333,  1.6303,  0.0573],
        [-0.9721,  0.0475, -0.7703, -1.5547, -2.1732, -0.4930]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-10-2] _________________________

batch_shape = (4,), m = 2, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...0.8433],
        [ 0.9058,  0.1582,  0.8185,  0.6440, -0.1228,  0.7623, -0.3727, -0.0103,
         -2.6528, -1.9059]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-10-3] _________________________

batch_shape = (4,), m = 3, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...0.6327],
        [ 2.3189,  0.9049, -0.3727, -0.0103, -2.6528, -1.9059,  0.2395,  0.5696,
          0.3293,  0.1652]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-10-4] _________________________

batch_shape = (4,), m = 4, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...2.7479],
        [ 1.7876,  1.0033,  0.2395,  0.5696,  0.3293,  0.1652, -1.6648,  2.2198,
          0.2171,  1.3690]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-10-5] _________________________

batch_shape = (4,), m = 5, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0.5779],
        [ 0.0347, -1.2997, -0.7377, -1.2602, -0.3562,  1.0706,  1.6303,  0.0573,
         -0.9721,  0.0475]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(4,)-10-6] _________________________

batch_shape = (4,), m = 6, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...0.5276],
        [ 1.1303, -0.6333,  1.6303,  0.0573, -0.9721,  0.0475, -0.7703, -1.5547,
         -2.1732, -0.4930]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-2-3] ________________________

batch_shape = (2, 3), m = 3, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...        [-1.0815, -1.3027]],

        [[ 1.0827, -1.3841],
         [ 0.4033, -1.2239],
         [ 0.7017,  2.2139]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-2-4] ________________________

batch_shape = (2, 3), m = 4, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...        [ 0.7017,  2.2139]],

        [[-0.0276,  1.0541],
         [ 0.5661, -0.3820],
         [ 0.8807,  0.2710]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-2-5] ________________________

batch_shape = (2, 3), m = 5, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...        [ 0.8807,  0.2710]],

        [[ 0.7694,  0.3453],
         [ 1.8979, -0.2357],
         [ 0.7885,  0.3208]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-2-6] ________________________

batch_shape = (2, 3), m = 6, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...        [ 0.7885,  0.3208]],

        [[ 0.8456, -0.3621],
         [ 0.1027, -3.5310],
         [ 0.5485, -1.6063]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-2-10] ________________________

batch_shape = (2, 3), m = 10, n = 2, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...        [ 0.2913, -0.5023]],

        [[-0.9306,  0.9086],
         [-0.7788, -1.4453],
         [ 0.7636, -0.2469]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-3-2] ________________________

batch_shape = (2, 3), m = 2, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...,

        [[ 1.3409, -1.0863, -0.7031],
         [-0.8052, -0.3629, -0.4370],
         [-0.4687,  0.5772, -0.6045]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-3-4] ________________________

batch_shape = (2, 3), m = 4, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...,

        [[-0.9216,  0.3293,  0.1652],
         [-1.6648,  2.2198,  0.2171],
         [ 1.3690, -0.3084, -0.3429]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-3-5] ________________________

batch_shape = (2, 3), m = 5, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...,

        [[-0.5345, -0.3084, -0.3429],
         [ 0.0434,  0.1462,  1.6398],
         [-0.3044,  0.0143,  0.1944]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-3-6] ________________________

batch_shape = (2, 3), m = 6, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...,

        [[-0.0780,  0.0143,  0.1944],
         [-1.1805,  1.0556,  0.1799],
         [-0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-3-10] ________________________

batch_shape = (2, 3), m = 10, n = 3, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...,

        [[ 0.4511, -1.0943,  0.6026],
         [ 0.5989, -1.4274, -0.6300],
         [ 0.4348, -1.0199,  0.6034]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-4-2] ________________________

batch_shape = (2, 3), m = 2, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...85,  1.7166, -0.7307],
         [ 0.5772, -0.6045,  1.4284,  0.7550],
         [-1.1005,  1.6964,  0.5282,  1.0614]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-4-3] ________________________

batch_shape = (2, 3), m = 3, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...33, -0.3930, -0.9216],
         [ 0.3293,  0.1652, -1.6648,  2.2198],
         [ 0.2171,  1.3690, -0.3084, -0.3429]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-4-5] ________________________

batch_shape = (2, 3), m = 5, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...72, -0.3975, -0.0780],
         [ 0.0143,  0.1944, -1.1805,  1.0556],
         [ 0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-4-6] ________________________

batch_shape = (2, 3), m = 6, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...79,  0.0347, -1.2997],
         [-0.7377, -1.2602, -0.3562,  1.0706],
         [ 1.6303,  0.0573, -0.9721,  0.0475]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-4-10] ________________________

batch_shape = (2, 3), m = 10, n = 4, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...36, -1.1751, -0.0094],
         [-1.0199,  0.6034, -2.1064, -0.0826],
         [ 0.7310,  0.2947, -0.8301,  0.3838]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-5-2] ________________________

batch_shape = (2, 3), m = 2, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...15],
         [ 2.0122, -0.2271,  0.5282,  1.0614,  0.2744],
         [ 2.2121, -0.0944, -0.8925,  0.8185,  0.6440]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-5-3] ________________________

batch_shape = (2, 3), m = 3, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...98],
         [-0.1966, -0.5345, -0.3084, -0.3429,  0.0434],
         [ 0.1462,  1.6398, -0.3044,  0.0143,  0.1944]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-5-4] ________________________

batch_shape = (2, 3), m = 4, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...72],
         [-0.3975, -0.0780,  0.0143,  0.1944, -1.1805],
         [ 1.0556,  0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-5-6] ________________________

batch_shape = (2, 3), m = 6, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...33],
         [-0.5996,  0.9252, -0.9721,  0.0475, -0.7703],
         [-1.5547, -2.1732, -0.4930, -0.0340, -0.6844]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-5-10] ________________________

batch_shape = (2, 3), m = 10, n = 5, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...80],
         [ 0.6207,  0.0158, -0.8301,  0.3838, -1.6000],
         [ 0.3316, -0.0565,  0.9084,  0.7460,  0.2136]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-6-2] ________________________

batch_shape = (2, 3), m = 2, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...0108,  0.8433,  0.9058,  0.1582,  0.8185,  0.6440],
         [-0.1228,  0.7623, -0.3727, -0.0103, -2.6528, -1.9059]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-6-3] ________________________

batch_shape = (2, 3), m = 3, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...1843, -0.9072, -0.3975, -0.0780,  0.0143,  0.1944],
         [-1.1805,  1.0556,  0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-6-4] ________________________

batch_shape = (2, 3), m = 4, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...9640, -0.5779,  0.0347, -1.2997, -0.7377, -1.2602],
         [-0.3562,  1.0706,  1.6303,  0.0573, -0.9721,  0.0475]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[full-(2, 3)-6-5] ________________________

batch_shape = (2, 3), m = 5, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...1303, -0.6333, -0.5996,  0.9252, -0.9721,  0.0475],
         [-0.7703, -1.5547, -2.1732, -0.4930, -0.0340, -0.6844]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-6-10] ________________________

batch_shape = (2, 3), m = 10, n = 6, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...5293, -0.4531, -1.1189,  0.1871,  0.7460,  0.2136],
         [ 0.1550, -0.0921,  1.3397,  0.0687,  0.3938, -0.4968]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-10-2] ________________________

batch_shape = (2, 3), m = 2, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...780],
         [ 1.1016, -0.1718, -1.1805,  1.0556,  0.1799, -0.4323, -0.7377,
          -1.2602, -0.3562,  1.0706]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-10-3] ________________________

batch_shape = (2, 3), m = 3, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...087],
         [ 1.8196,  0.4511, -1.0943,  0.6026,  0.5989, -1.4274, -0.6300,
           0.4348, -1.0199,  0.6034]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-10-4] ________________________

batch_shape = (2, 3), m = 4, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...536],
         [-1.1751, -0.0094, -1.0199,  0.6034, -2.1064, -0.0826,  0.7310,
           0.2947, -0.8301,  0.3838]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-10-5] ________________________

batch_shape = (2, 3), m = 5, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...380],
         [ 0.6207,  0.0158, -0.8301,  0.3838, -1.6000,  0.3316, -0.0565,
           0.9084,  0.7460,  0.2136]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[full-(2, 3)-10-6] ________________________

batch_shape = (2, 3), m = 6, n = 10, mode = 'full'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...531],
         [-1.1189,  0.1871,  0.7460,  0.2136,  0.1550, -0.0921,  1.3397,
           0.0687,  0.3938, -0.4968]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-2-3] __________________________

batch_shape = (), m = 3, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-2-4] __________________________

batch_shape = (), m = 4, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-2-5] __________________________

batch_shape = (), m = 5, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-2-6] __________________________

batch_shape = (), m = 6, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-2-10] _________________________

batch_shape = (), m = 10, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-3-2] __________________________

batch_shape = (), m = 2, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-3-4] __________________________

batch_shape = (), m = 4, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-3-5] __________________________

batch_shape = (), m = 5, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-3-6] __________________________

batch_shape = (), m = 6, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-3-10] _________________________

batch_shape = (), m = 10, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-4-2] __________________________

batch_shape = (), m = 2, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-4-3] __________________________

batch_shape = (), m = 3, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-4-5] __________________________

batch_shape = (), m = 5, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-4-6] __________________________

batch_shape = (), m = 6, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-4-10] _________________________

batch_shape = (), m = 10, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-5-2] __________________________

batch_shape = (), m = 2, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-5-3] __________________________

batch_shape = (), m = 3, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-5-4] __________________________

batch_shape = (), m = 4, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-5-6] __________________________

batch_shape = (), m = 6, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085,  0.2103]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-5-10] _________________________

batch_shape = (), m = 10, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653,  0.3528]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-6-2] __________________________

batch_shape = (), m = 2, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-6-3] __________________________

batch_shape = (), m = 3, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-6-4] __________________________

batch_shape = (), m = 4, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-6-5] __________________________

batch_shape = (), m = 5, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-6-10] _________________________

batch_shape = (), m = 10, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653,  0.3528,  0.9728]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-10-2] _________________________

batch_shape = (), m = 2, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,
         0.2103, -0.3908]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-10-3] _________________________

batch_shape = (), m = 3, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103,
        -0.3908,  0.2350]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-10-4] _________________________

batch_shape = (), m = 4, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,
         0.2350,  0.6653]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-10-5] _________________________

batch_shape = (), m = 5, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,  0.2350,
         0.6653,  0.3528]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[valid-()-10-6] _________________________

batch_shape = (), m = 6, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,  0.2350,  0.6653,
         0.3528,  0.9728]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-2-3] _________________________

batch_shape = (4,), m = 3, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...8]]), tensor([[ 0.2350,  0.6653],
        [ 0.3528,  0.9728],
        [-0.0386, -0.8861],
        [-0.4709, -0.4269]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-2-4] _________________________

batch_shape = (4,), m = 4, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...4]]), tensor([[-0.0386, -0.8861],
        [-0.4709, -0.4269],
        [-0.0283,  1.4220],
        [-0.3886, -0.8903]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-2-5] _________________________

batch_shape = (4,), m = 5, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...1]]), tensor([[ 0.7290,  1.2775],
        [-1.0815, -1.3027],
        [ 1.0827, -1.3841],
        [ 0.4033, -1.2239]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-2-6] _________________________

batch_shape = (4,), m = 6, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...1]]), tensor([[ 1.0827, -1.3841],
        [ 0.4033, -1.2239],
        [ 0.7017,  2.2139],
        [-0.0276,  1.0541]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-2-10] ________________________

batch_shape = (4,), m = 10, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...5]]), tensor([[ 0.7885,  0.3208],
        [ 0.8456, -0.3621],
        [ 0.1027, -3.5310],
        [ 0.5485, -1.6063]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-3-2] _________________________

batch_shape = (4,), m = 2, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...2103],
        [-0.3908,  0.2350,  0.6653],
        [ 0.3528,  0.9728, -0.0386],
        [-0.8861, -0.4709, -0.4269]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-3-4] _________________________

batch_shape = (4,), m = 4, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...4709],
        [-0.4269, -0.0283,  1.4220],
        [-0.3886, -0.8903, -0.9601],
        [-0.4087,  1.0764, -0.4015]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-3-5] _________________________

batch_shape = (4,), m = 5, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0815],
        [-1.3027,  1.0827, -1.3841],
        [ 0.4033, -1.2239,  0.7017],
        [ 2.2139, -0.0276,  1.0541]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-3-6] _________________________

batch_shape = (4,), m = 6, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...4033],
        [-1.2239,  0.7017,  2.2139],
        [-0.0276,  1.0541,  0.5661],
        [-0.3820,  0.8807,  0.2710]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-3-10] ________________________

batch_shape = (4,), m = 10, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...8456],
        [-0.3621,  0.1027, -3.5310],
        [ 0.5485, -1.6063,  0.7281],
        [ 0.6609,  0.2391,  0.0340]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-4-2] _________________________

batch_shape = (4,), m = 2, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten....4622, -0.2632, -0.7370],
        [-2.4337,  0.3042, -1.2614,  0.4050],
        [ 0.6603, -0.5331, -0.6030,  0.7264]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-4-3] _________________________

batch_shape = (4,), m = 3, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ....7768,  1.2984, -0.6320],
        [ 0.6603, -0.5331, -0.6030,  0.7264],
        [-0.6457,  0.4280, -0.4844,  0.0673]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-4-5] _________________________

batch_shape = (4,), m = 5, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ....4585,  1.7166, -0.7307],
        [ 0.5772, -0.6045,  1.4284,  0.7550],
        [-1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-4-6] _________________________

batch_shape = (4,), m = 6, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,....7076,  0.7600,  0.5615],
        [-1.1005,  1.6964,  0.5282,  1.0614],
        [ 0.2744,  2.2121, -0.0944, -0.8925]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-4-10] ________________________

batch_shape = (4,), m = 10, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ....7479,  1.7876,  1.0033],
        [ 0.2395,  0.5696,  0.3293,  0.1652],
        [-1.6648,  2.2198,  0.2171,  1.3690]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-5-2] _________________________

batch_shape = (4,), m = 2, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten....0847],
        [ 0.1440, -1.1005,  1.0618, -0.6267, -1.0863],
        [-0.7031, -0.8052, -0.3629, -0.4370, -0.4687]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-5-3] _________________________

batch_shape = (4,), m = 3, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ....3409],
        [-0.0374, -1.5155, -0.8052, -0.3629, -0.4370],
        [-0.4687,  0.5772, -0.6045,  1.4284,  0.7550]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-5-4] _________________________

batch_shape = (4,), m = 4, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572....4585],
        [ 1.7166, -0.7307,  0.5772, -0.6045,  1.4284],
        [ 0.7550, -1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-5-6] _________________________

batch_shape = (4,), m = 6, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,....9216],
        [ 0.2086, -0.1922, -1.6648,  2.2198,  0.2171],
        [ 1.3690, -0.3084, -0.3429,  0.0434,  0.1462]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-5-10] ________________________

batch_shape = (4,), m = 10, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ....5779],
        [ 0.0347, -1.2997, -0.7377, -1.2602, -0.3562],
        [ 1.0706,  1.6303,  0.0573, -0.9721,  0.0475]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-6-2] _________________________

batch_shape = (4,), m = 2, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...0.1829,  1.3409, -0.0374, -1.5155, -0.8052, -0.3629],
        [-0.4370, -0.4687,  0.5772, -0.6045,  1.4284,  0.7550]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-6-3] _________________________

batch_shape = (4,), m = 3, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...1.6309,  0.4585,  1.7166, -0.7307,  0.5772, -0.6045],
        [ 1.4284,  0.7550, -1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-6-4] _________________________

batch_shape = (4,), m = 4, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...0.6114, -0.7076,  0.7600,  0.5615, -1.1005,  1.6964],
        [ 0.5282,  1.0614,  0.2744,  2.2121, -0.0944, -0.8925]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-6-5] _________________________

batch_shape = (4,), m = 5, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0.3930, -0.9216,  0.2086, -0.1922, -1.6648,  2.2198],
        [ 0.2171,  1.3690, -0.3084, -0.3429,  0.0434,  0.1462]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-6-10] ________________________

batch_shape = (4,), m = 10, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...1.2775, -0.5276,  1.1303, -0.6333,  1.6303,  0.0573],
        [-0.9721,  0.0475, -0.7703, -1.5547, -2.1732, -0.4930]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-10-2] ________________________

batch_shape = (4,), m = 2, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...0.8433],
        [ 0.9058,  0.1582,  0.8185,  0.6440, -0.1228,  0.7623, -0.3727, -0.0103,
         -2.6528, -1.9059]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-10-3] ________________________

batch_shape = (4,), m = 3, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...0.6327],
        [ 2.3189,  0.9049, -0.3727, -0.0103, -2.6528, -1.9059,  0.2395,  0.5696,
          0.3293,  0.1652]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-10-4] ________________________

batch_shape = (4,), m = 4, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...2.7479],
        [ 1.7876,  1.0033,  0.2395,  0.5696,  0.3293,  0.1652, -1.6648,  2.2198,
          0.2171,  1.3690]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-10-5] ________________________

batch_shape = (4,), m = 5, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0.5779],
        [ 0.0347, -1.2997, -0.7377, -1.2602, -0.3562,  1.0706,  1.6303,  0.0573,
         -0.9721,  0.0475]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[valid-(4,)-10-6] ________________________

batch_shape = (4,), m = 6, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...0.5276],
        [ 1.1303, -0.6333,  1.6303,  0.0573, -0.9721,  0.0475, -0.7703, -1.5547,
         -2.1732, -0.4930]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-2-3] ________________________

batch_shape = (2, 3), m = 3, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...        [-1.0815, -1.3027]],

        [[ 1.0827, -1.3841],
         [ 0.4033, -1.2239],
         [ 0.7017,  2.2139]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-2-4] ________________________

batch_shape = (2, 3), m = 4, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...        [ 0.7017,  2.2139]],

        [[-0.0276,  1.0541],
         [ 0.5661, -0.3820],
         [ 0.8807,  0.2710]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-2-5] ________________________

batch_shape = (2, 3), m = 5, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...        [ 0.8807,  0.2710]],

        [[ 0.7694,  0.3453],
         [ 1.8979, -0.2357],
         [ 0.7885,  0.3208]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-2-6] ________________________

batch_shape = (2, 3), m = 6, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...        [ 0.7885,  0.3208]],

        [[ 0.8456, -0.3621],
         [ 0.1027, -3.5310],
         [ 0.5485, -1.6063]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-2-10] _______________________

batch_shape = (2, 3), m = 10, n = 2, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...        [ 0.2913, -0.5023]],

        [[-0.9306,  0.9086],
         [-0.7788, -1.4453],
         [ 0.7636, -0.2469]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-3-2] ________________________

batch_shape = (2, 3), m = 2, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...,

        [[ 1.3409, -1.0863, -0.7031],
         [-0.8052, -0.3629, -0.4370],
         [-0.4687,  0.5772, -0.6045]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-3-4] ________________________

batch_shape = (2, 3), m = 4, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...,

        [[-0.9216,  0.3293,  0.1652],
         [-1.6648,  2.2198,  0.2171],
         [ 1.3690, -0.3084, -0.3429]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-3-5] ________________________

batch_shape = (2, 3), m = 5, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...,

        [[-0.5345, -0.3084, -0.3429],
         [ 0.0434,  0.1462,  1.6398],
         [-0.3044,  0.0143,  0.1944]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-3-6] ________________________

batch_shape = (2, 3), m = 6, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...,

        [[-0.0780,  0.0143,  0.1944],
         [-1.1805,  1.0556,  0.1799],
         [-0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-3-10] _______________________

batch_shape = (2, 3), m = 10, n = 3, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...,

        [[ 0.4511, -1.0943,  0.6026],
         [ 0.5989, -1.4274, -0.6300],
         [ 0.4348, -1.0199,  0.6034]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-4-2] ________________________

batch_shape = (2, 3), m = 2, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...85,  1.7166, -0.7307],
         [ 0.5772, -0.6045,  1.4284,  0.7550],
         [-1.1005,  1.6964,  0.5282,  1.0614]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-4-3] ________________________

batch_shape = (2, 3), m = 3, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...33, -0.3930, -0.9216],
         [ 0.3293,  0.1652, -1.6648,  2.2198],
         [ 0.2171,  1.3690, -0.3084, -0.3429]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-4-5] ________________________

batch_shape = (2, 3), m = 5, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...72, -0.3975, -0.0780],
         [ 0.0143,  0.1944, -1.1805,  1.0556],
         [ 0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-4-6] ________________________

batch_shape = (2, 3), m = 6, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...79,  0.0347, -1.2997],
         [-0.7377, -1.2602, -0.3562,  1.0706],
         [ 1.6303,  0.0573, -0.9721,  0.0475]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-4-10] _______________________

batch_shape = (2, 3), m = 10, n = 4, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...36, -1.1751, -0.0094],
         [-1.0199,  0.6034, -2.1064, -0.0826],
         [ 0.7310,  0.2947, -0.8301,  0.3838]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-5-2] ________________________

batch_shape = (2, 3), m = 2, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...15],
         [ 2.0122, -0.2271,  0.5282,  1.0614,  0.2744],
         [ 2.2121, -0.0944, -0.8925,  0.8185,  0.6440]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-5-3] ________________________

batch_shape = (2, 3), m = 3, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...98],
         [-0.1966, -0.5345, -0.3084, -0.3429,  0.0434],
         [ 0.1462,  1.6398, -0.3044,  0.0143,  0.1944]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-5-4] ________________________

batch_shape = (2, 3), m = 4, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...72],
         [-0.3975, -0.0780,  0.0143,  0.1944, -1.1805],
         [ 1.0556,  0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-5-6] ________________________

batch_shape = (2, 3), m = 6, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...33],
         [-0.5996,  0.9252, -0.9721,  0.0475, -0.7703],
         [-1.5547, -2.1732, -0.4930, -0.0340, -0.6844]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-5-10] _______________________

batch_shape = (2, 3), m = 10, n = 5, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...80],
         [ 0.6207,  0.0158, -0.8301,  0.3838, -1.6000],
         [ 0.3316, -0.0565,  0.9084,  0.7460,  0.2136]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-6-2] ________________________

batch_shape = (2, 3), m = 2, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...0108,  0.8433,  0.9058,  0.1582,  0.8185,  0.6440],
         [-0.1228,  0.7623, -0.3727, -0.0103, -2.6528, -1.9059]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-6-3] ________________________

batch_shape = (2, 3), m = 3, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...1843, -0.9072, -0.3975, -0.0780,  0.0143,  0.1944],
         [-1.1805,  1.0556,  0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-6-4] ________________________

batch_shape = (2, 3), m = 4, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...9640, -0.5779,  0.0347, -1.2997, -0.7377, -1.2602],
         [-0.3562,  1.0706,  1.6303,  0.0573, -0.9721,  0.0475]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-6-5] ________________________

batch_shape = (2, 3), m = 5, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...1303, -0.6333, -0.5996,  0.9252, -0.9721,  0.0475],
         [-0.7703, -1.5547, -2.1732, -0.4930, -0.0340, -0.6844]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-6-10] _______________________

batch_shape = (2, 3), m = 10, n = 6, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...5293, -0.4531, -1.1189,  0.1871,  0.7460,  0.2136],
         [ 0.1550, -0.0921,  1.3397,  0.0687,  0.3938, -0.4968]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-10-2] _______________________

batch_shape = (2, 3), m = 2, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...780],
         [ 1.1016, -0.1718, -1.1805,  1.0556,  0.1799, -0.4323, -0.7377,
          -1.2602, -0.3562,  1.0706]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-10-3] _______________________

batch_shape = (2, 3), m = 3, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...087],
         [ 1.8196,  0.4511, -1.0943,  0.6026,  0.5989, -1.4274, -0.6300,
           0.4348, -1.0199,  0.6034]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-10-4] _______________________

batch_shape = (2, 3), m = 4, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...536],
         [-1.1751, -0.0094, -1.0199,  0.6034, -2.1064, -0.0826,  0.7310,
           0.2947, -0.8301,  0.3838]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-10-5] _______________________

batch_shape = (2, 3), m = 5, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...380],
         [ 0.6207,  0.0158, -0.8301,  0.3838, -1.6000,  0.3316, -0.0565,
           0.9084,  0.7460,  0.2136]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[valid-(2, 3)-10-6] _______________________

batch_shape = (2, 3), m = 6, n = 10, mode = 'valid'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...531],
         [-1.1189,  0.1871,  0.7460,  0.2136,  0.1550, -0.0921,  1.3397,
           0.0687,  0.3938, -0.4968]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-2-3] __________________________

batch_shape = (), m = 3, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-2-4] __________________________

batch_shape = (), m = 4, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-2-5] __________________________

batch_shape = (), m = 5, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-2-6] __________________________

batch_shape = (), m = 6, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-2-10] __________________________

batch_shape = (), m = 10, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-3-2] __________________________

batch_shape = (), m = 2, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-3-4] __________________________

batch_shape = (), m = 4, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-3-5] __________________________

batch_shape = (), m = 5, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-3-6] __________________________

batch_shape = (), m = 6, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-3-10] __________________________

batch_shape = (), m = 10, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-4-2] __________________________

batch_shape = (), m = 2, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-4-3] __________________________

batch_shape = (), m = 3, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-4-5] __________________________

batch_shape = (), m = 5, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-4-6] __________________________

batch_shape = (), m = 6, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-4-10] __________________________

batch_shape = (), m = 10, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-5-2] __________________________

batch_shape = (), m = 2, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-5-3] __________________________

batch_shape = (), m = 3, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-5-4] __________________________

batch_shape = (), m = 4, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-5-6] __________________________

batch_shape = (), m = 6, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085,  0.2103]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-5-10] __________________________

batch_shape = (), m = 10, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653,  0.3528]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-6-2] __________________________

batch_shape = (), m = 2, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-6-3] __________________________

batch_shape = (), m = 3, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-6-4] __________________________

batch_shape = (), m = 4, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
__________________________ test_convolve[same-()-6-5] __________________________

batch_shape = (), m = 5, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-6-10] __________________________

batch_shape = (), m = 10, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,
         0.3239, -0.1085]), tensor([ 0.2103, -0.3908,  0.2350,  0.6653,  0.3528,  0.9728]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-10-2] __________________________

batch_shape = (), m = 2, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204]), tensor([-0.3696, -0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,
         0.2103, -0.3908]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-10-3] __________________________

batch_shape = (), m = 3, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696]), tensor([-0.2404, -1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103,
        -0.3908,  0.2350]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-10-4] __________________________

batch_shape = (), m = 4, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404]), tensor([-1.1969,  0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,
         0.2350,  0.6653]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-10-5] __________________________

batch_shape = (), m = 5, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]), tensor([ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,  0.2350,
         0.6653,  0.3528]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-()-10-6] __________________________

batch_shape = (), m = 6, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969,  0.2093]), tensor([-0.9724, -0.7550,  0.3239, -0.1085,  0.2103, -0.3908,  0.2350,  0.6653,
         0.3528,  0.9728]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 0

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-2-3] _________________________

batch_shape = (4,), m = 3, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...8]]), tensor([[ 0.2350,  0.6653],
        [ 0.3528,  0.9728],
        [-0.0386, -0.8861],
        [-0.4709, -0.4269]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-2-4] _________________________

batch_shape = (4,), m = 4, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...4]]), tensor([[-0.0386, -0.8861],
        [-0.4709, -0.4269],
        [-0.0283,  1.4220],
        [-0.3886, -0.8903]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-2-5] _________________________

batch_shape = (4,), m = 5, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...1]]), tensor([[ 0.7290,  1.2775],
        [-1.0815, -1.3027],
        [ 1.0827, -1.3841],
        [ 0.4033, -1.2239]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-2-6] _________________________

batch_shape = (4,), m = 6, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...1]]), tensor([[ 1.0827, -1.3841],
        [ 0.4033, -1.2239],
        [ 0.7017,  2.2139],
        [-0.0276,  1.0541]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-2-10] _________________________

batch_shape = (4,), m = 10, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...5]]), tensor([[ 0.7885,  0.3208],
        [ 0.8456, -0.3621],
        [ 0.1027, -3.5310],
        [ 0.5485, -1.6063]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-3-2] _________________________

batch_shape = (4,), m = 2, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...2103],
        [-0.3908,  0.2350,  0.6653],
        [ 0.3528,  0.9728, -0.0386],
        [-0.8861, -0.4709, -0.4269]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-3-4] _________________________

batch_shape = (4,), m = 4, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...4709],
        [-0.4269, -0.0283,  1.4220],
        [-0.3886, -0.8903, -0.9601],
        [-0.4087,  1.0764, -0.4015]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-3-5] _________________________

batch_shape = (4,), m = 5, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0815],
        [-1.3027,  1.0827, -1.3841],
        [ 0.4033, -1.2239,  0.7017],
        [ 2.2139, -0.0276,  1.0541]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-3-6] _________________________

batch_shape = (4,), m = 6, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...4033],
        [-1.2239,  0.7017,  2.2139],
        [-0.0276,  1.0541,  0.5661],
        [-0.3820,  0.8807,  0.2710]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-3-10] _________________________

batch_shape = (4,), m = 10, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...8456],
        [-0.3621,  0.1027, -3.5310],
        [ 0.5485, -1.6063,  0.7281],
        [ 0.6609,  0.2391,  0.0340]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-4-2] _________________________

batch_shape = (4,), m = 2, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten....4622, -0.2632, -0.7370],
        [-2.4337,  0.3042, -1.2614,  0.4050],
        [ 0.6603, -0.5331, -0.6030,  0.7264]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-4-3] _________________________

batch_shape = (4,), m = 3, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ....7768,  1.2984, -0.6320],
        [ 0.6603, -0.5331, -0.6030,  0.7264],
        [-0.6457,  0.4280, -0.4844,  0.0673]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-4-5] _________________________

batch_shape = (4,), m = 5, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ....4585,  1.7166, -0.7307],
        [ 0.5772, -0.6045,  1.4284,  0.7550],
        [-1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-4-6] _________________________

batch_shape = (4,), m = 6, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,....7076,  0.7600,  0.5615],
        [-1.1005,  1.6964,  0.5282,  1.0614],
        [ 0.2744,  2.2121, -0.0944, -0.8925]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-4-10] _________________________

batch_shape = (4,), m = 10, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ....7479,  1.7876,  1.0033],
        [ 0.2395,  0.5696,  0.3293,  0.1652],
        [-1.6648,  2.2198,  0.2171,  1.3690]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-5-2] _________________________

batch_shape = (4,), m = 2, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten....0847],
        [ 0.1440, -1.1005,  1.0618, -0.6267, -1.0863],
        [-0.7031, -0.8052, -0.3629, -0.4370, -0.4687]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-5-3] _________________________

batch_shape = (4,), m = 3, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ....3409],
        [-0.0374, -1.5155, -0.8052, -0.3629, -0.4370],
        [-0.4687,  0.5772, -0.6045,  1.4284,  0.7550]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-5-4] _________________________

batch_shape = (4,), m = 4, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572....4585],
        [ 1.7166, -0.7307,  0.5772, -0.6045,  1.4284],
        [ 0.7550, -1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-5-6] _________________________

batch_shape = (4,), m = 6, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,....9216],
        [ 0.2086, -0.1922, -1.6648,  2.2198,  0.2171],
        [ 1.3690, -0.3084, -0.3429,  0.0434,  0.1462]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-5-10] _________________________

batch_shape = (4,), m = 10, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ....5779],
        [ 0.0347, -1.2997, -0.7377, -1.2602, -0.3562],
        [ 1.0706,  1.6303,  0.0573, -0.9721,  0.0475]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-6-2] _________________________

batch_shape = (4,), m = 2, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...0.1829,  1.3409, -0.0374, -1.5155, -0.8052, -0.3629],
        [-0.4370, -0.4687,  0.5772, -0.6045,  1.4284,  0.7550]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-6-3] _________________________

batch_shape = (4,), m = 3, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...1.6309,  0.4585,  1.7166, -0.7307,  0.5772, -0.6045],
        [ 1.4284,  0.7550, -1.1005,  1.6964,  0.5282,  1.0614]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-6-4] _________________________

batch_shape = (4,), m = 4, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...0.6114, -0.7076,  0.7600,  0.5615, -1.1005,  1.6964],
        [ 0.5282,  1.0614,  0.2744,  2.2121, -0.0944, -0.8925]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_________________________ test_convolve[same-(4,)-6-5] _________________________

batch_shape = (4,), m = 5, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0.3930, -0.9216,  0.2086, -0.1922, -1.6648,  2.2198],
        [ 0.2171,  1.3690, -0.3084, -0.3429,  0.0434,  0.1462]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-6-10] _________________________

batch_shape = (4,), m = 10, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648, -1.0580,
         -0.3048,  0.0572],
        ...1.2775, -0.5276,  1.1303, -0.6333,  1.6303,  0.0573],
        [-0.9721,  0.0475, -0.7703, -1.5547, -2.1732, -0.4930]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-10-2] _________________________

batch_shape = (4,), m = 2, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204],
        [-0.3696, -0.2404],
        [-1.1969,  0.2093],
        [-0.9724, -0.7550]]), ten...0.8433],
        [ 0.9058,  0.1582,  0.8185,  0.6440, -0.1228,  0.7623, -0.3727, -0.0103,
         -2.6528, -1.9059]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-10-3] _________________________

batch_shape = (4,), m = 3, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[-0.1115,  0.1204, -0.3696],
        [-0.2404, -1.1969,  0.2093],
        [-0.9724, -0.7550,  0.3239],
      ...0.6327],
        [ 2.3189,  0.9049, -0.3727, -0.0103, -2.6528, -1.9059,  0.2395,  0.5696,
          0.3293,  0.1652]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-10-4] _________________________

batch_shape = (4,), m = 4, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665],
        [ 0.3767,  0.2252,  0.4648, -1.0580],
        [-0.3048,  0.0572...2.7479],
        [ 1.7876,  1.0033,  0.2395,  0.5696,  0.3293,  0.1652, -1.6648,  2.2198,
          0.2171,  1.3690]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-10-5] _________________________

batch_shape = (4,), m = 5, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665, -0.7536],
        [ 0.0974, -1.0334,  0.1820, -1.0745,  0.9522],
       ...0.5779],
        [ 0.0347, -1.2997, -0.7377, -1.2602, -0.3562,  1.0706,  1.6303,  0.0573,
         -0.9721,  0.0475]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(4,)-10-6] _________________________

batch_shape = (4,), m = 6, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
        [ 0.4648, -1.0580, -1.0745,  0.9522,  1.9336,...0.5276],
        [ 1.1303, -0.6333,  1.6303,  0.0573, -0.9721,  0.0475, -0.7703, -1.5547,
         -2.1732, -0.4930]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 1

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-2-3] ________________________

batch_shape = (2, 3), m = 3, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...        [-1.0815, -1.3027]],

        [[ 1.0827, -1.3841],
         [ 0.4033, -1.2239],
         [ 0.7017,  2.2139]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-2-4] ________________________

batch_shape = (2, 3), m = 4, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...        [ 0.7017,  2.2139]],

        [[-0.0276,  1.0541],
         [ 0.5661, -0.3820],
         [ 0.8807,  0.2710]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-2-5] ________________________

batch_shape = (2, 3), m = 5, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...        [ 0.8807,  0.2710]],

        [[ 0.7694,  0.3453],
         [ 1.8979, -0.2357],
         [ 0.7885,  0.3208]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-2-6] ________________________

batch_shape = (2, 3), m = 6, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...        [ 0.7885,  0.3208]],

        [[ 0.8456, -0.3621],
         [ 0.1027, -3.5310],
         [ 0.5485, -1.6063]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-2-10] ________________________

batch_shape = (2, 3), m = 10, n = 2, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...        [ 0.2913, -0.5023]],

        [[-0.9306,  0.9086],
         [-0.7788, -1.4453],
         [ 0.7636, -0.2469]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (2) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-3-2] ________________________

batch_shape = (2, 3), m = 2, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...,

        [[ 1.3409, -1.0863, -0.7031],
         [-0.8052, -0.3629, -0.4370],
         [-0.4687,  0.5772, -0.6045]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-3-4] ________________________

batch_shape = (2, 3), m = 4, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...,

        [[-0.9216,  0.3293,  0.1652],
         [-1.6648,  2.2198,  0.2171],
         [ 1.3690, -0.3084, -0.3429]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-3-5] ________________________

batch_shape = (2, 3), m = 5, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...,

        [[-0.5345, -0.3084, -0.3429],
         [ 0.0434,  0.1462,  1.6398],
         [-0.3044,  0.0143,  0.1944]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-3-6] ________________________

batch_shape = (2, 3), m = 6, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...,

        [[-0.0780,  0.0143,  0.1944],
         [-1.1805,  1.0556,  0.1799],
         [-0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-3-10] ________________________

batch_shape = (2, 3), m = 10, n = 3, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...,

        [[ 0.4511, -1.0943,  0.6026],
         [ 0.5989, -1.4274, -0.6300],
         [ 0.4348, -1.0199,  0.6034]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (3) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-4-2] ________________________

batch_shape = (2, 3), m = 2, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...85,  1.7166, -0.7307],
         [ 0.5772, -0.6045,  1.4284,  0.7550],
         [-1.1005,  1.6964,  0.5282,  1.0614]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-4-3] ________________________

batch_shape = (2, 3), m = 3, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...33, -0.3930, -0.9216],
         [ 0.3293,  0.1652, -1.6648,  2.2198],
         [ 0.2171,  1.3690, -0.3084, -0.3429]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-4-5] ________________________

batch_shape = (2, 3), m = 5, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...72, -0.3975, -0.0780],
         [ 0.0143,  0.1944, -1.1805,  1.0556],
         [ 0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-4-6] ________________________

batch_shape = (2, 3), m = 6, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...79,  0.0347, -1.2997],
         [-0.7377, -1.2602, -0.3562,  1.0706],
         [ 1.6303,  0.0573, -0.9721,  0.0475]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-4-10] ________________________

batch_shape = (2, 3), m = 10, n = 4, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...36, -1.1751, -0.0094],
         [-1.0199,  0.6034, -2.1064, -0.0826],
         [ 0.7310,  0.2947, -0.8301,  0.3838]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (4) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-5-2] ________________________

batch_shape = (2, 3), m = 2, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...15],
         [ 2.0122, -0.2271,  0.5282,  1.0614,  0.2744],
         [ 2.2121, -0.0944, -0.8925,  0.8185,  0.6440]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-5-3] ________________________

batch_shape = (2, 3), m = 3, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...98],
         [-0.1966, -0.5345, -0.3084, -0.3429,  0.0434],
         [ 0.1462,  1.6398, -0.3044,  0.0143,  0.1944]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-5-4] ________________________

batch_shape = (2, 3), m = 4, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...72],
         [-0.3975, -0.0780,  0.0143,  0.1944, -1.1805],
         [ 1.0556,  0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-5-6] ________________________

batch_shape = (2, 3), m = 6, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...33],
         [-0.5996,  0.9252, -0.9721,  0.0475, -0.7703],
         [-1.5547, -2.1732, -0.4930, -0.0340, -0.6844]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-5-10] ________________________

batch_shape = (2, 3), m = 10, n = 5, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...80],
         [ 0.6207,  0.0158, -0.8301,  0.3838, -1.6000],
         [ 0.3316, -0.0565,  0.9084,  0.7460,  0.2136]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (5) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-6-2] ________________________

batch_shape = (2, 3), m = 2, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...0108,  0.8433,  0.9058,  0.1582,  0.8185,  0.6440],
         [-0.1228,  0.7623, -0.3727, -0.0103, -2.6528, -1.9059]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-6-3] ________________________

batch_shape = (2, 3), m = 3, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...1843, -0.9072, -0.3975, -0.0780,  0.0143,  0.1944],
         [-1.1805,  1.0556,  0.1799, -0.4323, -0.7377, -1.2602]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-6-4] ________________________

batch_shape = (2, 3), m = 4, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...9640, -0.5779,  0.0347, -1.2997, -0.7377, -1.2602],
         [-0.3562,  1.0706,  1.6303,  0.0573, -0.9721,  0.0475]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
________________________ test_convolve[same-(2, 3)-6-5] ________________________

batch_shape = (2, 3), m = 5, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...1303, -0.6333, -0.5996,  0.9252, -0.9721,  0.0475],
         [-0.7703, -1.5547, -2.1732, -0.4930, -0.0340, -0.6844]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-6-10] ________________________

batch_shape = (2, 3), m = 10, n = 6, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252,  0.4648,
          -1.0580, -0.3048,  0.0572],
      ...5293, -0.4531, -1.1189,  0.1871,  0.7460,  0.2136],
         [ 0.1550, -0.0921,  1.3397,  0.0687,  0.3938, -0.4968]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (10) must match the size of tensor b (6) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-10-2] ________________________

batch_shape = (2, 3), m = 2, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[-0.1115,  0.1204],
         [-0.3696, -0.2404],
         [-1.1969,  0.2093]],

        [[-0.9724, -0.7550],...780],
         [ 1.1016, -0.1718, -1.1805,  1.0556,  0.1799, -0.4323, -0.7377,
          -1.2602, -0.3562,  1.0706]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (2) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-10-3] ________________________

batch_shape = (2, 3), m = 3, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  1.2984],
         [-0.6320, -0.7536,  0.0974],
         [-1.0334,  0.1820, -1.0745]],

 ...087],
         [ 1.8196,  0.4511, -1.0943,  0.6026,  0.5989, -1.4274, -0.6300,
           0.4348, -1.0199,  0.6034]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (3) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-10-4] ________________________

batch_shape = (2, 3), m = 4, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665],
         [ 0.3767,  0.2252,  0.4648, -1.0580],
         [-1.0745,  0.9...536],
         [-1.1751, -0.0094, -1.0199,  0.6034, -2.1064, -0.0826,  0.7310,
           0.2947, -0.8301,  0.3838]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (4) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-10-5] ________________________

batch_shape = (2, 3), m = 5, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767],
         [ 0.2252,  0.4648, -1.0580, -0.3048,  0.0572],
     ...380],
         [ 0.6207,  0.0158, -0.8301,  0.3838, -1.6000,  0.3316, -0.0565,
           0.9084,  0.7460,  0.2136]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (5) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
_______________________ test_convolve[same-(2, 3)-10-6] ________________________

batch_shape = (2, 3), m = 6, n = 10, mode = 'same'

    @pytest.mark.parametrize("m", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("n", [2, 3, 4, 5, 6, 10])
    @pytest.mark.parametrize("batch_shape", [(), (4,), (2, 3)], ids=str)
    @pytest.mark.parametrize("mode", ["full", "valid", "same"])
    def test_convolve(batch_shape, m, n, mode):
        signal = torch.randn(*batch_shape, m)
        kernel = torch.randn(*batch_shape, n)
>       actual = convolve(signal, kernel, mode)

/local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py:126: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/tensor_utils.py:183: in convolve
    return convolve(signal, kernel, mode)
/local/data0/moved_data/publishablew/pyro/pyro/pyro/ops/temp.py:21: in convolve
    signal, kernel = torch.broadcast_tensors(signal, kernel)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:75: in broadcast_tensors
    return handle_torch_function(broadcast_tensors, tensors, *tensors)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/overrides.py:1717: in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/utils/_device.py:106: in __torch_function__
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

tensors = (tensor([[[ 0.9098,  0.1538,  0.6344,  0.3665,  0.3767,  0.2252],
         [ 0.4648, -1.0580, -0.3048,  0.0572, -1.178...531],
         [-1.1189,  0.1871,  0.7460,  0.2136,  0.1550, -0.0921,  1.3397,
           0.0687,  0.3938, -0.4968]]]))

    def broadcast_tensors(*tensors):
        r"""broadcast_tensors(*tensors) -> List of Tensors
    
        Broadcasts the given tensors according to :ref:`broadcasting-semantics`.
    
        Args:
            *tensors: any number of tensors of the same type
    
        .. warning::
    
            More than one element of a broadcasted tensor may refer to a single
            memory location. As a result, in-place operations (especially ones that
            are vectorized) may result in incorrect behavior. If you need to write
            to the tensors, please clone them first.
    
        Example::
    
            >>> x = torch.arange(3).view(1, 3)
            >>> y = torch.arange(2).view(2, 1)
            >>> a, b = torch.broadcast_tensors(x, y)
            >>> a.size()
            torch.Size([2, 3])
            >>> a
            tensor([[0, 1, 2],
                    [0, 1, 2]])
        """
        # This wrapper exists to support variadic args.
        if has_torch_function(tensors):
            return handle_torch_function(broadcast_tensors, tensors, *tensors)
>       return _VF.broadcast_tensors(tensors)  # type: ignore[attr-defined]
E       RuntimeError: The size of tensor a (6) must match the size of tensor b (10) at non-singleton dimension 2

/local/data0/moved_data/publishablew/pyro/pyro/venv/lib/python3.11/site-packages/torch/functional.py:76: RuntimeError
=========================== short test summary info ============================
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-10]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-4]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-5]
FAILED ../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-6]
======================= 270 failed, 54 passed in 15.48s ========================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/pyro/pyro/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/pyro/pyro
configfile: setup.cfg
plugins: typeguard-4.4.1, jaxtyping-0.2.19
collecting ... collected 324 items

../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-10] PASSED

============================= 324 passed in 0.76s ==============================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/pyro/pyro/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/pyro/pyro
configfile: setup.cfg
plugins: typeguard-4.4.1, jaxtyping-0.2.19
collecting ... collected 324 items

../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[full-(2, 3)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[valid-(2, 3)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-()-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(4,)-10-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-2-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-3-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-4-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-5-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-6-10] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-2] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-3] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-4] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-5] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-6] PASSED
../../../../../../local/data0/moved_data/publishablew/pyro/pyro/tests/ops/test_tensor_utils.py::test_convolve[same-(2, 3)-10-10] PASSED

============================= 324 passed in 2.22s ==============================
