output file:
processed_korniafilter3d78.json
function:
filter3d
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-circular] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-reflect]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter_2batch_2ch[cpu-float32] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_gradcheck[cpu] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-2]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-replicate] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-False] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-True] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-constant]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-reflect]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_even_sized_filter[cpu-float32] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter_2batch_2ch[cpu-float32]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_noncontiguous[cpu-float32]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-True]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-replicate]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-8]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter[cpu-float32] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-circular]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-circular]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-reflect] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-constant] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-circular] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_gradcheck[cpu]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_normalized_mean_filter[cpu-float32] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-constant]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter[cpu-float32]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_normalized_mean_filter[cpu-float32]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-constant] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-replicate] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-replicate]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-False]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-6] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-8] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_even_sized_filter[cpu-float32]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-reflect] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_noncontiguous[cpu-float32] FAILED'}

All Test Cases On Generated code:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'jit', 'inductor', 'onnxrt', 'cudagraphs', 'openxla', None, 'tvm'}
model weights cached: []

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1, jaxtyping-0.2.38
collecting ... collected 22 items

../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-constant] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-reflect] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-replicate] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-circular] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-constant] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-reflect] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-replicate] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-circular] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-2] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-3] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-6] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-8] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_exception FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter[cpu-float32] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter_2batch_2ch[cpu-float32] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_normalized_mean_filter[cpu-float32] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_even_sized_filter[cpu-float32] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_noncontiguous[cpu-float32] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_gradcheck[cpu] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_module SKIPPED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-True] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-False] FAILED

=================================== FAILURES ===================================
______________ TestFilter3D.test_smoke[cpu-float32-True-constant] ______________

self = <test_filters.TestFilter3D object at 0x7c617411e590>
border_type = 'constant', normalized = True, device = device(type='cpu')
dtype = torch.float32

    @pytest.mark.parametrize("border_type", ["constant", "reflect", "replicate", "circular"])
    @pytest.mark.parametrize("normalized", [True, False])
    def test_smoke(self, border_type, normalized, device, dtype):
        if torch_version_le(1, 9, 1) and border_type == "reflect":
            pytest.skip(reason="Reflect border is not implemented for 3D on torch < 1.9.1")
    
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(1, 1, 6, 7, 8, device=device, dtype=dtype)
>       actual = filter3d(data, kernel, border_type, normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]])
kernel = tensor([[[[[[0.0569, 0.0512, 0.0097],
            [0.0678, 0.0030, 0.0553],
            [0.0079, 0.0384, 0.0604]],

  ...
           [[0.0525, 0.0499, 0.0245],
            [0.0366, 0.0317, 0.0128],
            [0.0002, 0.0530, 0.0085]]]]]])
border_type = 'constant', normalized = True

    def filter3d(input, kernel, border_type='reflect', normalized=False):
        if normalized:
            kernel = kernel / kernel.abs().sum()
        kernel_size = kernel.shape
        padding = [(k - 1) // 2 for k in kernel_size]
        padding = padding[::-1] * 2
        input_padded = F.pad(input, padding, mode=border_type)
        kernel = kernel.unsqueeze(0).unsqueeze(0)
>       kernel = kernel.expand(input.size(1), -1, -1, -1, -1)
E       RuntimeError: expand(torch.FloatTensor{[1, 1, 1, 3, 3, 3]}, size=[1, -1, -1, -1, -1]): the number of sizes provided (5) must be greater or equal to the number of dimensions in the tensor (6)

/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:17: RuntimeError
______________ TestFilter3D.test_smoke[cpu-float32-True-reflect] _______________

self = <test_filters.TestFilter3D object at 0x7c617411e3b0>
border_type = 'reflect', normalized = True, device = device(type='cpu')
dtype = torch.float32

    @pytest.mark.parametrize("border_type", ["constant", "reflect", "replicate", "circular"])
    @pytest.mark.parametrize("normalized", [True, False])
    def test_smoke(self, border_type, normalized, device, dtype):
        if torch_version_le(1, 9, 1) and border_type == "reflect":
            pytest.skip(reason="Reflect border is not implemented for 3D on torch < 1.9.1")
    
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(1, 1, 6, 7, 8, device=device, dtype=dtype)
>       actual = filter3d(data, kernel, border_type, normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'reflect', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_____________ TestFilter3D.test_smoke[cpu-float32-True-replicate] ______________

self = <test_filters.TestFilter3D object at 0x7c617411e8f0>
border_type = 'replicate', normalized = True, device = device(type='cpu')
dtype = torch.float32

    @pytest.mark.parametrize("border_type", ["constant", "reflect", "replicate", "circular"])
    @pytest.mark.parametrize("normalized", [True, False])
    def test_smoke(self, border_type, normalized, device, dtype):
        if torch_version_le(1, 9, 1) and border_type == "reflect":
            pytest.skip(reason="Reflect border is not implemented for 3D on torch < 1.9.1")
    
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(1, 1, 6, 7, 8, device=device, dtype=dtype)
>       actual = filter3d(data, kernel, border_type, normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
______________ TestFilter3D.test_smoke[cpu-float32-True-circular] ______________

self = <test_filters.TestFilter3D object at 0x7c617411e9b0>
border_type = 'circular', normalized = True, device = device(type='cpu')
dtype = torch.float32

    @pytest.mark.parametrize("border_type", ["constant", "reflect", "replicate", "circular"])
    @pytest.mark.parametrize("normalized", [True, False])
    def test_smoke(self, border_type, normalized, device, dtype):
        if torch_version_le(1, 9, 1) and border_type == "reflect":
            pytest.skip(reason="Reflect border is not implemented for 3D on torch < 1.9.1")
    
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(1, 1, 6, 7, 8, device=device, dtype=dtype)
>       actual = filter3d(data, kernel, border_type, normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'circular', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_____________ TestFilter3D.test_smoke[cpu-float32-False-constant] ______________

self = <test_filters.TestFilter3D object at 0x7c617411ea70>
border_type = 'constant', normalized = False, device = device(type='cpu')
dtype = torch.float32

    @pytest.mark.parametrize("border_type", ["constant", "reflect", "replicate", "circular"])
    @pytest.mark.parametrize("normalized", [True, False])
    def test_smoke(self, border_type, normalized, device, dtype):
        if torch_version_le(1, 9, 1) and border_type == "reflect":
            pytest.skip(reason="Reflect border is not implemented for 3D on torch < 1.9.1")
    
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(1, 1, 6, 7, 8, device=device, dtype=dtype)
>       actual = filter3d(data, kernel, border_type, normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]])
kernel = tensor([[[[[[0.5750, 0.9626, 0.1447],
            [0.9259, 0.0994, 0.2963],
            [0.4139, 0.5675, 0.4416]],

  ...
           [[0.7871, 0.0816, 0.3975],
            [0.3794, 0.5313, 0.9595],
            [0.6992, 0.6311, 0.5078]]]]]])
border_type = 'constant', normalized = False

    def filter3d(input, kernel, border_type='reflect', normalized=False):
        if normalized:
            kernel = kernel / kernel.abs().sum()
        kernel_size = kernel.shape
        padding = [(k - 1) // 2 for k in kernel_size]
        padding = padding[::-1] * 2
        input_padded = F.pad(input, padding, mode=border_type)
        kernel = kernel.unsqueeze(0).unsqueeze(0)
>       kernel = kernel.expand(input.size(1), -1, -1, -1, -1)
E       RuntimeError: expand(torch.FloatTensor{[1, 1, 1, 3, 3, 3]}, size=[1, -1, -1, -1, -1]): the number of sizes provided (5) must be greater or equal to the number of dimensions in the tensor (6)

/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:17: RuntimeError
______________ TestFilter3D.test_smoke[cpu-float32-False-reflect] ______________

self = <test_filters.TestFilter3D object at 0x7c617411eb30>
border_type = 'reflect', normalized = False, device = device(type='cpu')
dtype = torch.float32

    @pytest.mark.parametrize("border_type", ["constant", "reflect", "replicate", "circular"])
    @pytest.mark.parametrize("normalized", [True, False])
    def test_smoke(self, border_type, normalized, device, dtype):
        if torch_version_le(1, 9, 1) and border_type == "reflect":
            pytest.skip(reason="Reflect border is not implemented for 3D on torch < 1.9.1")
    
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(1, 1, 6, 7, 8, device=device, dtype=dtype)
>       actual = filter3d(data, kernel, border_type, normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'reflect', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_____________ TestFilter3D.test_smoke[cpu-float32-False-replicate] _____________

self = <test_filters.TestFilter3D object at 0x7c617411ebf0>
border_type = 'replicate', normalized = False, device = device(type='cpu')
dtype = torch.float32

    @pytest.mark.parametrize("border_type", ["constant", "reflect", "replicate", "circular"])
    @pytest.mark.parametrize("normalized", [True, False])
    def test_smoke(self, border_type, normalized, device, dtype):
        if torch_version_le(1, 9, 1) and border_type == "reflect":
            pytest.skip(reason="Reflect border is not implemented for 3D on torch < 1.9.1")
    
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(1, 1, 6, 7, 8, device=device, dtype=dtype)
>       actual = filter3d(data, kernel, border_type, normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_____________ TestFilter3D.test_smoke[cpu-float32-False-circular] ______________

self = <test_filters.TestFilter3D object at 0x7c617411ecb0>
border_type = 'circular', normalized = False, device = device(type='cpu')
dtype = torch.float32

    @pytest.mark.parametrize("border_type", ["constant", "reflect", "replicate", "circular"])
    @pytest.mark.parametrize("normalized", [True, False])
    def test_smoke(self, border_type, normalized, device, dtype):
        if torch_version_le(1, 9, 1) and border_type == "reflect":
            pytest.skip(reason="Reflect border is not implemented for 3D on torch < 1.9.1")
    
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(1, 1, 6, 7, 8, device=device, dtype=dtype)
>       actual = filter3d(data, kernel, border_type, normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ... 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'circular', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_________________ TestFilter3D.test_cardinality[cpu-float32-2] _________________

self = <test_filters.TestFilter3D object at 0x7c617411f040>, batch_size = 2
device = device(type='cpu'), dtype = torch.float32

    @pytest.mark.parametrize("batch_size", [2, 3, 6, 8])
    def test_cardinality(self, batch_size, device, dtype):
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(batch_size, 3, 6, 7, 8, device=device, dtype=dtype)
>       assert filter3d(data, kernel).shape == data.shape

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:390: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1....., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_________________ TestFilter3D.test_cardinality[cpu-float32-3] _________________

self = <test_filters.TestFilter3D object at 0x7c617411ef80>, batch_size = 3
device = device(type='cpu'), dtype = torch.float32

    @pytest.mark.parametrize("batch_size", [2, 3, 6, 8])
    def test_cardinality(self, batch_size, device, dtype):
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(batch_size, 3, 6, 7, 8, device=device, dtype=dtype)
>       assert filter3d(data, kernel).shape == data.shape

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:390: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1....., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_________________ TestFilter3D.test_cardinality[cpu-float32-6] _________________

self = <test_filters.TestFilter3D object at 0x7c617411f2e0>, batch_size = 6
device = device(type='cpu'), dtype = torch.float32

    @pytest.mark.parametrize("batch_size", [2, 3, 6, 8])
    def test_cardinality(self, batch_size, device, dtype):
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(batch_size, 3, 6, 7, 8, device=device, dtype=dtype)
>       assert filter3d(data, kernel).shape == data.shape

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:390: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1....., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_________________ TestFilter3D.test_cardinality[cpu-float32-8] _________________

self = <test_filters.TestFilter3D object at 0x7c617411f3a0>, batch_size = 8
device = device(type='cpu'), dtype = torch.float32

    @pytest.mark.parametrize("batch_size", [2, 3, 6, 8])
    def test_cardinality(self, batch_size, device, dtype):
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(batch_size, 3, 6, 7, 8, device=device, dtype=dtype)
>       assert filter3d(data, kernel).shape == data.shape

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:390: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1....., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_________________________ TestFilter3D.test_exception __________________________

self = <test_filters.TestFilter3D object at 0x7c617411f550>

    def test_exception(self):
        k = torch.ones(1, 1, 1, 1)
        data = torch.ones(1, 1, 1, 1, 1)
        with pytest.raises(TypeError) as errinfo:
            filter3d(1, k)
>       assert "Not a Tensor type." in str(errinfo)
E       assert 'Not a Tensor type.' in '<ExceptionInfo TypeError("pad(): argument \'input\' (position 1) must be Tensor, not int") tblen=4>'
E        +  where '<ExceptionInfo TypeError("pad(): argument \'input\' (position 1) must be Tensor, not int") tblen=4>' = str(<ExceptionInfo TypeError("pad(): argument 'input' (position 1) must be Tensor, not int") tblen=4>)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:397: AssertionError
__________________ TestFilter3D.test_mean_filter[cpu-float32] __________________

self = <test_filters.TestFilter3D object at 0x7c617411f850>
device = device(type='cpu'), dtype = torch.float32

    def test_mean_filter(self, device, dtype):
        kernel = torch.ones(1, 3, 3, 3, device=device, dtype=dtype)
        sample = torch.tensor(
            [
                [
                    [
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 5.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                    ]
                ]
            ],
            device=device,
            dtype=dtype,
        )
    
        expected = torch.tensor(
            [
                [
                    [
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                    ]
                ]
            ],
            device=device,
            dtype=dtype,
        )
    
>       actual = filter3d(sample, kernel)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:481: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0.,...0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
____________ TestFilter3D.test_mean_filter_2batch_2ch[cpu-float32] _____________

self = <test_filters.TestFilter3D object at 0x7c617411fb80>
device = device(type='cpu'), dtype = torch.float32

    def test_mean_filter_2batch_2ch(self, device, dtype):
        kernel = torch.ones(1, 3, 3, 3, device=device, dtype=dtype)
        sample = torch.tensor(
            [
                [
                    [
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 5.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                    ]
                ]
            ],
            device=device,
            dtype=dtype,
        )
        sample = sample.expand(2, 2, -1, -1, -1)
    
        expected = torch.tensor(
            [
                [
                    [
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 5.0, 5.0, 5.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                    ]
                ]
            ],
            device=device,
            dtype=dtype,
        )
        expected = expected.expand(2, 2, -1, -1, -1)
    
>       actual = filter3d(sample, kernel)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:552: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0.,...0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
____________ TestFilter3D.test_normalized_mean_filter[cpu-float32] _____________

self = <test_filters.TestFilter3D object at 0x7c617411feb0>
device = device(type='cpu'), dtype = torch.float32

    def test_normalized_mean_filter(self, device, dtype):
        kernel = torch.ones(1, 3, 3, 3, device=device, dtype=dtype)
        sample = torch.tensor(
            [
                [
                    [
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 5.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                    ]
                ]
            ],
            device=device,
            dtype=dtype,
        )
        sample = sample.expand(2, 2, -1, -1, -1)
    
        nv = 5.0 / 27  # normalization value
        expected = torch.tensor(
            [
                [
                    [
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, nv, nv, nv, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                    ]
                ]
            ],
            device=device,
            dtype=dtype,
        )
        expected = expected.expand(2, 2, -1, -1, -1)
    
>       actual = filter3d(sample, kernel, normalized=True)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:624: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0.,...0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_______________ TestFilter3D.test_even_sized_filter[cpu-float32] _______________

self = <test_filters.TestFilter3D object at 0x7c6174144220>
device = device(type='cpu'), dtype = torch.float32

    def test_even_sized_filter(self, device, dtype):
        kernel = torch.ones(1, 2, 2, 2, device=device, dtype=dtype)
        sample = torch.tensor(
            [
                [
                    [
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 5.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                    ]
                ]
            ],
            device=device,
            dtype=dtype,
        )
    
        expected = torch.tensor(
            [
                [
                    [
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 0.0, 0.0],
                            [0.0, 5.0, 5.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                        [
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                            [0.0, 0.0, 0.0, 0.0, 0.0],
                        ],
                    ]
                ]
            ],
            device=device,
            dtype=dtype,
        )
    
>       actual = filter3d(sample, kernel)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:694: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0.,...0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.],
           [0., 0., 0., 0., 0.]]]]])
pad = [0, 0, 0, 0, 0, 0, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_________________ TestFilter3D.test_noncontiguous[cpu-float32] _________________

self = <test_filters.TestFilter3D object at 0x7c6174144550>
device = device(type='cpu'), dtype = torch.float32

    def test_noncontiguous(self, device, dtype):
        batch_size = 3
        inp = torch.rand(3, 5, 5, 5, device=device, dtype=dtype).expand(batch_size, -1, -1, -1, -1)
        kernel = torch.ones(1, 2, 2, 2, device=device, dtype=dtype)
    
>       actual = filter3d(inp, kernel)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:702: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[0.9528, 0.7579, 0.6702, 0.7346, 0.6132],
           [0.6619, 0.6028, 0.9187, 0.8859, 0.2393],
           [... 0.7200],
           [0.9824, 0.7063, 0.6611, 0.4870, 0.4138],
           [0.2415, 0.6597, 0.0362, 0.5995, 0.6414]]]]])
pad = [0, 0, 0, 0, 0, 0, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_______________________ TestFilter3D.test_gradcheck[cpu] _______________________

self = <test_filters.TestFilter3D object at 0x7c6174144820>
device = device(type='cpu')

    def test_gradcheck(self, device):
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=torch.float64)
        sample = torch.ones(1, 1, 6, 7, 8, device=device, dtype=torch.float64)
    
        # evaluate function gradient
>       self.gradcheck(filter3d, (sample, kernel), nondet_tol=1e-8)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:710: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/testing/base.py:143: in gradcheck
    return gradcheck(func, inputs, raise_exception=raise_exception, fast_mode=fast_mode, **kwargs)
/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:2052: in gradcheck
    return _gradcheck_helper(**args)
/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:2074: in _gradcheck_helper
    func_out = func(*tupled_inputs)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., ..., 1., 1., 1., 1., 1.],
           [1., 1., 1., 1., 1., 1., 1., 1.]]]]], dtype=torch.float64,
       requires_grad=True)
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_____________ TestFilter3D.test_dynamo[cpu-float32-inductor-True] ______________

self = <test_filters.TestFilter3D object at 0x7c6174144e50>, normalized = True
device = device(type='cpu'), dtype = torch.float32
torch_optimizer = functools.partial(<function compile at 0x7c623f517400>, backend='inductor')

    @pytest.mark.parametrize("normalized", [True, False])
    def test_dynamo(self, normalized, device, dtype, torch_optimizer):
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(2, 3, 4, 10, 10, device=device, dtype=dtype)
        op = filter3d
        op_optimized = torch_optimizer(op)
    
>       expected = op(data, kernel, normalized=normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:722: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1....., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
_____________ TestFilter3D.test_dynamo[cpu-float32-inductor-False] _____________

self = <test_filters.TestFilter3D object at 0x7c6174144dc0>, normalized = False
device = device(type='cpu'), dtype = torch.float32
torch_optimizer = functools.partial(<function compile at 0x7c623f517400>, backend='inductor')

    @pytest.mark.parametrize("normalized", [True, False])
    def test_dynamo(self, normalized, device, dtype, torch_optimizer):
        kernel = torch.rand(1, 3, 3, 3, device=device, dtype=dtype)
        data = torch.ones(2, 3, 4, 10, 10, device=device, dtype=dtype)
        op = filter3d
        op_optimized = torch_optimizer(op)
    
>       expected = op(data, kernel, normalized=normalized)

/local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py:722: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/filter.py:142: in filter3d
    return filter3d(input, kernel, border_type, normalized)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/filters/temp.py:15: in filter3d
    input_padded = F.pad(input, padding, mode=border_type)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[[1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1....., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.],
           [1., 1., 1.,  ..., 1., 1., 1.]]]]])
pad = [1, 1, 1, 0, 1, 1, ...], mode = 'replicate', value = None

    def pad(
        input: Tensor,
        pad: List[int],
        mode: str = "constant",
        value: Optional[float] = None,
    ) -> Tensor:
        r"""
        pad(input, pad, mode="constant", value=None) -> Tensor
    
        Pads tensor.
    
        Padding size:
            The padding size by which to pad some dimensions of :attr:`input`
            are described starting from the last dimension and moving forward.
            :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions
            of ``input`` will be padded.
            For example, to pad only the last dimension of the input tensor, then
            :attr:`pad` has the form
            :math:`(\text{padding\_left}, \text{padding\_right})`;
            to pad the last 2 dimensions of the input tensor, then use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom})`;
            to pad the last 3 dimensions, use
            :math:`(\text{padding\_left}, \text{padding\_right},`
            :math:`\text{padding\_top}, \text{padding\_bottom}`
            :math:`\text{padding\_front}, \text{padding\_back})`.
    
        Padding mode:
            See :class:`torch.nn.CircularPad2d`, :class:`torch.nn.ConstantPad2d`,
            :class:`torch.nn.ReflectionPad2d`, and :class:`torch.nn.ReplicationPad2d`
            for concrete examples on how each of the padding modes works. Constant
            padding is implemented for arbitrary dimensions. Circular, replicate and
            reflection padding are implemented for padding the last 3 dimensions of a
            4D or 5D input tensor, the last 2 dimensions of a 3D or 4D input tensor,
            or the last dimension of a 2D or 3D input tensor.
    
        Note:
            When using the CUDA backend, this operation may induce nondeterministic
            behaviour in its backward pass that is not easily switched off.
            Please see the notes on :doc:`/notes/randomness` for background.
    
        Args:
            input (Tensor): N-dimensional tensor
            pad (tuple): m-elements tuple, where
                :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.
            mode: ``'constant'``, ``'reflect'``, ``'replicate'`` or ``'circular'``.
                Default: ``'constant'``
            value: fill value for ``'constant'`` padding. Default: ``0``
    
        Examples::
    
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p1d = (1, 1) # pad last dim by 1 on each side
            >>> out = F.pad(t4d, p1d, "constant", 0)  # effectively zero padding
            >>> print(out.size())
            torch.Size([3, 3, 4, 4])
            >>> p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)
            >>> out = F.pad(t4d, p2d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 3, 8, 4])
            >>> t4d = torch.empty(3, 3, 4, 2)
            >>> p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)
            >>> out = F.pad(t4d, p3d, "constant", 0)
            >>> print(out.size())
            torch.Size([3, 9, 7, 3])
        """
        if has_torch_function_unary(input):
            return handle_torch_function(
                torch.nn.functional.pad, (input,), input, pad, mode=mode, value=value
            )
        if not torch.jit.is_scripting():
            if torch.are_deterministic_algorithms_enabled() and (
                input.is_cuda or input.is_xpu
            ):
                if mode == "replicate":
                    # Use slow decomp whose backward will be in terms of index_put.
                    # importlib is required because the import cannot be top level
                    # (cycle) and cannot be nested (TS doesn't support)
                    return importlib.import_module(
                        "torch._decomp.decompositions"
                    )._replication_pad(input, pad)
>       return torch._C._nn.pad(input, pad, mode, value)
E       NotImplementedError: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now

/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/nn/functional.py:5096: NotImplementedError
=========================== short test summary info ============================
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-constant]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-reflect]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-replicate]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-circular]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-constant]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-reflect]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-replicate]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-circular]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-2]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-3]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-6]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-8]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_exception
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter[cpu-float32]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter_2batch_2ch[cpu-float32]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_normalized_mean_filter[cpu-float32]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_even_sized_filter[cpu-float32]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_noncontiguous[cpu-float32]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_gradcheck[cpu]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-True]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-False]
======================== 21 failed, 1 skipped in 1.63s =========================


Final Test Result:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'tvm', 'inductor', 'jit', 'cudagraphs', 'openxla', 'onnxrt', None}
model weights cached: []

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1, jaxtyping-0.2.38
collecting ... collected 22 items

../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-constant] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-reflect] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-replicate] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-circular] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-constant] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-reflect] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-replicate] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-circular] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-2] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-6] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-8] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_exception PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter_2batch_2ch[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_normalized_mean_filter[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_even_sized_filter[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_noncontiguous[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_gradcheck[cpu] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_module SKIPPED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-True] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-False] PASSED

======================== 21 passed, 1 skipped in 2.07s =========================


Initial Result:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'tvm', 'onnxrt', 'jit', 'openxla', 'cudagraphs', 'inductor', None}
model weights cached: []

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1, jaxtyping-0.2.38
collecting ... collected 22 items

../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-constant] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-reflect] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-replicate] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-True-circular] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-constant] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-reflect] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-replicate] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_smoke[cpu-float32-False-circular] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-2] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-6] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_cardinality[cpu-float32-8] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_exception PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_mean_filter_2batch_2ch[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_normalized_mean_filter[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_even_sized_filter[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_noncontiguous[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_gradcheck[cpu] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_module SKIPPED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-True] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/filters/test_filters.py::TestFilter3D::test_dynamo[cpu-float32-inductor-False] PASSED

======================== 21 passed, 1 skipped in 2.09s =========================
