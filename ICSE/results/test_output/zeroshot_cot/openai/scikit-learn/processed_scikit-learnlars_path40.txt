output file:
processed_scikit-learnlars_path40.json
function:
lars_path
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_X_none_gram_not_none FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[True]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_singular_matrix', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsCV-True-args4]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[False] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsCV-True-args4] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LarsCV-True-args3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LarsCV-True-args3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[False]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_add_features', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLars-True-args1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lasso]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_precomputed FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lasso] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-Lars-True-args0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LassoLarsIC]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_path_length', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LarsCV] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_add_features FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_ic', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_gives_lstsq_solution FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_singular_matrix FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv_max_iter', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[aic] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_early_stopping FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned2', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_x_none_gram_none_raises_value_error', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lasso] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LassoLarsIC] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_ic FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_X_none_gram_not_none', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lasso]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_gives_lstsq_solution', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LarsCV]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_readonly_data FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_all_precomputed', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_collinearity FAILED', 'FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[True]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LarsCV-True-args3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_multitarget FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_positive FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple_precomputed FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[aic]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple_precomputed', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsIC-False-args2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[Lars-True-args0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[Lars]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv_max_iter FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_collinearity', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_early_stopping', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_all_precomputed FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsCV-True-args4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLars-True-args1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsCV-True-args4] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned2 FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_lstsq', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[True] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsIC-False-args2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_positive_constraint FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsIC-False-args2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LarsCV-True-args3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[True]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsCV-True-args4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[bic]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-Lars-True-args0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLars-True-args1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[False] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsIC-False-args2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_multitarget', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lar] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_precomputed', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_positive_constraint', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsIC-False-args2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_estimatorclasses_positive_constraint FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LarsCV-True-args3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLars-True-args1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_readonly_data', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-Lars-True-args0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsIC-False-args2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[True] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_positive', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsCV-True-args4]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_R_implementation', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[Lars] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lar]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_x_none_gram_none_raises_value_error FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-Lars-True-args0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_n_nonzero_coefs FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LarsCV-True-args3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLars-True-args1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_lstsq FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_all_precomputed FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lar] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_all_precomputed', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_rank_deficient_design', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[False]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[False]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[True] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lar]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_rank_deficient_design FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_R_implementation FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_estimatorclasses_positive_constraint', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLars-True-args1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_n_nonzero_coefs', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[False] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[Lars-True-args0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[bic] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_path_length FAILED'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 62 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple I: Seeding RNGs with 1129772027
FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple_precomputed FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lar] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lasso] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lar] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lasso] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_x_none_gram_none_raises_value_error FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_all_precomputed FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_lstsq FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_gives_lstsq_solution FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_collinearity FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_precomputed FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_all_precomputed FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[Lars] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LarsCV] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LassoLarsIC] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_singular_matrix FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_rank_deficient_design FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_early_stopping FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_path_length FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned2 FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_add_features FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_n_nonzero_coefs FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_multitarget FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv_max_iter FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_ic FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_readonly_data FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_positive_constraint FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_estimatorclasses_positive_constraint FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_positive FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_R_implementation FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[True] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[False] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[True] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[False] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_X_none_gram_not_none FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_copy_X_with_auto_gram PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-Lars-True-args0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLars-True-args1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsIC-False-args2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LarsCV-True-args3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsCV-True-args4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-Lars-True-args0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLars-True-args1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsIC-False-args2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LarsCV-True-args3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsCV-True-args4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[Lars-True-args0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLars-True-args1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsIC-False-args2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LarsCV-True-args3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsCV-True-args4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[aic] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[bic] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[True] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[False] FAILED

=================================== FAILURES ===================================
_________________________________ test_simple __________________________________

    def test_simple():
        # Principle of Lars is to keep covariances tied and decreasing
    
        # also test verbose output
        import sys
        from io import StringIO
    
        old_stdout = sys.stdout
        try:
            sys.stdout = StringIO()
    
>           _, _, coef_path_ = linear_model.lars_path(X, y, method="lar", verbose=10)
E           TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:48: TypeError
___________________________ test_simple_precomputed ____________________________

    def test_simple_precomputed():
        # The same, with precomputed Gram matrix
    
>       _, _, coef_path_ = linear_model.lars_path(X, y, Gram=G, method="lar")
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:70: TypeError
___________________ test_lars_path_gram_equivalent[True-lar] ___________________

method = 'lar', return_path = True

    @pytest.mark.parametrize("method", ["lar", "lasso"])
    @pytest.mark.parametrize("return_path", [True, False])
    def test_lars_path_gram_equivalent(method, return_path):
>       _assert_same_lars_path_result(
            linear_model.lars_path_gram(
                Xy=Xy, Gram=G, n_samples=n_samples, method=method, return_path=return_path
            ),
            linear_model.lars_path(X, y, Gram=G, method=method, return_path=return_path),
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

output1 = (array([2.14804358, 2.01202214, 1.02465091, 0.71509814, 0.29441072,
       0.20086946, 0.15602894, 0.04520626, 0.01239...       0.        ,    0.        ,   12.07957664,   54.76900516,
          64.48867506,   64.60826229,   67.62669218]]))
output2 = None

    def _assert_same_lars_path_result(output1, output2):
>       assert len(output1) == len(output2)
E       TypeError: object of type 'NoneType' has no len()

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:86: TypeError
__________________ test_lars_path_gram_equivalent[True-lasso] __________________

method = 'lasso', return_path = True

    @pytest.mark.parametrize("method", ["lar", "lasso"])
    @pytest.mark.parametrize("return_path", [True, False])
    def test_lars_path_gram_equivalent(method, return_path):
>       _assert_same_lars_path_result(
            linear_model.lars_path_gram(
                Xy=Xy, Gram=G, n_samples=n_samples, method=method, return_path=return_path
            ),
            linear_model.lars_path(X, y, Gram=G, method=method, return_path=return_path),
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

output1 = (array([2.14804358, 2.01202214, 1.02465091, 0.71509814, 0.29441072,
       0.20086946, 0.15602894, 0.04520626, 0.01239....07957664,   54.76900516,
          64.48867506,   64.60826229,   66.3321337 ,   67.18060543,
          67.62669218]]))
output2 = None

    def _assert_same_lars_path_result(output1, output2):
>       assert len(output1) == len(output2)
E       TypeError: object of type 'NoneType' has no len()

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:86: TypeError
__________________ test_lars_path_gram_equivalent[False-lar] ___________________

method = 'lar', return_path = False

    @pytest.mark.parametrize("method", ["lar", "lasso"])
    @pytest.mark.parametrize("return_path", [True, False])
    def test_lars_path_gram_equivalent(method, return_path):
>       _assert_same_lars_path_result(
            linear_model.lars_path_gram(
                Xy=Xy, Gram=G, n_samples=n_samples, method=method, return_path=return_path
            ),
            linear_model.lars_path(X, y, Gram=G, method=method, return_path=return_path),
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

output1 = (array([0.]), [np.int64(2), np.int64(8), np.int64(3), np.int64(6), np.int64(1), np.int64(9), ...], array([ -10.0098663...324.3846455 ,
       -792.17563855,  476.73902101,  101.04326794,  177.06323767,
        751.27369956,   67.62669218]))
output2 = None

    def _assert_same_lars_path_result(output1, output2):
>       assert len(output1) == len(output2)
E       TypeError: object of type 'NoneType' has no len()

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:86: TypeError
_________________ test_lars_path_gram_equivalent[False-lasso] __________________

method = 'lasso', return_path = False

    @pytest.mark.parametrize("method", ["lar", "lasso"])
    @pytest.mark.parametrize("return_path", [True, False])
    def test_lars_path_gram_equivalent(method, return_path):
>       _assert_same_lars_path_result(
            linear_model.lars_path_gram(
                Xy=Xy, Gram=G, n_samples=n_samples, method=method, return_path=return_path
            ),
            linear_model.lars_path(X, y, Gram=G, method=method, return_path=return_path),
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:94: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

output1 = (array([0.]), [np.int64(2), np.int64(8), np.int64(3), np.int64(1), np.int64(9), np.int64(4), ...], array([ -10.0098663...324.3846455 ,
       -792.17563855,  476.73902101,  101.04326794,  177.06323767,
        751.27369956,   67.62669218]))
output2 = None

    def _assert_same_lars_path_result(output1, output2):
>       assert len(output1) == len(output2)
E       TypeError: object of type 'NoneType' has no len()

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:86: TypeError
___________________ test_x_none_gram_none_raises_value_error ___________________

    def test_x_none_gram_none_raises_value_error():
        # Test that lars_path with no X and Gram raises exception
        Xy = np.dot(X.T, y)
        with pytest.raises(ValueError, match="X and Gram cannot both be unspecified"):
>           linear_model.lars_path(None, y, Gram=None, Xy=Xy)
E           Failed: DID NOT RAISE <class 'ValueError'>

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:106: Failed
_____________________________ test_all_precomputed _____________________________

    def test_all_precomputed():
        # Test that lars_path with precomputed Gram and Xy gives the right answer
        G = np.dot(X.T, X)
        Xy = np.dot(X.T, y)
        for method in "lar", "lasso":
            output = linear_model.lars_path(X, y, method=method)
            output_pre = linear_model.lars_path(X, y, Gram=G, Xy=Xy, method=method)
>           for expected, got in zip(output, output_pre):
E           TypeError: 'NoneType' object is not iterable

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:116: TypeError
_______________________________ test_lars_lstsq ________________________________

    @pytest.mark.filterwarnings("ignore: `rcond` parameter will change")
    def test_lars_lstsq():
        # Test that Lars gives least square solution at the end
        # of the path
        X1 = 3 * X  # use un-normalized dataset
        clf = linear_model.LassoLars(alpha=0.0)
>       clf.fit(X1, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:127: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars(alpha=0.0)
X = array([[ 0.11422772,  0.15204036,  0.18508862, ..., -0.00777679,
         0.05972246, -0.05293838],
       [-0.0056460...619, -0.07779102],
       [-0.13641743, -0.13392491, -0.21909091, ..., -0.11848015,
        -0.01266454,  0.00919323]])
y = array([[-1.13348416e+00],
       [-7.71334842e+01],
       [-1.11334842e+01],
       [ 5.38665158e+01],
       [-1.713...58665158e+01],
       [-4.81334842e+01],
       [-2.01334842e+01],
       [ 6.78665158e+01],
       [-9.51334842e+01]])
max_iter = 500, alpha = 0.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_______________________ test_lasso_gives_lstsq_solution ________________________

    @pytest.mark.filterwarnings("ignore: `rcond` parameter will change")
    def test_lasso_gives_lstsq_solution():
        # Test that Lars Lasso gives least square solution at the end
        # of the path
>       _, _, coef_path_ = linear_model.lars_path(X, y, method="lasso")
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:137: TypeError
______________________________ test_collinearity _______________________________

    def test_collinearity():
        # Check that lars_path is robust to collinearity in input
        X = np.array([[3.0, 3.0, 1.0], [2.0, 2.0, 0.0], [1.0, 1.0, 0]])
        y = np.array([1.0, 0.0, 0])
        rng = np.random.RandomState(0)
    
        f = ignore_warnings
>       _, _, coef_path_ = f(linear_model.lars_path)(X, y, alpha_min=0.01)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:149: TypeError
_________________________________ test_no_path _________________________________

    def test_no_path():
        # Test that the ``return_path=False`` option returns the correct output
>       alphas_, _, coef_path_ = linear_model.lars_path(X, y, method="lar")
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:173: TypeError
___________________________ test_no_path_precomputed ___________________________

    def test_no_path_precomputed():
        # Test that the ``return_path=False`` option with Gram remains correct
>       alphas_, _, coef_path_ = linear_model.lars_path(X, y, method="lar", Gram=G)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:182: TypeError
_________________________ test_no_path_all_precomputed _________________________

    def test_no_path_all_precomputed():
        # Test that the ``return_path=False`` option with Gram and Xy remains
        # correct
        X, y = 3 * diabetes.data, diabetes.target
        G = np.dot(X.T, X)
        Xy = np.dot(X.T, y)
>       alphas_, _, coef_path_ = linear_model.lars_path(
            X, y, method="lasso", Xy=Xy, Gram=G, alpha_min=0.9
        )
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:197: TypeError
__________________________ test_lars_precompute[Lars] __________________________

classifier = <class 'sklearn.linear_model._least_angle.Lars'>

    @pytest.mark.parametrize(
        "classifier", [linear_model.Lars, linear_model.LarsCV, linear_model.LassoLarsIC]
    )
    def test_lars_precompute(classifier):
        # Check for different values of precompute
        G = np.dot(X.T, X)
    
        clf = classifier(precompute=G)
>       output_1 = ignore_warnings(clf.fit)(X, y).coef_

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:147: in wrapper
    return fn(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Lars(precompute=array([[ 1.        ,  0.1737371 ,  0.18508467,  0.33542759,  0.26006082,
         0.21924314, -0.07518...322,  0.38867999,  0.39043002,  0.32571675,
         0.29060038, -0.2736973 ,  0.41721211,  0.46466885,  1.        ]]))
X = array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,
         0.01990749, -0.01764613],
       [-0.0018820...873, -0.02593034],
       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,
        -0.00422151,  0.00306441]])
y = array([[-1.13348416e+00],
       [-7.71334842e+01],
       [-1.11334842e+01],
       [ 5.38665158e+01],
       [-1.713...58665158e+01],
       [-4.81334842e+01],
       [-2.01334842e+01],
       [ 6.78665158e+01],
       [-9.51334842e+01]])
max_iter = 500, alpha = 0.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_________________________ test_lars_precompute[LarsCV] _________________________

classifier = <class 'sklearn.linear_model._least_angle.LarsCV'>

    @pytest.mark.parametrize(
        "classifier", [linear_model.Lars, linear_model.LarsCV, linear_model.LassoLarsIC]
    )
    def test_lars_precompute(classifier):
        # Check for different values of precompute
        G = np.dot(X.T, X)
    
        clf = classifier(precompute=G)
>       output_1 = ignore_warnings(clf.fit)(X, y).coef_

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:147: in wrapper
    return fn(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-0.04774781, -0.04509556, -0.05086479, ..., -0.04304051,
        -0.02770017,  0.01631028],
       [ 0.0103728...398, -0.0292529 ],
       [-0.04774781, -0.04509556, -0.07565447, ..., -0.04304051,
        -0.00712626, -0.00025815]])
y_train = array([ -45.54390935,  -58.54390935,    7.45609065, -108.54390935,
        -60.54390935,  -66.54390935,    5.45609065,... -92.54390935, -108.54390935,
         21.45609065,  -52.54390935,  -24.54390935,   63.45609065,
        -99.54390935])
X_test = array([[ 3.58005702e-02,  5.02261940e-02,  5.90720402e-02,
         1.92073047e-02, -4.74840162e-02, -3.81717330e-02,
...-3.51004405e-02, -4.03637722e-02,
         3.97731608e-02, -4.30405057e-02, -3.74265210e-02,
         6.60155623e-02]])
y_test = array([  -5.54390935,  -81.54390935,  -15.54390935,   49.45609065,
        -21.54390935,  -59.54390935,  -18.54390935,...-104.54390935,   53.45609065,
        -91.54390935,  -15.54390935, -101.54390935,  -22.54390935,
       -114.54390935])
Gram = 'auto', copy = False, method = 'lar', verbose = 0, fit_intercept = True
max_iter = 500, eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
______________________ test_lars_precompute[LassoLarsIC] _______________________

classifier = <class 'sklearn.linear_model._least_angle.LassoLarsIC'>

    @pytest.mark.parametrize(
        "classifier", [linear_model.Lars, linear_model.LarsCV, linear_model.LassoLarsIC]
    )
    def test_lars_precompute(classifier):
        # Check for different values of precompute
        G = np.dot(X.T, X)
    
        clf = classifier(precompute=G)
>       output_1 = ignore_warnings(clf.fit)(X, y).coef_

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:216: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:147: in wrapper
    return fn(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC(precompute=array([[ 1.        ,  0.1737371 ,  0.18508467,  0.33542759,  0.26006082,
         0.21924314, -...322,  0.38867999,  0.39043002,  0.32571675,
         0.29060038, -0.2736973 ,  0.41721211,  0.46466885,  1.        ]]))
X = array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,
         0.01990749, -0.01764613],
       [-0.0018820...873, -0.02593034],
       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,
        -0.00422151,  0.00306441]])
y = array([-1.13348416e+00, -7.71334842e+01, -1.11334842e+01,  5.38665158e+01,
       -1.71334842e+01, -5.51334842e+01, -1...1,
       -1.04133484e+02,  2.58665158e+01, -4.81334842e+01, -2.01334842e+01,
        6.78665158e+01, -9.51334842e+01])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
_____________________________ test_singular_matrix _____________________________

    def test_singular_matrix():
        # Test when input is a singular matrix
        X1 = np.array([[1, 1.0], [1.0, 1.0]])
        y1 = np.array([1, 1])
>       _, _, coef_path = linear_model.lars_path(X1, y1)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:227: TypeError
__________________________ test_rank_deficient_design __________________________

    def test_rank_deficient_design():
        # consistency test that checks that LARS Lasso is handling rank
        # deficient input data (with n_features < rank) in the same way
        # as coordinate descent Lasso
        y = [5, 0, 5]
        for X in ([[5, 0], [0, 5], [10, 10]], [[10, 10, 0], [1e-32, 0, 0], [0, 0, 1]]):
            # To be able to use the coefs to compute the objective function,
            # we need to turn off normalization
            lars = linear_model.LassoLars(0.1)
>           coef_lars_ = lars.fit(X, y).coef_

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:240: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars(alpha=0.1)
X = array([[ 0., -5.],
       [-5.,  0.],
       [ 5.,  5.]])
y = array([[ 1.66666667],
       [-3.33333333],
       [ 1.66666667]])
max_iter = 500, alpha = 0.1, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_________________________ test_lasso_lars_vs_lasso_cd __________________________

    def test_lasso_lars_vs_lasso_cd():
        # Test that LassoLars and Lasso using coordinate descent give the
        # same results.
        X = 3 * diabetes.data
    
>       alphas, _, lasso_path = linear_model.lars_path(X, y, method="lasso")
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:257: TypeError
__________________ test_lasso_lars_vs_lasso_cd_early_stopping __________________

    def test_lasso_lars_vs_lasso_cd_early_stopping():
        # Test that LassoLars and Lasso using coordinate descent give the
        # same results when early stopping is used.
        # (test : before, in the middle, and in the last part of the path)
        alphas_min = [10, 0.9, 1e-4]
    
        X = diabetes.data
    
        for alpha_min in alphas_min:
>           alphas, _, lasso_path = linear_model.lars_path(
                X, y, method="lasso", alpha_min=alpha_min
            )
E           TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:298: TypeError
_________________________ test_lasso_lars_path_length __________________________

    def test_lasso_lars_path_length():
        # Test that the path length of the LassoLars is right
        lasso = linear_model.LassoLars()
>       lasso.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:325: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars()
X = array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,
         0.01990749, -0.01764613],
       [-0.0018820...873, -0.02593034],
       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,
        -0.00422151,  0.00306441]])
y = array([[-1.13348416e+00],
       [-7.71334842e+01],
       [-1.11334842e+01],
       [ 5.38665158e+01],
       [-1.713...58665158e+01],
       [-4.81334842e+01],
       [-2.01334842e+01],
       [ 6.78665158e+01],
       [-9.51334842e+01]])
max_iter = 500, alpha = 1.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_________________ test_lasso_lars_vs_lasso_cd_ill_conditioned __________________

    def test_lasso_lars_vs_lasso_cd_ill_conditioned():
        # Test lasso lars on a very ill-conditioned design, and check that
        # it does not blow up, and stays somewhat close to a solution given
        # by the coordinate descent solver
        # Also test that lasso_path (using lars_path output style) gives
        # the same result as lars_path and previous lasso output style
        # under these conditions.
        rng = np.random.RandomState(42)
    
        # Generate data
        n, m = 70, 100
        k = 5
        X = rng.randn(n, m)
        w = np.zeros((m, 1))
        i = np.arange(0, m)
        rng.shuffle(i)
        supp = i[:k]
        w[supp] = np.sign(rng.randn(k, 1)) * (rng.rand(k, 1) + 1)
        y = np.dot(X, w)
        sigma = 0.2
        y += sigma * rng.rand(*y.shape)
        y = y.squeeze()
>       lars_alphas, _, lars_coef = linear_model.lars_path(X, y, method="lasso")
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:355: TypeError
_________________ test_lasso_lars_vs_lasso_cd_ill_conditioned2 _________________

    def test_lasso_lars_vs_lasso_cd_ill_conditioned2():
        # Create an ill-conditioned situation in which the LARS has to go
        # far in the path to converge, and check that LARS and coordinate
        # descent give the same answers
        # Note it used to be the case that Lars had to use the drop for good
        # strategy for this but this is no longer the case with the
        # equality_tolerance checks
        X = [[1e20, 1e20, 0], [-1e-32, 0, 0], [1, 1, 1]]
        y = [10, 10, 1]
        alpha = 0.0001
    
        def objective_function(coef):
            return 1.0 / (2.0 * len(X)) * linalg.norm(
                y - np.dot(X, coef)
            ) ** 2 + alpha * linalg.norm(coef, 1)
    
        lars = linear_model.LassoLars(alpha=alpha)
        warning_message = "Regressors in active set degenerate."
        with pytest.warns(ConvergenceWarning, match=warning_message):
>           lars.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:381: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars(alpha=0.0001)
X = array([[ 6.66666667e+19,  6.66666667e+19, -3.33333333e-01],
       [-3.33333333e+19, -3.33333333e+19, -3.33333333e-01],
       [-3.33333333e+19, -3.33333333e+19,  6.66666667e-01]])
y = array([[ 3.],
       [ 3.],
       [-6.]]), max_iter = 500, alpha = 0.0001
fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError

During handling of the above exception, another exception occurred:

    def test_lasso_lars_vs_lasso_cd_ill_conditioned2():
        # Create an ill-conditioned situation in which the LARS has to go
        # far in the path to converge, and check that LARS and coordinate
        # descent give the same answers
        # Note it used to be the case that Lars had to use the drop for good
        # strategy for this but this is no longer the case with the
        # equality_tolerance checks
        X = [[1e20, 1e20, 0], [-1e-32, 0, 0], [1, 1, 1]]
        y = [10, 10, 1]
        alpha = 0.0001
    
        def objective_function(coef):
            return 1.0 / (2.0 * len(X)) * linalg.norm(
                y - np.dot(X, coef)
            ) ** 2 + alpha * linalg.norm(coef, 1)
    
        lars = linear_model.LassoLars(alpha=alpha)
        warning_message = "Regressors in active set degenerate."
        with pytest.warns(ConvergenceWarning, match=warning_message):
>           lars.fit(X, y)
E           Failed: DID NOT WARN. No warnings of type (<class 'sklearn.exceptions.ConvergenceWarning'>,) were emitted.
E            Emitted warnings: [].

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:381: Failed
____________________________ test_lars_add_features ____________________________

    def test_lars_add_features():
        # assure that at least some features get added if necessary
        # test for 6d2b4c
        # Hilbert matrix
        n = 5
        H = 1.0 / (np.arange(1, n + 1) + np.arange(n)[:, np.newaxis])
>       clf = linear_model.Lars(fit_intercept=False).fit(H, np.arange(n))

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:398: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Lars(fit_intercept=False)
X = array([[1.        , 0.5       , 0.33333333, 0.25      , 0.2       ],
       [0.5       , 0.33333333, 0.25      , 0.2  ... 0.2       , 0.16666667, 0.14285714, 0.125     ],
       [0.2       , 0.16666667, 0.14285714, 0.125     , 0.11111111]])
y = array([[0.],
       [1.],
       [2.],
       [3.],
       [4.]])
max_iter = 500, alpha = 0.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
__________________________ test_lars_n_nonzero_coefs ___________________________

verbose = False

    def test_lars_n_nonzero_coefs(verbose=False):
        lars = linear_model.Lars(n_nonzero_coefs=6, verbose=verbose)
>       lars.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:404: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Lars(n_nonzero_coefs=6)
X = array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,
         0.01990749, -0.01764613],
       [-0.0018820...873, -0.02593034],
       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,
        -0.00422151,  0.00306441]])
y = array([[-1.13348416e+00],
       [-7.71334842e+01],
       [-1.11334842e+01],
       [ 5.38665158e+01],
       [-1.713...58665158e+01],
       [-4.81334842e+01],
       [-2.01334842e+01],
       [ 6.78665158e+01],
       [-9.51334842e+01]])
max_iter = 6, alpha = 0.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_______________________________ test_multitarget _______________________________

    def test_multitarget():
        # Assure that estimators receiving multidimensional y do the right thing
        Y = np.vstack([y, y**2]).T
        n_targets = Y.shape[1]
        estimators = [
            linear_model.LassoLars(),
            linear_model.Lars(),
            # regression test for gh-1615
            linear_model.LassoLars(fit_intercept=False),
            linear_model.Lars(fit_intercept=False),
        ]
    
        for estimator in estimators:
>           estimator.fit(X, Y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:424: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars()
X = array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,
         0.01990749, -0.01764613],
       [-0.0018820...873, -0.02593034],
       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,
        -0.00422151,  0.00306441]])
y = array([[-1.13348416e+00, -6.27348190e+03],
       [-7.71334842e+01, -2.34494819e+04],
       [-1.11334842e+01, -9.1934....01334842e+01, -1.16504819e+04],
       [ 6.78665158e+01,  1.93255181e+04],
       [-9.51334842e+01, -2.58254819e+04]])
max_iter = 500, alpha = 1.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_________________________________ test_lars_cv _________________________________

    def test_lars_cv():
        # Test the LassoLarsCV object by checking that the optimal alpha
        # increases as the number of samples increases.
        # This property is not actually guaranteed in general and is just a
        # property of the given dataset, with the given steps chosen.
        old_alpha = 0
        lars_cv = linear_model.LassoLarsCV()
        for length in (400, 200, 100):
            X = diabetes.data[:length]
            y = diabetes.target[:length]
>           lars_cv.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:452: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[ 0.06825767, -0.04587359,  0.01016512, ..., -0.0056032 ,
         0.02246708, -0.00673092],
       [ 0.0101370...855, -0.06886253],
       [ 0.05736005,  0.04944816,  0.02094324, ..., -0.0056032 ,
         0.0366145 ,  0.01812172]])
y_train = array([-1.3068750e+01, -1.0506875e+02, -1.0406875e+02,  5.3931250e+01,
       -9.1068750e+01, -1.5068750e+01, -1.01068...-8.7068750e+01,  1.1693125e+02,  1.0193125e+02,
       -1.1306875e+02,  4.1931250e+01,  8.5931250e+01,  7.5931250e+01])
X_test = array([[ 3.55648218e-02,  4.94481605e-02,  5.97444705e-02,
         1.96208077e-02, -4.69543913e-02, -3.72524346e-02,
...-1.71994107e-04,  1.75485462e-02,
         1.37543687e-02, -5.60320457e-03, -7.12595427e-02,
        -2.74414583e-02]])
y_test = array([  -5.06875,  -81.06875,  -15.06875,   49.93125,  -21.06875,
        -59.06875,  -18.06875,  -93.06875,  -46.068...75,  113.93125,   45.93125,  -45.06875,  -71.06875,
       -114.06875,   13.93125,   43.93125,   95.93125,  -43.06875])
Gram = 'auto', copy = False, method = 'lasso', verbose = 0, fit_intercept = True
max_iter = 500, eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
____________________________ test_lars_cv_max_iter _____________________________

recwarn = WarningsRecorder(record=True)

    def test_lars_cv_max_iter(recwarn):
        warnings.simplefilter("always")
        with np.errstate(divide="raise", invalid="raise"):
            X = diabetes.data
            y = diabetes.target
            rng = np.random.RandomState(42)
            x = rng.randn(len(y))
            X = diabetes.data
            X = np.c_[X, x, x]  # add correlated features
            X = StandardScaler().fit_transform(X)
            lars_cv = linear_model.LassoLarsCV(max_iter=5, cv=5)
>           lars_cv.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:469: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-1.00384031, -0.94807988, -1.069371  , ...,  0.34290399,
         0.46835995,  0.46835995],
       [ 0.2180756...753,  0.33506753],
       [-1.00384031, -0.94807988, -1.59054413, ..., -0.00542726,
        -0.09488695, -0.09488695]])
y_train = array([ -45.54390935,  -58.54390935,    7.45609065, -108.54390935,
        -60.54390935,  -66.54390935,    5.45609065,... -92.54390935, -108.54390935,
         21.45609065,  -52.54390935,  -24.54390935,   63.45609065,
        -99.54390935])
X_test = array([[ 0.75266389,  1.05594526,  1.24191853, ..., -0.44084131,
         0.4512826 ,  0.4512826 ],
       [-0.0874033...199,  0.27800199],
       [-1.1565798 ,  1.05594526, -0.91075309, ...,  1.38789772,
        -0.60768909, -0.60768909]])
y_test = array([  -5.54390935,  -81.54390935,  -15.54390935,   49.45609065,
        -21.54390935,  -59.54390935,  -18.54390935,...-104.54390935,   53.45609065,
        -91.54390935,  -15.54390935, -101.54390935,  -22.54390935,
       -114.54390935])
Gram = 'auto', copy = False, method = 'lasso', verbose = 0, fit_intercept = True
max_iter = 5, eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
______________________________ test_lasso_lars_ic ______________________________

    def test_lasso_lars_ic():
        # Test the LassoLarsIC object by checking that
        # - some good features are selected.
        # - alpha_bic > alpha_aic
        # - n_nonzero_bic < n_nonzero_aic
        lars_bic = linear_model.LassoLarsIC("bic")
        lars_aic = linear_model.LassoLarsIC("aic")
        rng = np.random.RandomState(42)
        X = diabetes.data
        X = np.c_[X, rng.randn(X.shape[0], 5)]  # add 5 bad features
        X = StandardScaler().fit_transform(X)
>       lars_bic.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:490: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC(criterion='bic')
X = array([[ 0.80050009,  1.06548848,  1.29708846, ...,  0.62889677,
         1.55910844, -0.33493618],
       [-0.0395671...515, -1.05680953],
       [-0.9560041 , -0.93853666, -1.53537419, ...,  0.32191385,
        -0.3617269 , -0.10162381]])
y = array([-1.13348416e+00, -7.71334842e+01, -1.11334842e+01,  5.38665158e+01,
       -1.71334842e+01, -5.51334842e+01, -1...1,
       -1.04133484e+02,  2.58665158e+01, -4.81334842e+01, -2.01334842e+01,
        6.78665158e+01, -9.51334842e+01])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
_________________________ test_lars_path_readonly_data _________________________

    def test_lars_path_readonly_data():
        # When using automated memory mapping on large input, the
        # fold data is in read-only mode
        # This is a non-regression test for:
        # https://github.com/scikit-learn/scikit-learn/issues/4597
        splitted_data = train_test_split(X, y, random_state=42)
        with TempMemmap(splitted_data) as (X_train, X_test, y_train, y_test):
            # The following should not fail despite copy=False
>           _lars_path_residues(X_train, y_train, X_test, y_test, copy=False)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:507: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = memmap([[-0.00652979, -0.04492506,  0.03997022, ..., -0.040356  ,
          0.05053921,  0.02466493],
        [ 0.0624...9, -0.04160878],
        [-0.09371071, -0.04492506,  0.02595866, ..., -0.040356  ,
         -0.00687997, -0.00432982]])
y_train = memmap([  11.65558912,   34.65558912,   18.65558912,   65.65558912,
          51.65558912,  -57.34441088,  -94.3444108...       -13.34441088,  115.65558912,  -20.34441088,   47.65558912,
          -6.34441088,  -90.34441088,  147.65558912])
X_test = memmap([[ 0.04432575, -0.04492506, -0.00853132, ...,  0.03344624,
          0.03069455, -0.00847192],
        [ 0.0915...5, -0.02918246],
        [ 0.06612098,  0.0503967 , -0.03871006, ...,  0.03344624,
         -0.0005902 ,  0.02880704]])
y_test = memmap([  64.65558912,  -84.34441088,   47.65558912,   75.65558912,
         -43.34441088,  -70.34441088,   87.6555891...        42.65558912,  -57.34441088, -101.34441088,  -83.34441088,
         107.65558912, -102.34441088,  -52.34441088])
Gram = None, copy = False, method = 'lar', verbose = False, fit_intercept = True
max_iter = 500, eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
______________________ test_lars_path_positive_constraint ______________________

    def test_lars_path_positive_constraint():
        # this is the main test for the positive parameter on the lars_path method
        # the estimator classes just make use of this function
    
        # we do the test on the diabetes dataset
    
        # ensure that we get negative coefficients when positive=False
        # and all positive when positive=True
        # for method 'lar' (default) and lasso
    
        err_msg = "Positive constraint not supported for 'lar' coding method."
        with pytest.raises(ValueError, match=err_msg):
>           linear_model.lars_path(
                diabetes["data"], diabetes["target"], method="lar", positive=True
            )
E           Failed: DID NOT RAISE <class 'ValueError'>

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:522: Failed
__________________ test_estimatorclasses_positive_constraint ___________________

    def test_estimatorclasses_positive_constraint():
        # testing the transmissibility for the positive option of all estimator
        # classes in this same function here
        default_parameter = {"fit_intercept": False}
    
        estimator_parameter_map = {
            "LassoLars": {"alpha": 0.1},
            "LassoLarsCV": {},
            "LassoLarsIC": {},
        }
        for estname in estimator_parameter_map:
            params = default_parameter.copy()
            params.update(estimator_parameter_map[estname])
            estimator = getattr(linear_model, estname)(positive=False, **params)
>           estimator.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:563: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars(alpha=0.1, fit_intercept=False)
X = array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,
         0.01990749, -0.01764613],
       [-0.0018820...873, -0.02593034],
       [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,
        -0.00422151,  0.00306441]])
y = array([[151.],
       [ 75.],
       [141.],
       [206.],
       [135.],
       [ 97.],
       [138.],
       [ 63.]...      [ 49.],
       [ 64.],
       [ 48.],
       [178.],
       [104.],
       [132.],
       [220.],
       [ 57.]])
max_iter = 500, alpha = 0.1, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_____________________ test_lasso_lars_vs_lasso_cd_positive _____________________

    def test_lasso_lars_vs_lasso_cd_positive():
        # Test that LassoLars and Lasso using coordinate descent give the
        # same results when using the positive option
    
        # This test is basically a copy of the above with additional positive
        # option. However for the middle part, the comparison of coefficient values
        # for a range of alphas, we had to make an adaptations. See below.
    
        # not normalized data
        X = 3 * diabetes.data
    
>       alphas, _, lasso_path = linear_model.lars_path(X, y, method="lasso", positive=True)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:581: TypeError
_____________________ test_lasso_lars_vs_R_implementation ______________________

    def test_lasso_lars_vs_R_implementation():
        # Test that sklearn LassoLars implementation agrees with the LassoLars
        # implementation available in R (lars library) when fit_intercept=False.
    
        # Let's generate the data used in the bug report 7778
        y = np.array([-6.45006793, -3.51251449, -8.52445396, 6.12277822, -19.42109366])
        x = np.array(
            [
                [0.47299829, 0, 0, 0, 0],
                [0.08239882, 0.85784863, 0, 0, 0],
                [0.30114139, -0.07501577, 0.80895216, 0, 0],
                [-0.01460346, -0.1015233, 0.0407278, 0.80338378, 0],
                [-0.69363927, 0.06754067, 0.18064514, -0.0803561, 0.40427291],
            ]
        )
    
        X = x.T
    
        # The R result was obtained using the following code:
        #
        # library(lars)
        # model_lasso_lars = lars(X, t(y), type="lasso", intercept=FALSE,
        #                         trace=TRUE, normalize=FALSE)
        # r = t(model_lasso_lars$beta)
        #
    
        r = np.array(
            [
                [
                    0,
                    0,
                    0,
                    0,
                    0,
                    -79.810362809499026,
                    -83.528788732782829,
                    -83.777653739190711,
                    -83.784156932888934,
                    -84.033390591756657,
                ],
                [0, 0, 0, 0, -0.476624256777266, 0, 0, 0, 0, 0.025219751009936],
                [
                    0,
                    -3.577397088285891,
                    -4.702795355871871,
                    -7.016748621359461,
                    -7.614898471899412,
                    -0.336938391359179,
                    0,
                    0,
                    0.001213370600853,
                    0.048162321585148,
                ],
                [
                    0,
                    0,
                    0,
                    2.231558436628169,
                    2.723267514525966,
                    2.811549786389614,
                    2.813766976061531,
                    2.817462468949557,
                    2.817368178703816,
                    2.816221090636795,
                ],
                [
                    0,
                    0,
                    -1.218422599914637,
                    -3.457726183014808,
                    -4.021304522060710,
                    -45.827461592423745,
                    -47.776608869312305,
                    -47.911561610746404,
                    -47.914845922736234,
                    -48.039562334265717,
                ],
            ]
        )
    
        model_lasso_lars = linear_model.LassoLars(alpha=0, fit_intercept=False)
>       model_lasso_lars.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:703: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars(alpha=0, fit_intercept=False)
X = array([[ 0.47299829,  0.08239882,  0.30114139, -0.01460346, -0.69363927],
       [ 0.        ,  0.85784863, -0.0750157...   ,  0.        ,  0.80338378, -0.0803561 ],
       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.40427291]])
y = array([[ -6.45006793],
       [ -3.51251449],
       [ -8.52445396],
       [  6.12277822],
       [-19.42109366]])
max_iter = 500, alpha = 0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
____________________ test_lasso_lars_copyX_behaviour[True] _____________________

copy_X = True

    @pytest.mark.parametrize("copy_X", [True, False])
    def test_lasso_lars_copyX_behaviour(copy_X):
        """
        Test that user input regarding copy_X is not being overridden (it was until
        at least version 0.21)
    
        """
        lasso_lars = LassoLarsIC(copy_X=copy_X, precompute=False)
        rng = np.random.RandomState(0)
        X = rng.normal(0, 1, (100, 5))
        X_copy = X.copy()
        y = X[:, 2]
>       lasso_lars.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:721: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC(precompute=False)
X = array([[ 1.86700896e+00,  4.17030806e-01,  1.03715066e+00,
         2.33516718e+00,  1.72181332e+00],
       [-8.74321...2.55047938e+00],
       [ 2.90319447e-02, -6.41679369e-01, -4.55821293e-01,
        -9.23767890e-01, -2.23599427e-01]])
y = array([ 1.03715066, -0.09294453,  0.8194504 , -0.14674559,  0.92284887,
       -0.12877118, -0.82937307,  1.26079252, ...302861, -0.38077685, -0.73470469, -0.21725786,
       -0.82400615, -0.85035057,  0.61154474,  1.15869701, -0.45582129])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
____________________ test_lasso_lars_copyX_behaviour[False] ____________________

copy_X = False

    @pytest.mark.parametrize("copy_X", [True, False])
    def test_lasso_lars_copyX_behaviour(copy_X):
        """
        Test that user input regarding copy_X is not being overridden (it was until
        at least version 0.21)
    
        """
        lasso_lars = LassoLarsIC(copy_X=copy_X, precompute=False)
        rng = np.random.RandomState(0)
        X = rng.normal(0, 1, (100, 5))
        X_copy = X.copy()
        y = X[:, 2]
>       lasso_lars.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:721: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC(copy_X=False, precompute=False)
X = array([[ 1.86700896e+00,  4.17030806e-01,  1.03715066e+00,
         2.33516718e+00,  1.72181332e+00],
       [-8.74321...2.55047938e+00],
       [ 2.90319447e-02, -6.41679369e-01, -4.55821293e-01,
        -9.23767890e-01, -2.23599427e-01]])
y = array([ 1.03715066, -0.09294453,  0.8194504 , -0.14674559,  0.92284887,
       -0.12877118, -0.82937307,  1.26079252, ...302861, -0.38077685, -0.73470469, -0.21725786,
       -0.82400615, -0.85035057,  0.61154474,  1.15869701, -0.45582129])
copy_X = False

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
__________________ test_lasso_lars_fit_copyX_behaviour[True] ___________________

copy_X = True

    @pytest.mark.parametrize("copy_X", [True, False])
    def test_lasso_lars_fit_copyX_behaviour(copy_X):
        """
        Test that user input to .fit for copy_X overrides default __init__ value
    
        """
        lasso_lars = LassoLarsIC(precompute=False)
        rng = np.random.RandomState(0)
        X = rng.normal(0, 1, (100, 5))
        X_copy = X.copy()
        y = X[:, 2]
>       lasso_lars.fit(X, y, copy_X=copy_X)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:736: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC(precompute=False)
X = array([[ 1.86700896e+00,  4.17030806e-01,  1.03715066e+00,
         2.33516718e+00,  1.72181332e+00],
       [-8.74321...2.55047938e+00],
       [ 2.90319447e-02, -6.41679369e-01, -4.55821293e-01,
        -9.23767890e-01, -2.23599427e-01]])
y = array([ 1.03715066, -0.09294453,  0.8194504 , -0.14674559,  0.92284887,
       -0.12877118, -0.82937307,  1.26079252, ...302861, -0.38077685, -0.73470469, -0.21725786,
       -0.82400615, -0.85035057,  0.61154474,  1.15869701, -0.45582129])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
__________________ test_lasso_lars_fit_copyX_behaviour[False] __________________

copy_X = False

    @pytest.mark.parametrize("copy_X", [True, False])
    def test_lasso_lars_fit_copyX_behaviour(copy_X):
        """
        Test that user input to .fit for copy_X overrides default __init__ value
    
        """
        lasso_lars = LassoLarsIC(precompute=False)
        rng = np.random.RandomState(0)
        X = rng.normal(0, 1, (100, 5))
        X_copy = X.copy()
        y = X[:, 2]
>       lasso_lars.fit(X, y, copy_X=copy_X)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:736: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC(precompute=False)
X = array([[ 1.86700896e+00,  4.17030806e-01,  1.03715066e+00,
         2.33516718e+00,  1.72181332e+00],
       [-8.74321...2.55047938e+00],
       [ 2.90319447e-02, -6.41679369e-01, -4.55821293e-01,
        -9.23767890e-01, -2.23599427e-01]])
y = array([ 1.03715066, -0.09294453,  0.8194504 , -0.14674559,  0.92284887,
       -0.12877118, -0.82937307,  1.26079252, ...302861, -0.38077685, -0.73470469, -0.21725786,
       -0.82400615, -0.85035057,  0.61154474,  1.15869701, -0.45582129])
copy_X = False

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
_________________________ test_lars_with_jitter[est0] __________________________

est = LassoLars(alpha=0.001, fit_intercept=False)

    @pytest.mark.parametrize("est", (LassoLars(alpha=1e-3), Lars()))
    def test_lars_with_jitter(est):
        # Test that a small amount of jitter helps stability,
        # using example provided in issue #2746
    
        X = np.array([[0.0, 0.0, 0.0, -1.0, 0.0], [0.0, -1.0, 0.0, 0.0, 0.0]])
        y = [-2.5, -2.5]
        expected_coef = [0, 2.5, 0, 2.5, 0]
    
        # set to fit_intercept to False since target is constant and we want check
        # the value of coef. coef would be all zeros otherwise.
        est.set_params(fit_intercept=False)
        est_jitter = clone(est).set_params(jitter=10e-8, random_state=0)
    
>       est.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:754: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars(alpha=0.001, fit_intercept=False)
X = array([[ 0.,  0.,  0., -1.,  0.],
       [ 0., -1.,  0.,  0.,  0.]])
y = array([[-2.5],
       [-2.5]]), max_iter = 500, alpha = 0.001
fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_________________________ test_lars_with_jitter[est1] __________________________

est = Lars(fit_intercept=False)

    @pytest.mark.parametrize("est", (LassoLars(alpha=1e-3), Lars()))
    def test_lars_with_jitter(est):
        # Test that a small amount of jitter helps stability,
        # using example provided in issue #2746
    
        X = np.array([[0.0, 0.0, 0.0, -1.0, 0.0], [0.0, -1.0, 0.0, 0.0, 0.0]])
        y = [-2.5, -2.5]
        expected_coef = [0, 2.5, 0, 2.5, 0]
    
        # set to fit_intercept to False since target is constant and we want check
        # the value of coef. coef would be all zeros otherwise.
        est.set_params(fit_intercept=False)
        est_jitter = clone(est).set_params(jitter=10e-8, random_state=0)
    
>       est.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:754: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Lars(fit_intercept=False)
X = array([[ 0.,  0.,  0., -1.,  0.],
       [ 0., -1.,  0.,  0.,  0.]])
y = array([[-2.5],
       [-2.5]]), max_iter = 500, alpha = 0.0, fit_path = True
Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
__________________________ test_X_none_gram_not_none ___________________________

    def test_X_none_gram_not_none():
        with pytest.raises(ValueError, match="X cannot be None if Gram is not None"):
>           lars_path(X=None, y=np.array([1]), Gram=True)
E           Failed: DID NOT RAISE <class 'ValueError'>

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:763: Failed
________________ test_lars_dtype_match[float32-Lars-True-args0] ________________

LARS = <class 'sklearn.linear_model._least_angle.Lars'>, has_coef_path = True
args = {}, dtype = <class 'numpy.float32'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Lars()
X = array([[ 0.04650927,  0.20447403,  0.01966864,  0.1009531 , -0.05128083,
         0.17952287],
       [-0.06471705,  0...893429],
       [ 0.31149358, -0.11420956,  0.2980085 ,  0.13734281,  0.40679976,
         0.22616035]], dtype=float32)
y = array([[ 0.221237  ],
       [-0.00269294],
       [ 0.45206636],
       [ 0.13997293],
       [-0.08016226],
       [... [ 0.08685547],
       [ 0.07030797],
       [ 0.14918351],
       [ 0.14808595],
       [-0.07259884]], dtype=float32)
max_iter = 500, alpha = 0.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_____________ test_lars_dtype_match[float32-LassoLars-True-args1] ______________

LARS = <class 'sklearn.linear_model._least_angle.LassoLars'>
has_coef_path = True, args = {}, dtype = <class 'numpy.float32'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars()
X = array([[ 0.04650927,  0.20447403,  0.01966864,  0.1009531 , -0.05128083,
         0.17952287],
       [-0.06471705,  0...893429],
       [ 0.31149358, -0.11420956,  0.2980085 ,  0.13734281,  0.40679976,
         0.22616035]], dtype=float32)
y = array([[ 0.221237  ],
       [-0.00269294],
       [ 0.45206636],
       [ 0.13997293],
       [-0.08016226],
       [... [ 0.08685547],
       [ 0.07030797],
       [ 0.14918351],
       [ 0.14808595],
       [-0.07259884]], dtype=float32)
max_iter = 500, alpha = 1.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
____________ test_lars_dtype_match[float32-LassoLarsIC-False-args2] ____________

LARS = <class 'sklearn.linear_model._least_angle.LassoLarsIC'>
has_coef_path = False, args = {}, dtype = <class 'numpy.float32'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC()
X = array([[ 0.04650927,  0.20447403,  0.01966864,  0.1009531 , -0.05128083,
         0.17952287],
       [-0.06471705,  0...893429],
       [ 0.31149358, -0.11420956,  0.2980085 ,  0.13734281,  0.40679976,
         0.22616035]], dtype=float32)
y = array([ 0.221237  , -0.00269294,  0.45206636,  0.13997293, -0.08016226,
        0.10237592, -0.4848241 , -0.20244247, ...-0.20573497,  0.06594759,
        0.08685547,  0.07030797,  0.14918351,  0.14808595, -0.07259884],
      dtype=float32)
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
_______________ test_lars_dtype_match[float32-LarsCV-True-args3] _______________

LARS = <class 'sklearn.linear_model._least_angle.LarsCV'>, has_coef_path = True
args = {}, dtype = <class 'numpy.float32'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-0.3638183 ,  0.21418753, -0.42201   ,  0.5031696 ,  0.03424615,
         0.00594401],
       [-0.2175371 ,  0...128098],
       [ 0.33170512, -0.02922773,  0.31573993,  0.1397736 ,  0.3941332 ,
         0.28381366]], dtype=float32)
y_train = array([-0.02950081,  0.15303737, -0.43416265, -0.15178102,  0.20681769,
       -0.16327825,  0.16465956, -0.02458715, ...0.15507352,
        0.11660904,  0.13751692,  0.12096941,  0.19984496,  0.1987474 ,
       -0.0219374 ], dtype=float32)
X_test = array([[ 0.06672081,  0.28945586,  0.03740007,  0.1033839 , -0.06394738,
         0.23717618],
       [-0.04450551,  0...390192],
       [ 0.29606405,  0.4442787 ,  0.41325504,  0.35765928, -0.02612281,
         0.37181127]], dtype=float32)
y_test = array([0.27189845, 0.04796851, 0.5027278 , 0.19063437], dtype=float32)
Gram = 'auto', copy = False, method = 'lar', verbose = 0, fit_intercept = True
max_iter = 500, eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
____________ test_lars_dtype_match[float32-LassoLarsCV-True-args4] _____________

LARS = <class 'sklearn.linear_model._least_angle.LassoLarsCV'>
has_coef_path = True, args = {'max_iter': 5}, dtype = <class 'numpy.float32'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-0.3638183 ,  0.21418753, -0.42201   ,  0.5031696 ,  0.03424615,
         0.00594401],
       [-0.2175371 ,  0...128098],
       [ 0.33170512, -0.02922773,  0.31573993,  0.1397736 ,  0.3941332 ,
         0.28381366]], dtype=float32)
y_train = array([-0.02950081,  0.15303737, -0.43416265, -0.15178102,  0.20681769,
       -0.16327825,  0.16465956, -0.02458715, ...0.15507352,
        0.11660904,  0.13751692,  0.12096941,  0.19984496,  0.1987474 ,
       -0.0219374 ], dtype=float32)
X_test = array([[ 0.06672081,  0.28945586,  0.03740007,  0.1033839 , -0.06394738,
         0.23717618],
       [-0.04450551,  0...390192],
       [ 0.29606405,  0.4442787 ,  0.41325504,  0.35765928, -0.02612281,
         0.37181127]], dtype=float32)
y_test = array([0.27189845, 0.04796851, 0.5027278 , 0.19063437], dtype=float32)
Gram = 'auto', copy = False, method = 'lasso', verbose = 0, fit_intercept = True
max_iter = 5, eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
________________ test_lars_dtype_match[float64-Lars-True-args0] ________________

LARS = <class 'sklearn.linear_model._least_angle.Lars'>, has_coef_path = True
args = {}, dtype = <class 'numpy.float64'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Lars()
X = array([[ 0.04650925,  0.20447407,  0.01966873,  0.10095314, -0.05128084,
         0.17952285],
       [-0.06471704,  0...,
        -0.16893431],
       [ 0.31149357, -0.11420956,  0.29800855,  0.13734283,  0.40679972,
         0.22616033]])
y = array([[ 0.22123699],
       [-0.00269291],
       [ 0.45206634],
       [ 0.13997291],
       [-0.08016225],
       [...594762],
       [ 0.08685547],
       [ 0.07030795],
       [ 0.14918353],
       [ 0.14808598],
       [-0.07259886]])
max_iter = 500, alpha = 0.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_____________ test_lars_dtype_match[float64-LassoLars-True-args1] ______________

LARS = <class 'sklearn.linear_model._least_angle.LassoLars'>
has_coef_path = True, args = {}, dtype = <class 'numpy.float64'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars()
X = array([[ 0.04650925,  0.20447407,  0.01966873,  0.10095314, -0.05128084,
         0.17952285],
       [-0.06471704,  0...,
        -0.16893431],
       [ 0.31149357, -0.11420956,  0.29800855,  0.13734283,  0.40679972,
         0.22616033]])
y = array([[ 0.22123699],
       [-0.00269291],
       [ 0.45206634],
       [ 0.13997291],
       [-0.08016225],
       [...594762],
       [ 0.08685547],
       [ 0.07030795],
       [ 0.14918353],
       [ 0.14808598],
       [-0.07259886]])
max_iter = 500, alpha = 1.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
____________ test_lars_dtype_match[float64-LassoLarsIC-False-args2] ____________

LARS = <class 'sklearn.linear_model._least_angle.LassoLarsIC'>
has_coef_path = False, args = {}, dtype = <class 'numpy.float64'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC()
X = array([[ 0.04650925,  0.20447407,  0.01966873,  0.10095314, -0.05128084,
         0.17952285],
       [-0.06471704,  0...,
        -0.16893431],
       [ 0.31149357, -0.11420956,  0.29800855,  0.13734283,  0.40679972,
         0.22616033]])
y = array([ 0.22123699, -0.00269291,  0.45206634,  0.13997291, -0.08016225,
        0.10237592, -0.4848241 , -0.20244248, ...524859, -0.36854323, -0.20573497,  0.06594762,
        0.08685547,  0.07030795,  0.14918353,  0.14808598, -0.07259886])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
_______________ test_lars_dtype_match[float64-LarsCV-True-args3] _______________

LARS = <class 'sklearn.linear_model._least_angle.LarsCV'>, has_coef_path = True
args = {}, dtype = <class 'numpy.float64'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-0.36381826,  0.21418759, -0.42200999,  0.50316964,  0.03424612,
         0.00594399],
       [-0.21753707,  0...,
        -0.11128099],
       [ 0.33170513, -0.02922769,  0.31573992,  0.1397736 ,  0.39413316,
         0.28381364]])
y_train = array([-0.02950079,  0.15303738, -0.43416264, -0.15178102,  0.2068177 ,
       -0.16327823,  0.16465959, -0.02458714, -0.31788177, -0.15507351,
        0.11660907,  0.13751692,  0.12096941,  0.19984498,  0.19874743,
       -0.0219374 ])
X_test = array([[ 0.06672082,  0.28945594,  0.0374001 ,  0.10338391, -0.0639474 ,
         0.23717617],
       [-0.04450547,  0...,
         0.4239019 ],
       [ 0.29606406,  0.44427872,  0.41325507,  0.35765929, -0.02612284,
         0.37181123]])
y_test = array([0.27189844, 0.04796855, 0.5027278 , 0.19063436]), Gram = 'auto'
copy = False, method = 'lar', verbose = 0, fit_intercept = True, max_iter = 500
eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
____________ test_lars_dtype_match[float64-LassoLarsCV-True-args4] _____________

LARS = <class 'sklearn.linear_model._least_angle.LassoLarsCV'>
has_coef_path = True, args = {'max_iter': 5}, dtype = <class 'numpy.float64'>

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    @pytest.mark.parametrize("dtype", (np.float32, np.float64))
    def test_lars_dtype_match(LARS, has_coef_path, args, dtype):
        # The test ensures that the fit method preserves input dtype
        rng = np.random.RandomState(0)
        X = rng.rand(20, 6).astype(dtype)
        y = rng.rand(20).astype(dtype)
    
        model = LARS(**args)
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:798: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[-0.36381826,  0.21418759, -0.42200999,  0.50316964,  0.03424612,
         0.00594399],
       [-0.21753707,  0...,
        -0.11128099],
       [ 0.33170513, -0.02922769,  0.31573992,  0.1397736 ,  0.39413316,
         0.28381364]])
y_train = array([-0.02950079,  0.15303738, -0.43416264, -0.15178102,  0.2068177 ,
       -0.16327823,  0.16465959, -0.02458714, -0.31788177, -0.15507351,
        0.11660907,  0.13751692,  0.12096941,  0.19984498,  0.19874743,
       -0.0219374 ])
X_test = array([[ 0.06672082,  0.28945594,  0.0374001 ,  0.10338391, -0.0639474 ,
         0.23717617],
       [-0.04450547,  0...,
         0.4239019 ],
       [ 0.29606406,  0.44427872,  0.41325507,  0.35765929, -0.02612284,
         0.37181123]])
y_test = array([0.27189844, 0.04796855, 0.5027278 , 0.19063436]), Gram = 'auto'
copy = False, method = 'lasso', verbose = 0, fit_intercept = True, max_iter = 5
eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
________________ test_lars_numeric_consistency[Lars-True-args0] ________________

LARS = <class 'sklearn.linear_model._least_angle.Lars'>, has_coef_path = True
args = {}

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    def test_lars_numeric_consistency(LARS, has_coef_path, args):
        # The test ensures numerical consistency between trained coefficients
        # of float32 and float64.
        rtol = 1e-5
        atol = 1e-5
    
        rng = np.random.RandomState(0)
        X_64 = rng.rand(10, 6)
        y_64 = rng.rand(10)
    
>       model_64 = LARS(**args).fit(X_64, y_64)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:826: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = Lars()
X = array([[ 0.09386709,  0.1132988 , -0.00217703,  0.00767651, -0.00257428,
         0.1726277 ],
       [-0.0173592 ,  0...,
        -0.3712216 ],
       [-0.24606965, -0.44058104,  0.04816792, -0.28391507,  0.04008169,
        -0.22884082]])
y = array([[-0.18916068],
       [-0.23775512],
       [ 0.30819933],
       [-0.20994731],
       [-0.1515479 ],
       [ 0.02059491],
       [ 0.47286297],
       [-0.25102899],
       [ 0.48981465],
       [-0.25203185]])
max_iter = 500, alpha = 0.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
_____________ test_lars_numeric_consistency[LassoLars-True-args1] ______________

LARS = <class 'sklearn.linear_model._least_angle.LassoLars'>
has_coef_path = True, args = {}

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    def test_lars_numeric_consistency(LARS, has_coef_path, args):
        # The test ensures numerical consistency between trained coefficients
        # of float32 and float64.
        rtol = 1e-5
        atol = 1e-5
    
        rng = np.random.RandomState(0)
        X_64 = rng.rand(10, 6)
        y_64 = rng.rand(10)
    
>       model_64 = LARS(**args).fit(X_64, y_64)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:826: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:823: in fit
    self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path, Xy=Xy)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLars()
X = array([[ 0.09386709,  0.1132988 , -0.00217703,  0.00767651, -0.00257428,
         0.1726277 ],
       [-0.0173592 ,  0...,
        -0.3712216 ],
       [-0.24606965, -0.44058104,  0.04816792, -0.28391507,  0.04008169,
        -0.22884082]])
y = array([[-0.18916068],
       [-0.23775512],
       [ 0.30819933],
       [-0.20994731],
       [-0.1515479 ],
       [ 0.02059491],
       [ 0.47286297],
       [-0.25102899],
       [ 0.48981465],
       [-0.25203185]])
max_iter = 500, alpha = 1.0, fit_path = True, Xy = None

    def _fit(self, X, y, max_iter, alpha, fit_path, Xy=None):
        """Auxiliary method to fit the model using X, y as training data"""
        n_features = X.shape[1]
        X, y, X_offset, y_offset, X_scale = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=self.copy_X)
        if y.ndim == 1:
            y = y[:, np.newaxis]
        n_targets = y.shape[1]
        Gram = self._get_gram(self.precompute, X, y)
        self.alphas_ = []
        self.n_iter_ = []
        self.coef_ = np.empty((n_targets, n_features), dtype=X.dtype)
        if fit_path:
            self.active_ = []
            self.coef_path_ = []
            for k in range(n_targets):
                this_Xy = None if Xy is None else Xy[:, k]
>               alphas, active, coef_path, n_iter_ = lars_path(X, y[:, k], Gram=Gram, Xy=this_Xy, copy_X=self.copy_X, copy_Gram=True, alpha_min=alpha, method=self.method, verbose=max(0, self.verbose - 1), max_iter=max_iter, eps=self.eps, return_path=True, return_n_iter=True, positive=self.positive)
E               TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:770: TypeError
____________ test_lars_numeric_consistency[LassoLarsIC-False-args2] ____________

LARS = <class 'sklearn.linear_model._least_angle.LassoLarsIC'>
has_coef_path = False, args = {}

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    def test_lars_numeric_consistency(LARS, has_coef_path, args):
        # The test ensures numerical consistency between trained coefficients
        # of float32 and float64.
        rtol = 1e-5
        atol = 1e-5
    
        rng = np.random.RandomState(0)
        X_64 = rng.rand(10, 6)
        y_64 = rng.rand(10)
    
>       model_64 = LARS(**args).fit(X_64, y_64)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:826: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC()
X = array([[ 0.09386709,  0.1132988 , -0.00217703,  0.00767651, -0.00257428,
         0.1726277 ],
       [-0.0173592 ,  0...,
        -0.3712216 ],
       [-0.24606965, -0.44058104,  0.04816792, -0.28391507,  0.04008169,
        -0.22884082]])
y = array([-0.18916068, -0.23775512,  0.30819933, -0.20994731, -0.1515479 ,
        0.02059491,  0.47286297, -0.25102899,  0.48981465, -0.25203185])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
_______________ test_lars_numeric_consistency[LarsCV-True-args3] _______________

LARS = <class 'sklearn.linear_model._least_angle.LarsCV'>, has_coef_path = True
args = {}

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    def test_lars_numeric_consistency(LARS, has_coef_path, args):
        # The test ensures numerical consistency between trained coefficients
        # of float32 and float64.
        rtol = 1e-5
        atol = 1e-5
    
        rng = np.random.RandomState(0)
        X_64 = rng.rand(10, 6)
        y_64 = rng.rand(10)
    
>       model_64 = LARS(**args).fit(X_64, y_64)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:826: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[ 0.12266164,  0.37410373, -0.48933618, -0.46833845, -0.36064547,
         0.38788546],
       [ 0.33277383,  0...,
        -0.34268958],
       [-0.23650617, -0.39018339,  0.09273609, -0.30217615,  0.0854469 ,
        -0.2003088 ]])
y_train = array([ 0.25483485, -0.26331179, -0.20491238, -0.03276957,  0.41949849,
       -0.30439346,  0.43645017, -0.30539633])
X_test = array([[ 0.10343058,  0.16369646,  0.04239114, -0.01058457,  0.04279093,
         0.20115972],
       [-0.00779571,  0.34028009,  0.40329052, -0.17202623,  0.41086117,
         0.08416053]])
y_test = array([-0.24252515, -0.2911196 ]), Gram = 'auto', copy = False
method = 'lar', verbose = 0, fit_intercept = True, max_iter = 500
eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
____________ test_lars_numeric_consistency[LassoLarsCV-True-args4] _____________

LARS = <class 'sklearn.linear_model._least_angle.LassoLarsCV'>
has_coef_path = True, args = {'max_iter': 5}

    @pytest.mark.parametrize(
        "LARS, has_coef_path, args",
        (
            (Lars, True, {}),
            (LassoLars, True, {}),
            (LassoLarsIC, False, {}),
            (LarsCV, True, {}),
            # max_iter=5 is for avoiding ConvergenceWarning
            (LassoLarsCV, True, {"max_iter": 5}),
        ),
    )
    def test_lars_numeric_consistency(LARS, has_coef_path, args):
        # The test ensures numerical consistency between trained coefficients
        # of float32 and float64.
        rtol = 1e-5
        atol = 1e-5
    
        rng = np.random.RandomState(0)
        X_64 = rng.rand(10, 6)
        y_64 = rng.rand(10)
    
>       model_64 = LARS(**args).fit(X_64, y_64)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:826: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1268: in fit
    cv_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)((delayed(_lars_path_residues)(X[train], y[train], X[test], y[test], Gram=Gram, copy=False, method=self.method, verbose=max(0, self.verbose - 1), fit_intercept=self.fit_intercept, max_iter=self.max_iter, eps=self.eps, positive=self.positive) for train, test in cv.split(X, y, **routed_params.splitter.split)))
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:77: in __call__
    return super().__call__(iterable_with_config)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1918: in __call__
    return output if self.return_generator else list(output)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/joblib/parallel.py:1847: in _get_sequential_output
    res = func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/parallel.py:139: in __call__
    return self.function(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X_train = array([[ 0.12266164,  0.37410373, -0.48933618, -0.46833845, -0.36064547,
         0.38788546],
       [ 0.33277383,  0...,
        -0.34268958],
       [-0.23650617, -0.39018339,  0.09273609, -0.30217615,  0.0854469 ,
        -0.2003088 ]])
y_train = array([ 0.25483485, -0.26331179, -0.20491238, -0.03276957,  0.41949849,
       -0.30439346,  0.43645017, -0.30539633])
X_test = array([[ 0.10343058,  0.16369646,  0.04239114, -0.01058457,  0.04279093,
         0.20115972],
       [-0.00779571,  0.34028009,  0.40329052, -0.17202623,  0.41086117,
         0.08416053]])
y_test = array([-0.24252515, -0.2911196 ]), Gram = 'auto', copy = False
method = 'lasso', verbose = 0, fit_intercept = True, max_iter = 5
eps = np.float64(2.220446049250313e-16), positive = False

    def _lars_path_residues(X_train, y_train, X_test, y_test, Gram=None, copy=True, method='lar', verbose=False, fit_intercept=True, max_iter=500, eps=np.finfo(float).eps, positive=False):
        """Compute the residues on left-out data for a full LARS path
    
        Parameters
        -----------
        X_train : array-like of shape (n_samples, n_features)
            The data to fit the LARS on
    
        y_train : array-like of shape (n_samples,)
            The target variable to fit LARS on
    
        X_test : array-like of shape (n_samples, n_features)
            The data to compute the residues on
    
        y_test : array-like of shape (n_samples,)
            The target variable to compute the residues on
    
        Gram : None, 'auto' or array-like of shape (n_features, n_features),             default=None
            Precomputed Gram matrix (X' * X), if ``'auto'``, the Gram
            matrix is precomputed from the given X, if there are more samples
            than features
    
        copy : bool, default=True
            Whether X_train, X_test, y_train and y_test should be copied;
            if False, they may be overwritten.
    
        method : {'lar' , 'lasso'}, default='lar'
            Specifies the returned model. Select ``'lar'`` for Least Angle
            Regression, ``'lasso'`` for the Lasso.
    
        verbose : bool or int, default=False
            Sets the amount of verbosity
    
        fit_intercept : bool, default=True
            whether to calculate the intercept for this model. If set
            to false, no intercept will be used in calculations
            (i.e. data is expected to be centered).
    
        positive : bool, default=False
            Restrict coefficients to be >= 0. Be aware that you might want to
            remove fit_intercept which is set True by default.
            See reservations for using this option in combination with method
            'lasso' for expected small values of alpha in the doc of LassoLarsCV
            and LassoLarsIC.
    
        max_iter : int, default=500
            Maximum number of iterations to perform.
    
        eps : float, default=np.finfo(float).eps
            The machine-precision regularization in the computation of the
            Cholesky diagonal factors. Increase this for very ill-conditioned
            systems. Unlike the ``tol`` parameter in some iterative
            optimization-based algorithms, this parameter does not control
            the tolerance of the optimization.
    
        Returns
        --------
        alphas : array-like of shape (n_alphas,)
            Maximum of covariances (in absolute value) at each iteration.
            ``n_alphas`` is either ``max_iter`` or ``n_features``, whichever
            is smaller.
    
        active : list
            Indices of active variables at the end of the path.
    
        coefs : array-like of shape (n_features, n_alphas)
            Coefficients along the path
    
        residues : array-like of shape (n_alphas, n_samples)
            Residues of the prediction on the test data
        """
        X_train = _check_copy_and_writeable(X_train, copy)
        y_train = _check_copy_and_writeable(y_train, copy)
        X_test = _check_copy_and_writeable(X_test, copy)
        y_test = _check_copy_and_writeable(y_test, copy)
        if fit_intercept:
            X_mean = X_train.mean(axis=0)
            X_train -= X_mean
            X_test -= X_mean
            y_mean = y_train.mean(axis=0)
            y_train = as_float_array(y_train, copy=False)
            y_train -= y_mean
            y_test = as_float_array(y_test, copy=False)
            y_test -= y_mean
>       alphas, active, coefs = lars_path(X_train, y_train, Gram=Gram, copy_X=False, copy_Gram=False, method=method, verbose=max(0, verbose - 1), max_iter=max_iter, eps=eps, positive=positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1071: TypeError
____________________ test_lassolarsic_alpha_selection[aic] _____________________

criterion = 'aic'

    @pytest.mark.parametrize("criterion", ["aic", "bic"])
    def test_lassolarsic_alpha_selection(criterion):
        """Check that we properly compute the AIC and BIC score.
    
        In this test, we reproduce the example of the Fig. 2 of Zou et al.
        (reference [1] in LassoLarsIC) In this example, only 7 features should be
        selected.
        """
        model = make_pipeline(StandardScaler(), LassoLarsIC(criterion=criterion))
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:844: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/pipeline.py:660: in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC()
X = array([[ 0.80050009,  1.06548848,  1.29708846, ..., -0.05449919,
         0.41853093, -0.37098854],
       [-0.0395671...291, -0.54515416],
       [-0.9560041 , -0.93853666, -1.53537419, ..., -0.83030083,
        -0.08875225,  0.06442552]])
y = array([-1.13348416e+00, -7.71334842e+01, -1.11334842e+01,  5.38665158e+01,
       -1.71334842e+01, -5.51334842e+01, -1...1,
       -1.04133484e+02,  2.58665158e+01, -4.81334842e+01, -2.01334842e+01,
        6.78665158e+01, -9.51334842e+01])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
____________________ test_lassolarsic_alpha_selection[bic] _____________________

criterion = 'bic'

    @pytest.mark.parametrize("criterion", ["aic", "bic"])
    def test_lassolarsic_alpha_selection(criterion):
        """Check that we properly compute the AIC and BIC score.
    
        In this test, we reproduce the example of the Fig. 2 of Zou et al.
        (reference [1] in LassoLarsIC) In this example, only 7 features should be
        selected.
        """
        model = make_pipeline(StandardScaler(), LassoLarsIC(criterion=criterion))
>       model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:844: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/pipeline.py:660: in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC(criterion='bic')
X = array([[ 0.80050009,  1.06548848,  1.29708846, ..., -0.05449919,
         0.41853093, -0.37098854],
       [-0.0395671...291, -0.54515416],
       [-0.9560041 , -0.93853666, -1.53537419, ..., -0.83030083,
        -0.08875225,  0.06442552]])
y = array([-1.13348416e+00, -7.71334842e+01, -1.11334842e+01,  5.38665158e+01,
       -1.71334842e+01, -5.51334842e+01, -1...1,
       -1.04133484e+02,  2.58665158e+01, -4.81334842e+01, -2.01334842e+01,
        6.78665158e+01, -9.51334842e+01])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
____________________ test_lassolarsic_noise_variance[True] _____________________

fit_intercept = True

    @pytest.mark.parametrize("fit_intercept", [True, False])
    def test_lassolarsic_noise_variance(fit_intercept):
        """Check the behaviour when `n_samples` < `n_features` and that one needs
        to provide the noise variance."""
        rng = np.random.RandomState(0)
        X, y = datasets.make_regression(
            n_samples=10, n_features=11 - fit_intercept, random_state=rng
        )
    
        model = make_pipeline(StandardScaler(), LassoLarsIC(fit_intercept=fit_intercept))
    
        err_msg = (
            "You are using LassoLarsIC in the case where the number of samples is smaller"
            " than the number of features"
        )
        with pytest.raises(ValueError, match=err_msg):
>           model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:866: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/pipeline.py:660: in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC()
X = array([[ 0.35844738, -0.72344867, -0.07330407, -1.92879483, -1.549835  ,
         2.2351854 ,  1.85249839,  1.97697724...8451, -0.1071056 , -0.00769461,  0.82403755,
         0.5258317 , -0.28564024,  0.35139234,  0.87698311,  0.17233727]])
y = array([  19.54564417,    4.74224266, -101.05542195,  136.92498782,
        268.3047647 , -251.85618176, -253.7242718 , -129.02045005,
        142.50052965,  163.63815656])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
____________________ test_lassolarsic_noise_variance[False] ____________________

fit_intercept = False

    @pytest.mark.parametrize("fit_intercept", [True, False])
    def test_lassolarsic_noise_variance(fit_intercept):
        """Check the behaviour when `n_samples` < `n_features` and that one needs
        to provide the noise variance."""
        rng = np.random.RandomState(0)
        X, y = datasets.make_regression(
            n_samples=10, n_features=11 - fit_intercept, random_state=rng
        )
    
        model = make_pipeline(StandardScaler(), LassoLarsIC(fit_intercept=fit_intercept))
    
        err_msg = (
            "You are using LassoLarsIC in the case where the number of samples is smaller"
            " than the number of features"
        )
        with pytest.raises(ValueError, match=err_msg):
>           model.fit(X, y)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py:866: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/pipeline.py:660: in fit
    self._final_estimator.fit(Xt, y, **last_step_params["fit"])
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/base.py:1330: in wrapper
    return fit_method(estimator, *args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = LassoLarsIC(fit_intercept=False)
X = array([[ 1.2759014 ,  0.12060788,  0.63319395, -0.57903603, -0.44192759,
        -0.10371343,  0.51479877,  1.93513155....19148181,  0.65010004,
         1.58225728, -0.06190065, -1.15316942, -0.23966625,  0.31023749,
         1.76474898]])
y = array([  15.57168649, -126.45532766,  170.10566023, -250.81607899,
        299.27929028, -320.93552573,  320.63240457, -387.35713627,
        293.50399238,  467.95787387])
copy_X = True

    @_fit_context(prefer_skip_nested_validation=True)
    def fit(self, X, y, copy_X=None):
        """Fit the model using X, y as training data.
    
        Parameters
        ----------
        X : array-like of shape (n_samples, n_features)
            Training data.
    
        y : array-like of shape (n_samples,)
            Target values. Will be cast to X's dtype if necessary.
    
        copy_X : bool, default=None
            If provided, this parameter will override the choice
            of copy_X made at instance creation.
            If ``True``, X will be copied; else, it may be overwritten.
    
        Returns
        -------
        self : object
            Returns an instance of self.
        """
        if copy_X is None:
            copy_X = self.copy_X
        X, y = validate_data(self, X, y, force_writeable=True, y_numeric=True)
        X, y, Xmean, ymean, Xstd = _preprocess_data(X, y, fit_intercept=self.fit_intercept, copy=copy_X)
        Gram = self.precompute
>       alphas_, _, coef_path_, self.n_iter_ = lars_path(X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0, method='lasso', verbose=self.verbose, max_iter=self.max_iter, eps=self.eps, return_n_iter=True, positive=self.positive)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/_least_angle.py:1692: TypeError
=========================== short test summary info ============================
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple_precomputed
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lar]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lasso]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lar]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lasso]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_x_none_gram_none_raises_value_error
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_all_precomputed
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_lstsq
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_gives_lstsq_solution
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_collinearity
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_precomputed
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_all_precomputed
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[Lars]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LarsCV]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LassoLarsIC]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_singular_matrix
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_rank_deficient_design
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_early_stopping
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_path_length
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned2
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_add_features
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_n_nonzero_coefs
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_multitarget
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv_max_iter
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_ic
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_readonly_data
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_positive_constraint
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_estimatorclasses_positive_constraint
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_positive
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_R_implementation
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[True]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[False]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[True]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[False]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_X_none_gram_not_none
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-Lars-True-args0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLars-True-args1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsIC-False-args2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LarsCV-True-args3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsCV-True-args4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-Lars-True-args0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLars-True-args1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsIC-False-args2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LarsCV-True-args3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsCV-True-args4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[Lars-True-args0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLars-True-args1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsIC-False-args2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LarsCV-True-args3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsCV-True-args4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[aic]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[bic]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[True]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[False]
========================= 61 failed, 1 passed in 2.09s =========================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 62 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple I: Seeding RNGs with 1668722579
PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple_precomputed PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lar] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lasso] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lar] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lasso] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_x_none_gram_none_raises_value_error PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_all_precomputed PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_lstsq PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_gives_lstsq_solution PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_collinearity PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_precomputed PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_all_precomputed PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[Lars] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LarsCV] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LassoLarsIC] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_singular_matrix PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_rank_deficient_design PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_early_stopping PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_path_length PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned2 PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_add_features PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_n_nonzero_coefs PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_multitarget PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv_max_iter PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_ic PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_readonly_data PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_positive_constraint PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_estimatorclasses_positive_constraint PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_positive PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_R_implementation PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[True] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[False] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[True] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[False] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_X_none_gram_not_none PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_copy_X_with_auto_gram PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-Lars-True-args0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLars-True-args1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsIC-False-args2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LarsCV-True-args3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsCV-True-args4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-Lars-True-args0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLars-True-args1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsIC-False-args2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LarsCV-True-args3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsCV-True-args4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[Lars-True-args0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLars-True-args1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsIC-False-args2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LarsCV-True-args3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsCV-True-args4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[aic] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[bic] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[True] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[False] PASSED

============================== 62 passed in 0.57s ==============================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 62 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple I: Seeding RNGs with 888721807
PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_simple_precomputed PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lar] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[True-lasso] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lar] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_gram_equivalent[False-lasso] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_x_none_gram_none_raises_value_error PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_all_precomputed PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_lstsq PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_gives_lstsq_solution PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_collinearity PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_precomputed PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_no_path_all_precomputed PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[Lars] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LarsCV] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_precompute[LassoLarsIC] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_singular_matrix PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_rank_deficient_design PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_early_stopping PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_path_length PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_ill_conditioned2 PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_add_features PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_n_nonzero_coefs PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_multitarget PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_cv_max_iter PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_ic PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_readonly_data PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_path_positive_constraint PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_estimatorclasses_positive_constraint PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_lasso_cd_positive PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_vs_R_implementation PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[True] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_copyX_behaviour[False] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[True] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lasso_lars_fit_copyX_behaviour[False] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_with_jitter[est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_X_none_gram_not_none PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_copy_X_with_auto_gram PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-Lars-True-args0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLars-True-args1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsIC-False-args2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LarsCV-True-args3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float32-LassoLarsCV-True-args4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-Lars-True-args0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLars-True-args1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsIC-False-args2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LarsCV-True-args3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_dtype_match[float64-LassoLarsCV-True-args4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[Lars-True-args0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLars-True-args1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsIC-False-args2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LarsCV-True-args3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lars_numeric_consistency[LassoLarsCV-True-args4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[aic] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_alpha_selection[bic] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[True] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py::test_lassolarsic_noise_variance[False] PASSED

============================== 62 passed in 0.57s ==============================
