output file:
processed_scikit-learn_grid_from_X109.json
function:
_grid_from_X
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-brute-data5] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-DecisionTreeRegressor-brute-data6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-auto-data4]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-MultiTaskLasso-brute-data11] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data9] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LogisticRegression-data1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LinearRegression-data0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data8]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-DecisionTreeRegressor-brute-data6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-auto-data4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-brute-data5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_pipeline', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-auto-data4] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data9] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-brute-data5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-MultiTaskLasso-brute-data11] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_of_fitted_estimator FAILED', "FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles3-'percentiles' values must be in \\\\[0, 1\\\\]]", 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data10] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data8] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data7]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data10]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-brute-data5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-brute-data5]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[2-percentiles0-percentiles are too close] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-MultiTaskLasso-brute-data11] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data7]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles5-percentiles\\\\[0\\\\] must be strictly less than] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data7] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data9] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data8] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data8]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-MultiTaskLasso-brute-data11] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-DecisionTreeRegressor-brute-data6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-brute-data5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-brute-data5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-DecisionTreeRegressor-brute-data6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-DecisionTreeRegressor-brute-data6]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-DecisionTreeRegressor-brute-data6] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data7]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-brute-data5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-brute-data5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-DecisionTreeRegressor-brute-data6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-DecisionTreeRegressor-brute-data6]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-MultiTaskLasso-brute-data11] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-brute-data5] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-brute-data5] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LinearRegression-data0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-MultiTaskLasso-brute-data11]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-auto-data4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LogisticRegression-data1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-MultiTaskLasso-brute-data11]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data9]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data9]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_warning_recursion_non_constant_init', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-MultiTaskLasso-brute-data11]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data7]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-DecisionTreeRegressor-brute-data6]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LogisticRegression-data1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-DecisionTreeRegressor-brute-data6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LogisticRegression-data1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_hist_gbdt_sw_not_supported FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data7] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LinearRegression-data0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-brute-data5] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-auto-data4]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data8]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data7] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_mixed_type_categorical', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data8] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data8] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data7]', "FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-12345-'percentiles' must be a sequence of 2 elements]", 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-MultiTaskLasso-brute-data11]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-brute-data5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data10]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data8] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-DecisionTreeRegressor-brute-data6]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data8] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data8]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data9] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data7] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-brute-data5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data7] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-DecisionTreeRegressor-brute-data6] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data3] FAILED', "../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles3-'percentiles' values must be in \\\\[0, 1\\\\]] FAILED", 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-brute-data5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-MultiTaskLasso-brute-data11]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data9]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data7] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data7] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data9]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data7]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data10] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data8]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data7]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-MultiTaskLasso-brute-data11]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-brute-data5]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data7] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data8]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-DecisionTreeRegressor-brute-data6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-MultiTaskLasso-brute-data11]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data7]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data8]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data10] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-DecisionTreeRegressor-brute-data6] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est1] FAILED', "../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-12345-'percentiles' must be a sequence of 2 elements] FAILED", 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-auto-data4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-auto-data4]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data9]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LogisticRegression-data1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est1]', "FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles1-'percentiles' must be a sequence of 2 elements]", '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data9] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-MultiTaskLasso-brute-data11]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data8]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data9] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data9] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data8] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator3] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-brute-data5] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-auto-data4]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data9] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-brute-data5]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-MultiTaskLasso-brute-data11] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_pipeline FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data8]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-auto-data4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[2-percentiles0-percentiles are too close]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-DecisionTreeRegressor-brute-data6] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data9] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-auto-data4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-DecisionTreeRegressor-brute-data6] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-MultiTaskLasso-brute-data11] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-DecisionTreeRegressor-brute-data6]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data7] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-auto-data4]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data9] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data10]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LogisticRegression-data1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-auto-data4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-MultiTaskLasso-brute-data11]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-MultiTaskLasso-brute-data11]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data7]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data8] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data7]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data10]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data3] FAILED', "FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles4-'percentiles' values must be in \\\\[0, 1\\\\]]", 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LinearRegression-data0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-MultiTaskLasso-brute-data11] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-DecisionTreeRegressor-brute-data6]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data7] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data1]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-DecisionTreeRegressor-brute-data6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-DecisionTreeRegressor-brute-data6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-DecisionTreeRegressor-brute-data6] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data7] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-brute-data5]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data8]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-DecisionTreeRegressor-brute-data6]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_of_fitted_estimator', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data7]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data10] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data7] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data9]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-DecisionTreeRegressor-brute-data6]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data10] FAILED', "../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles1-'percentiles' must be a sequence of 2 elements] FAILED", 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data10]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data8] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-brute-data5] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-DecisionTreeRegressor-brute-data6] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data2]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data9]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data9]', 'FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-auto-data4] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-MultiTaskLasso-brute-data11] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data9]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data8] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data9] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data3]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est0] FAILED', "FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[1-percentiles6-'grid_resolution' must be strictly greater than 1]", '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-brute-data5] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data2]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data2] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data8] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data3]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-brute-data5]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-MultiTaskLasso-brute-data11] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data7]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data10]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data10] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data8]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data9]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-MultiTaskLasso-brute-data11]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-auto-data4] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_warning_recursion_non_constant_init FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_mixed_type_categorical FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-MultiTaskLasso-brute-data11] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-auto-data4] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-auto-data4]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-brute-data5]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_hist_gbdt_sw_not_supported', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data9]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator3] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est1] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data1] FAILED', "../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles4-'percentiles' values must be in \\\\[0, 1\\\\]] FAILED", '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data8] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-brute-data5] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LinearRegression-data0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles5-percentiles\\\\[0\\\\] must be strictly less than]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-MultiTaskLasso-brute-data11]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LinearRegression-data0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data9] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data9]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data10]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X FAILED', "../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[1-percentiles6-'grid_resolution' must be strictly greater than 1] FAILED", '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data8]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-MultiTaskLasso-brute-data11] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data10]'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 269 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data0] I: Seeding RNGs with 2101779282
FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-auto-data4] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-brute-data5] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-DecisionTreeRegressor-brute-data6] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data7] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data8] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data9] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data10] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-MultiTaskLasso-brute-data11] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_with_categorical[2] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_with_categorical[100] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_heterogeneous_type[3] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_heterogeneous_type[100] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[2-percentiles0-percentiles are too close] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles1-'percentiles' must be a sequence of 2 elements] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-12345-'percentiles' must be a sequence of 2 elements] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles3-'percentiles' values must be in \\[0, 1\\]] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles4-'percentiles' values must be in \\[0, 1\\]] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles5-percentiles\\[0\\] must be strictly less than] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[1-percentiles6-'grid_resolution' must be strictly greater than 1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_tree_vs_forest_and_gbdt[0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[DecisionTreeClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[ExtraTreeClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[ExtraTreesClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[KNeighborsClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[RadiusNeighborsClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[RandomForestClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator0-params0-'estimator' must be a fitted regressor or classifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator1-params1-The response_method parameter is ignored for regressors] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator2-params2-'recursion' method, the response_method must be 'decision_function'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator3-params3-'recursion' method, the response_method must be 'decision_function'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator4-params4-The 'recursion' method only applies when 'kind' is set to 'average'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator5-params5-The 'recursion' method only applies when 'kind' is set to 'average'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator6-params6-Only the following estimators support the 'recursion' method:] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[-1-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[-1-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[10000-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[10000-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_string[estimator0] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_string[estimator1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_warning_recursion_non_constant_init FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_of_fitted_estimator FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_hist_gbdt_sw_not_supported FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_pipeline FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-None-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-None-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-passthrough-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-passthrough-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-None-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-None-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-passthrough-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-passthrough-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[scalar-int] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[scalar-str] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[list-int] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[list-str] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[mask] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LinearRegression-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LogisticRegression-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LinearRegression-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LogisticRegression-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator2] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator3] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LinearRegression-data0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LogisticRegression-data1] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_size_error PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_with_recursion PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_mixed_type_categorical FAILED

=================================== FAILURES ===================================
_ test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data0] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data1] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data2] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data3] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-5-GradientBoostingRegressor-auto-data4] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-5-GradientBoostingRegressor-brute-data5] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[average-features0-5-DecisionTreeRegressor-brute-data6] ___

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features0-5-LinearRegression-brute-data7] ______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features0-5-LinearRegression-brute-data8] ______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[average-features0-5-LogisticRegression-brute-data9] _____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[average-features0-5-LogisticRegression-brute-data10] ____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
______ test_output_shape[average-features0-5-MultiTaskLasso-brute-data11] ______

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data0] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data1] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data2] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data3] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-10-GradientBoostingRegressor-auto-data4] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features0-10-GradientBoostingRegressor-brute-data5] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[average-features0-10-DecisionTreeRegressor-brute-data6] ___

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features0-10-LinearRegression-brute-data7] _____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features0-10-LinearRegression-brute-data8] _____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[average-features0-10-LogisticRegression-brute-data9] ____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[average-features0-10-LogisticRegression-brute-data10] ____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features0-10-MultiTaskLasso-brute-data11] ______

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data0] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data1] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data2] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data3] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-5-GradientBoostingRegressor-auto-data4] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-5-GradientBoostingRegressor-brute-data5] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[average-features1-5-DecisionTreeRegressor-brute-data6] ___

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features1-5-LinearRegression-brute-data7] ______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features1-5-LinearRegression-brute-data8] ______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[average-features1-5-LogisticRegression-brute-data9] _____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[average-features1-5-LogisticRegression-brute-data10] ____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
______ test_output_shape[average-features1-5-MultiTaskLasso-brute-data11] ______

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data0] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data1] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data2] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data3] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-10-GradientBoostingRegressor-auto-data4] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[average-features1-10-GradientBoostingRegressor-brute-data5] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[average-features1-10-DecisionTreeRegressor-brute-data6] ___

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features1-10-LinearRegression-brute-data7] _____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features1-10-LinearRegression-brute-data8] _____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[average-features1-10-LogisticRegression-brute-data9] ____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[average-features1-10-LogisticRegression-brute-data10] ____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[average-features1-10-MultiTaskLasso-brute-data11] ______

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1, 2], kind = 'average'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data0] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data1] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data2] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data3] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-5-GradientBoostingRegressor-auto-data4] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-5-GradientBoostingRegressor-brute-data5] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-5-DecisionTreeRegressor-brute-data6] __

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[individual-features0-5-LinearRegression-brute-data7] ____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[individual-features0-5-LinearRegression-brute-data8] ____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[individual-features0-5-LogisticRegression-brute-data9] ___

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[individual-features0-5-LogisticRegression-brute-data10] ___

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[individual-features0-5-MultiTaskLasso-brute-data11] _____

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data0] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data1] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data2] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data3] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-10-GradientBoostingRegressor-auto-data4] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-10-GradientBoostingRegressor-brute-data5] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features0-10-DecisionTreeRegressor-brute-data6] _

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[individual-features0-10-LinearRegression-brute-data7] ____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[individual-features0-10-LinearRegression-brute-data8] ____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[individual-features0-10-LogisticRegression-brute-data9] ___

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[individual-features0-10-LogisticRegression-brute-data10] __

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[individual-features0-10-MultiTaskLasso-brute-data11] ____

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data0] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data1] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data2] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data3] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-5-GradientBoostingRegressor-auto-data4] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-5-GradientBoostingRegressor-brute-data5] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-5-DecisionTreeRegressor-brute-data6] __

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[individual-features1-5-LinearRegression-brute-data7] ____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[individual-features1-5-LinearRegression-brute-data8] ____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[individual-features1-5-LogisticRegression-brute-data9] ___

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[individual-features1-5-LogisticRegression-brute-data10] ___

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[individual-features1-5-MultiTaskLasso-brute-data11] _____

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data0] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data1] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data2] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data3] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-10-GradientBoostingRegressor-auto-data4] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-10-GradientBoostingRegressor-brute-data5] _

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[individual-features1-10-DecisionTreeRegressor-brute-data6] _

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[individual-features1-10-LinearRegression-brute-data7] ____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[individual-features1-10-LinearRegression-brute-data8] ____

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[individual-features1-10-LogisticRegression-brute-data9] ___

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[individual-features1-10-LogisticRegression-brute-data10] __

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[individual-features1-10-MultiTaskLasso-brute-data11] ____

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1, 2], kind = 'individual'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data0] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data1] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data2] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data3] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[both-features0-5-GradientBoostingRegressor-auto-data4] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-5-GradientBoostingRegressor-brute-data5] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[both-features0-5-DecisionTreeRegressor-brute-data6] _____

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______ test_output_shape[both-features0-5-LinearRegression-brute-data7] _______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______ test_output_shape[both-features0-5-LinearRegression-brute-data8] _______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
______ test_output_shape[both-features0-5-LogisticRegression-brute-data9] ______

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[both-features0-5-LogisticRegression-brute-data10] ______

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______ test_output_shape[both-features0-5-MultiTaskLasso-brute-data11] ________

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data0] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data1] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data2] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data3] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-10-GradientBoostingRegressor-auto-data4] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features0-10-GradientBoostingRegressor-brute-data5] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[both-features0-10-DecisionTreeRegressor-brute-data6] ____

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
______ test_output_shape[both-features0-10-LinearRegression-brute-data7] _______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
______ test_output_shape[both-features0-10-LinearRegression-brute-data8] _______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[both-features0-10-LogisticRegression-brute-data9] ______

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[both-features0-10-LogisticRegression-brute-data10] _____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______ test_output_shape[both-features0-10-MultiTaskLasso-brute-data11] _______

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data0] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data1] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data2] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data3] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___ test_output_shape[both-features1-5-GradientBoostingRegressor-auto-data4] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-5-GradientBoostingRegressor-brute-data5] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[both-features1-5-DecisionTreeRegressor-brute-data6] _____

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______ test_output_shape[both-features1-5-LinearRegression-brute-data7] _______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______ test_output_shape[both-features1-5-LinearRegression-brute-data8] _______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
______ test_output_shape[both-features1-5-LogisticRegression-brute-data9] ______

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[both-features1-5-LogisticRegression-brute-data10] ______

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______ test_output_shape[both-features1-5-MultiTaskLasso-brute-data11] ________

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 5, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data0] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data1] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'auto'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data2] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data3] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingClassifier'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(n_estimators=2)
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-10-GradientBoostingRegressor-auto-data4] ___

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'auto'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__ test_output_shape[both-features1-10-GradientBoostingRegressor-brute-data5] __

Estimator = <class 'sklearn.ensemble._gb.GradientBoostingRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=2)
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_output_shape[both-features1-10-DecisionTreeRegressor-brute-data6] ____

Estimator = <class 'sklearn.tree._classes.DecisionTreeRegressor'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
______ test_output_shape[both-features1-10-LinearRegression-brute-data7] _______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.00610...88965801,
       -286.14589783,  -45.74535974, -111.49496495,  135.425414  ,
       -106.87605291, -232.49244841])), 1)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.01728509,  0.71312032,  0.62532518, ...,  1.73676704,
         0.62444732, -0.5733674 ],
       [ 1.0061070...626, -1.60473853],
       [ 0.09152195, -0.03191108, -1.73173135, ..., -0.20697447,
        -0.79770276,  1.01473769]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
______ test_output_shape[both-features1-10-LinearRegression-brute-data8] _______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[both-features1-10-LogisticRegression-brute-data9] ______

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_output_shape[both-features1-10-LogisticRegression-brute-data10] _____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
method = 'brute'
data = ((array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03... 2, 0, 2, 2, 0,
       1, 0, 1, 0, 1, 2, 1, 0, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 1, 2, 0, 1,
       1, 1, 1, 0, 1, 2])), 3)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[-4.51303037e-01,  1.92793845e-02, -2.14166656e-01,
         1.84959125e+00, -4.99016638e-01, -3.06541176e-03,
... 7.45864065e-02,
        -8.58972388e-01, -8.61799502e-01, -1.35978073e+00,
        -4.24663302e-01,  1.31247037e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______ test_output_shape[both-features1-10-MultiTaskLasso-brute-data11] _______

Estimator = <class 'sklearn.linear_model._coordinate_descent.MultiTaskLasso'>
method = 'brute'
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)
grid_resolution = 10, features = [1, 2], kind = 'both'

    @pytest.mark.parametrize(
        "Estimator, method, data",
        [
            (GradientBoostingClassifier, "auto", binary_classification_data),
            (GradientBoostingClassifier, "auto", multiclass_classification_data),
            (GradientBoostingClassifier, "brute", binary_classification_data),
            (GradientBoostingClassifier, "brute", multiclass_classification_data),
            (GradientBoostingRegressor, "auto", regression_data),
            (GradientBoostingRegressor, "brute", regression_data),
            (DecisionTreeRegressor, "brute", regression_data),
            (LinearRegression, "brute", regression_data),
            (LinearRegression, "brute", multioutput_regression_data),
            (LogisticRegression, "brute", binary_classification_data),
            (LogisticRegression, "brute", multiclass_classification_data),
            (MultiTaskLasso, "brute", multioutput_regression_data),
        ],
    )
    @pytest.mark.parametrize("grid_resolution", (5, 10))
    @pytest.mark.parametrize("features", ([1], [1, 2]))
    @pytest.mark.parametrize("kind", ("average", "individual", "both"))
    def test_output_shape(Estimator, method, data, grid_resolution, features, kind):
        # Check that partial_dependence has consistent output shape for different
        # kinds of estimators:
        # - classifiers with binary and multiclass settings
        # - regressors
        # - multi-task regressors
    
        est = Estimator()
        if hasattr(est, "n_estimators"):
            est.set_params(n_estimators=2)  # speed-up computations
    
        # n_target corresponds to the number of classes (1 for binary classif) or
        # the number of tasks / outputs in multi task settings. It's equal to 1 for
        # classical regression_data.
        (X, y), n_targets = data
        n_instances = X.shape[0]
    
        est.fit(X, y)
>       result = partial_dependence(
            est,
            X=X,
            features=features,
            method=method,
            kind=kind,
            grid_resolution=grid_resolution,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:104: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = MultiTaskLasso()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______________________________ test_grid_from_X _______________________________

    def test_grid_from_X():
        # tests for _grid_from_X: sanity check for output, and for shapes.
    
        # Make sure that the grid is a cartesian product of the input (it will use
        # the unique values instead of the percentiles)
        percentiles = (0.05, 0.95)
        grid_resolution = 100
        is_categorical = [False, False]
        X = np.asarray([[1, 2], [3, 4]])
>       grid, axes = _grid_from_X(X, percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:142: TypeError
_______ test_grid_from_X_error[2-percentiles0-percentiles are too close] _______

grid_resolution = 2, percentiles = (0, 0.0001)
err_msg = 'percentiles are too close'

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:23: in _grid_from_X
    return _grid_from_X(X, percentiles, is_categorical, grid_resolution)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X = array([[1, 2],
       [3, 4]]), percentiles = (0, 0.0001)
is_categorical = [False], grid_resolution = 2

    def _grid_from_X(X, percentiles=(0.05, 0.95), is_categorical=None, grid_resolution=100):
        X = np.asarray(X)
        if is_categorical is None:
            is_categorical = [False] * X.shape[1]
        elif len(is_categorical) != X.shape[1]:
>           raise ValueError('Length of is_categorical must match the number of features in X.')
E           ValueError: Length of is_categorical must match the number of features in X.

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/temp.py:26: ValueError

During handling of the above exception, another exception occurred:

grid_resolution = 2, percentiles = (0, 0.0001)
err_msg = 'percentiles are too close'

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)
E           AssertionError: Regex pattern did not match.
E            Regex: 'percentiles are too close'
E            Input: 'Length of is_categorical must match the number of features in X.'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: AssertionError
_ test_grid_from_X_error[100-percentiles1-'percentiles' must be a sequence of 2 elements] _

grid_resolution = 100, percentiles = (1, 2, 3, 4)
err_msg = "'percentiles' must be a sequence of 2 elements"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:23: in _grid_from_X
    return _grid_from_X(X, percentiles, is_categorical, grid_resolution)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X = array([[1, 2],
       [3, 4]]), percentiles = (1, 2, 3, 4)
is_categorical = [False], grid_resolution = 100

    def _grid_from_X(X, percentiles=(0.05, 0.95), is_categorical=None, grid_resolution=100):
        X = np.asarray(X)
        if is_categorical is None:
            is_categorical = [False] * X.shape[1]
        elif len(is_categorical) != X.shape[1]:
>           raise ValueError('Length of is_categorical must match the number of features in X.')
E           ValueError: Length of is_categorical must match the number of features in X.

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/temp.py:26: ValueError

During handling of the above exception, another exception occurred:

grid_resolution = 100, percentiles = (1, 2, 3, 4)
err_msg = "'percentiles' must be a sequence of 2 elements"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)
E           AssertionError: Regex pattern did not match.
E            Regex: "'percentiles' must be a sequence of 2 elements"
E            Input: 'Length of is_categorical must match the number of features in X.'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: AssertionError
_ test_grid_from_X_error[100-12345-'percentiles' must be a sequence of 2 elements] _

grid_resolution = 100, percentiles = 12345
err_msg = "'percentiles' must be a sequence of 2 elements"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:23: in _grid_from_X
    return _grid_from_X(X, percentiles, is_categorical, grid_resolution)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X = array([[1, 2],
       [3, 4]]), percentiles = 12345
is_categorical = [False], grid_resolution = 100

    def _grid_from_X(X, percentiles=(0.05, 0.95), is_categorical=None, grid_resolution=100):
        X = np.asarray(X)
        if is_categorical is None:
            is_categorical = [False] * X.shape[1]
        elif len(is_categorical) != X.shape[1]:
>           raise ValueError('Length of is_categorical must match the number of features in X.')
E           ValueError: Length of is_categorical must match the number of features in X.

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/temp.py:26: ValueError

During handling of the above exception, another exception occurred:

grid_resolution = 100, percentiles = 12345
err_msg = "'percentiles' must be a sequence of 2 elements"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)
E           AssertionError: Regex pattern did not match.
E            Regex: "'percentiles' must be a sequence of 2 elements"
E            Input: 'Length of is_categorical must match the number of features in X.'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: AssertionError
_ test_grid_from_X_error[100-percentiles3-'percentiles' values must be in \\[0, 1\\]] _

grid_resolution = 100, percentiles = (-1, 0.95)
err_msg = "'percentiles' values must be in \\[0, 1\\]"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:23: in _grid_from_X
    return _grid_from_X(X, percentiles, is_categorical, grid_resolution)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X = array([[1, 2],
       [3, 4]]), percentiles = (-1, 0.95)
is_categorical = [False], grid_resolution = 100

    def _grid_from_X(X, percentiles=(0.05, 0.95), is_categorical=None, grid_resolution=100):
        X = np.asarray(X)
        if is_categorical is None:
            is_categorical = [False] * X.shape[1]
        elif len(is_categorical) != X.shape[1]:
>           raise ValueError('Length of is_categorical must match the number of features in X.')
E           ValueError: Length of is_categorical must match the number of features in X.

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/temp.py:26: ValueError

During handling of the above exception, another exception occurred:

grid_resolution = 100, percentiles = (-1, 0.95)
err_msg = "'percentiles' values must be in \\[0, 1\\]"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)
E           AssertionError: Regex pattern did not match.
E            Regex: "'percentiles' values must be in \\[0, 1\\]"
E            Input: 'Length of is_categorical must match the number of features in X.'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: AssertionError
_ test_grid_from_X_error[100-percentiles4-'percentiles' values must be in \\[0, 1\\]] _

grid_resolution = 100, percentiles = (0.05, 2)
err_msg = "'percentiles' values must be in \\[0, 1\\]"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:23: in _grid_from_X
    return _grid_from_X(X, percentiles, is_categorical, grid_resolution)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X = array([[1, 2],
       [3, 4]]), percentiles = (0.05, 2)
is_categorical = [False], grid_resolution = 100

    def _grid_from_X(X, percentiles=(0.05, 0.95), is_categorical=None, grid_resolution=100):
        X = np.asarray(X)
        if is_categorical is None:
            is_categorical = [False] * X.shape[1]
        elif len(is_categorical) != X.shape[1]:
>           raise ValueError('Length of is_categorical must match the number of features in X.')
E           ValueError: Length of is_categorical must match the number of features in X.

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/temp.py:26: ValueError

During handling of the above exception, another exception occurred:

grid_resolution = 100, percentiles = (0.05, 2)
err_msg = "'percentiles' values must be in \\[0, 1\\]"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)
E           AssertionError: Regex pattern did not match.
E            Regex: "'percentiles' values must be in \\[0, 1\\]"
E            Input: 'Length of is_categorical must match the number of features in X.'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: AssertionError
_ test_grid_from_X_error[100-percentiles5-percentiles\\[0\\] must be strictly less than] _

grid_resolution = 100, percentiles = (0.9, 0.1)
err_msg = 'percentiles\\[0\\] must be strictly less than'

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:23: in _grid_from_X
    return _grid_from_X(X, percentiles, is_categorical, grid_resolution)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X = array([[1, 2],
       [3, 4]]), percentiles = (0.9, 0.1)
is_categorical = [False], grid_resolution = 100

    def _grid_from_X(X, percentiles=(0.05, 0.95), is_categorical=None, grid_resolution=100):
        X = np.asarray(X)
        if is_categorical is None:
            is_categorical = [False] * X.shape[1]
        elif len(is_categorical) != X.shape[1]:
>           raise ValueError('Length of is_categorical must match the number of features in X.')
E           ValueError: Length of is_categorical must match the number of features in X.

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/temp.py:26: ValueError

During handling of the above exception, another exception occurred:

grid_resolution = 100, percentiles = (0.9, 0.1)
err_msg = 'percentiles\\[0\\] must be strictly less than'

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)
E           AssertionError: Regex pattern did not match.
E            Regex: 'percentiles\\[0\\] must be strictly less than'
E            Input: 'Length of is_categorical must match the number of features in X.'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: AssertionError
_ test_grid_from_X_error[1-percentiles6-'grid_resolution' must be strictly greater than 1] _

grid_resolution = 1, percentiles = (0.05, 0.95)
err_msg = "'grid_resolution' must be strictly greater than 1"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:23: in _grid_from_X
    return _grid_from_X(X, percentiles, is_categorical, grid_resolution)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

X = array([[1, 2],
       [3, 4]]), percentiles = (0.05, 0.95)
is_categorical = [False], grid_resolution = 1

    def _grid_from_X(X, percentiles=(0.05, 0.95), is_categorical=None, grid_resolution=100):
        X = np.asarray(X)
        if is_categorical is None:
            is_categorical = [False] * X.shape[1]
        elif len(is_categorical) != X.shape[1]:
>           raise ValueError('Length of is_categorical must match the number of features in X.')
E           ValueError: Length of is_categorical must match the number of features in X.

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/temp.py:26: ValueError

During handling of the above exception, another exception occurred:

grid_resolution = 1, percentiles = (0.05, 0.95)
err_msg = "'grid_resolution' must be strictly greater than 1"

    @pytest.mark.parametrize(
        "grid_resolution, percentiles, err_msg",
        [
            (2, (0, 0.0001), "percentiles are too close"),
            (100, (1, 2, 3, 4), "'percentiles' must be a sequence of 2 elements"),
            (100, 12345, "'percentiles' must be a sequence of 2 elements"),
            (100, (-1, 0.95), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.05, 2), r"'percentiles' values must be in \[0, 1\]"),
            (100, (0.9, 0.1), r"percentiles\[0\] must be strictly less than"),
            (1, (0.05, 0.95), "'grid_resolution' must be strictly greater than 1"),
        ],
    )
    def test_grid_from_X_error(grid_resolution, percentiles, err_msg):
        X = np.asarray([[1, 2], [3, 4]])
        is_categorical = [False]
        with pytest.raises(ValueError, match=err_msg):
>           _grid_from_X(X, percentiles, is_categorical, grid_resolution)
E           AssertionError: Regex pattern did not match.
E            Regex: "'grid_resolution' must be strictly greater than 1"
E            Input: 'Length of is_categorical must match the number of features in X.'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:239: AssertionError
___________________ test_recursion_decision_function[0-est0] ___________________

est = GradientBoostingClassifier(random_state=0), target_feature = 0

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [0]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[0-est1] ___________________

est = HistGradientBoostingClassifier(random_state=0), target_feature = 0

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [0]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[1-est0] ___________________

est = GradientBoostingClassifier(random_state=0), target_feature = 1

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[1-est1] ___________________

est = HistGradientBoostingClassifier(random_state=0), target_feature = 1

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[2-est0] ___________________

est = GradientBoostingClassifier(random_state=0), target_feature = 2

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[2-est1] ___________________

est = HistGradientBoostingClassifier(random_state=0), target_feature = 2

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[3-est0] ___________________

est = GradientBoostingClassifier(random_state=0), target_feature = 3

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[3-est1] ___________________

est = HistGradientBoostingClassifier(random_state=0), target_feature = 3

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[4-est0] ___________________

est = GradientBoostingClassifier(random_state=0), target_feature = 4

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [4]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[4-est1] ___________________

est = HistGradientBoostingClassifier(random_state=0), target_feature = 4

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [4]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[5-est0] ___________________

est = GradientBoostingClassifier(random_state=0), target_feature = 5

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [5]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_recursion_decision_function[5-est1] ___________________

est = HistGradientBoostingClassifier(random_state=0), target_feature = 5

    @pytest.mark.parametrize(
        "est",
        (
            GradientBoostingClassifier(random_state=0),
            HistGradientBoostingClassifier(random_state=0),
        ),
    )
    @pytest.mark.parametrize("target_feature", (0, 1, 2, 3, 4, 5))
    def test_recursion_decision_function(est, target_feature):
        # Make sure the recursion method (implicitly uses decision_function) has
        # the same result as using brute method with
        # response_method=decision_function
    
        X, y = make_classification(n_classes=2, n_clusters_per_class=1, random_state=1)
        assert np.mean(y) == 0.5  # make sure the init estimator predicts 0 anyway
    
        est = clone(est).fit(X, y)
    
>       preds_1 = partial_dependence(
            est,
            X,
            [target_feature],
            response_method="decision_function",
            method="recursion",
            kind="average",
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:388: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingClassifier(random_state=0)
X = array([[-0.58652394,  0.26686086,  1.6169496 , ...,  0.19915097,
        -1.23685338,  1.24328724],
       [ 0.1855356...673,  1.17899425],
       [ 0.94926809,  2.27449013,  1.37333246, ...,  0.71939066,
        -2.17071106, -1.6845077 ]])
features = [5]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________ test_partial_dependence_easy_target[1-est0] __________________

est = LinearRegression(), power = 1

    @pytest.mark.parametrize(
        "est",
        (
            LinearRegression(),
            GradientBoostingRegressor(random_state=0),
            HistGradientBoostingRegressor(
                random_state=0, min_samples_leaf=1, max_leaf_nodes=None, max_iter=1
            ),
            DecisionTreeRegressor(random_state=0),
        ),
    )
    @pytest.mark.parametrize("power", (1, 2))
    def test_partial_dependence_easy_target(est, power):
        # If the target y only depends on one feature in an obvious way (linear or
        # quadratic) then the partial dependence for that feature should reflect
        # it.
        # We here fit a linear regression_data model (with polynomial features if
        # needed) and compute r_squared to check that the partial dependence
        # correctly reflects the target.
    
        rng = np.random.RandomState(0)
        n_samples = 200
        target_variable = 2
        X = rng.normal(size=(n_samples, 5))
        y = X[:, target_variable] ** power
    
        est = clone(est).fit(X, y)
    
>       pdp = partial_dependence(
            est, features=[target_variable], X=X, grid_resolution=1000, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,
         2.24089320e+00,  1.86755799e+00],
       [-9.77277...2.89120505e-01],
       [ 4.12870820e-01, -1.98398897e-01,  9.41923003e-02,
        -1.14761094e+00, -3.58114075e-01]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________ test_partial_dependence_easy_target[1-est1] __________________

est = GradientBoostingRegressor(random_state=0), power = 1

    @pytest.mark.parametrize(
        "est",
        (
            LinearRegression(),
            GradientBoostingRegressor(random_state=0),
            HistGradientBoostingRegressor(
                random_state=0, min_samples_leaf=1, max_leaf_nodes=None, max_iter=1
            ),
            DecisionTreeRegressor(random_state=0),
        ),
    )
    @pytest.mark.parametrize("power", (1, 2))
    def test_partial_dependence_easy_target(est, power):
        # If the target y only depends on one feature in an obvious way (linear or
        # quadratic) then the partial dependence for that feature should reflect
        # it.
        # We here fit a linear regression_data model (with polynomial features if
        # needed) and compute r_squared to check that the partial dependence
        # correctly reflects the target.
    
        rng = np.random.RandomState(0)
        n_samples = 200
        target_variable = 2
        X = rng.normal(size=(n_samples, 5))
        y = X[:, target_variable] ** power
    
        est = clone(est).fit(X, y)
    
>       pdp = partial_dependence(
            est, features=[target_variable], X=X, grid_resolution=1000, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(random_state=0)
X = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,
         2.24089320e+00,  1.86755799e+00],
       [-9.77277...2.89120505e-01],
       [ 4.12870820e-01, -1.98398897e-01,  9.41923003e-02,
        -1.14761094e+00, -3.58114075e-01]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________ test_partial_dependence_easy_target[1-est2] __________________

est = HistGradientBoostingRegressor(max_iter=1, max_leaf_nodes=None,
                              min_samples_leaf=1, random_state=0)
power = 1

    @pytest.mark.parametrize(
        "est",
        (
            LinearRegression(),
            GradientBoostingRegressor(random_state=0),
            HistGradientBoostingRegressor(
                random_state=0, min_samples_leaf=1, max_leaf_nodes=None, max_iter=1
            ),
            DecisionTreeRegressor(random_state=0),
        ),
    )
    @pytest.mark.parametrize("power", (1, 2))
    def test_partial_dependence_easy_target(est, power):
        # If the target y only depends on one feature in an obvious way (linear or
        # quadratic) then the partial dependence for that feature should reflect
        # it.
        # We here fit a linear regression_data model (with polynomial features if
        # needed) and compute r_squared to check that the partial dependence
        # correctly reflects the target.
    
        rng = np.random.RandomState(0)
        n_samples = 200
        target_variable = 2
        X = rng.normal(size=(n_samples, 5))
        y = X[:, target_variable] ** power
    
        est = clone(est).fit(X, y)
    
>       pdp = partial_dependence(
            est, features=[target_variable], X=X, grid_resolution=1000, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingRegressor(max_iter=1, max_leaf_nodes=None,
                              min_samples_leaf=1, random_state=0)
X = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,
         2.24089320e+00,  1.86755799e+00],
       [-9.77277...2.89120505e-01],
       [ 4.12870820e-01, -1.98398897e-01,  9.41923003e-02,
        -1.14761094e+00, -3.58114075e-01]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________ test_partial_dependence_easy_target[1-est3] __________________

est = DecisionTreeRegressor(random_state=0), power = 1

    @pytest.mark.parametrize(
        "est",
        (
            LinearRegression(),
            GradientBoostingRegressor(random_state=0),
            HistGradientBoostingRegressor(
                random_state=0, min_samples_leaf=1, max_leaf_nodes=None, max_iter=1
            ),
            DecisionTreeRegressor(random_state=0),
        ),
    )
    @pytest.mark.parametrize("power", (1, 2))
    def test_partial_dependence_easy_target(est, power):
        # If the target y only depends on one feature in an obvious way (linear or
        # quadratic) then the partial dependence for that feature should reflect
        # it.
        # We here fit a linear regression_data model (with polynomial features if
        # needed) and compute r_squared to check that the partial dependence
        # correctly reflects the target.
    
        rng = np.random.RandomState(0)
        n_samples = 200
        target_variable = 2
        X = rng.normal(size=(n_samples, 5))
        y = X[:, target_variable] ** power
    
        est = clone(est).fit(X, y)
    
>       pdp = partial_dependence(
            est, features=[target_variable], X=X, grid_resolution=1000, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor(random_state=0)
X = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,
         2.24089320e+00,  1.86755799e+00],
       [-9.77277...2.89120505e-01],
       [ 4.12870820e-01, -1.98398897e-01,  9.41923003e-02,
        -1.14761094e+00, -3.58114075e-01]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________ test_partial_dependence_easy_target[2-est0] __________________

est = LinearRegression(), power = 2

    @pytest.mark.parametrize(
        "est",
        (
            LinearRegression(),
            GradientBoostingRegressor(random_state=0),
            HistGradientBoostingRegressor(
                random_state=0, min_samples_leaf=1, max_leaf_nodes=None, max_iter=1
            ),
            DecisionTreeRegressor(random_state=0),
        ),
    )
    @pytest.mark.parametrize("power", (1, 2))
    def test_partial_dependence_easy_target(est, power):
        # If the target y only depends on one feature in an obvious way (linear or
        # quadratic) then the partial dependence for that feature should reflect
        # it.
        # We here fit a linear regression_data model (with polynomial features if
        # needed) and compute r_squared to check that the partial dependence
        # correctly reflects the target.
    
        rng = np.random.RandomState(0)
        n_samples = 200
        target_variable = 2
        X = rng.normal(size=(n_samples, 5))
        y = X[:, target_variable] ** power
    
        est = clone(est).fit(X, y)
    
>       pdp = partial_dependence(
            est, features=[target_variable], X=X, grid_resolution=1000, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,
         2.24089320e+00,  1.86755799e+00],
       [-9.77277...2.89120505e-01],
       [ 4.12870820e-01, -1.98398897e-01,  9.41923003e-02,
        -1.14761094e+00, -3.58114075e-01]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________ test_partial_dependence_easy_target[2-est1] __________________

est = GradientBoostingRegressor(random_state=0), power = 2

    @pytest.mark.parametrize(
        "est",
        (
            LinearRegression(),
            GradientBoostingRegressor(random_state=0),
            HistGradientBoostingRegressor(
                random_state=0, min_samples_leaf=1, max_leaf_nodes=None, max_iter=1
            ),
            DecisionTreeRegressor(random_state=0),
        ),
    )
    @pytest.mark.parametrize("power", (1, 2))
    def test_partial_dependence_easy_target(est, power):
        # If the target y only depends on one feature in an obvious way (linear or
        # quadratic) then the partial dependence for that feature should reflect
        # it.
        # We here fit a linear regression_data model (with polynomial features if
        # needed) and compute r_squared to check that the partial dependence
        # correctly reflects the target.
    
        rng = np.random.RandomState(0)
        n_samples = 200
        target_variable = 2
        X = rng.normal(size=(n_samples, 5))
        y = X[:, target_variable] ** power
    
        est = clone(est).fit(X, y)
    
>       pdp = partial_dependence(
            est, features=[target_variable], X=X, grid_resolution=1000, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(random_state=0)
X = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,
         2.24089320e+00,  1.86755799e+00],
       [-9.77277...2.89120505e-01],
       [ 4.12870820e-01, -1.98398897e-01,  9.41923003e-02,
        -1.14761094e+00, -3.58114075e-01]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________ test_partial_dependence_easy_target[2-est2] __________________

est = HistGradientBoostingRegressor(max_iter=1, max_leaf_nodes=None,
                              min_samples_leaf=1, random_state=0)
power = 2

    @pytest.mark.parametrize(
        "est",
        (
            LinearRegression(),
            GradientBoostingRegressor(random_state=0),
            HistGradientBoostingRegressor(
                random_state=0, min_samples_leaf=1, max_leaf_nodes=None, max_iter=1
            ),
            DecisionTreeRegressor(random_state=0),
        ),
    )
    @pytest.mark.parametrize("power", (1, 2))
    def test_partial_dependence_easy_target(est, power):
        # If the target y only depends on one feature in an obvious way (linear or
        # quadratic) then the partial dependence for that feature should reflect
        # it.
        # We here fit a linear regression_data model (with polynomial features if
        # needed) and compute r_squared to check that the partial dependence
        # correctly reflects the target.
    
        rng = np.random.RandomState(0)
        n_samples = 200
        target_variable = 2
        X = rng.normal(size=(n_samples, 5))
        y = X[:, target_variable] ** power
    
        est = clone(est).fit(X, y)
    
>       pdp = partial_dependence(
            est, features=[target_variable], X=X, grid_resolution=1000, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingRegressor(max_iter=1, max_leaf_nodes=None,
                              min_samples_leaf=1, random_state=0)
X = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,
         2.24089320e+00,  1.86755799e+00],
       [-9.77277...2.89120505e-01],
       [ 4.12870820e-01, -1.98398897e-01,  9.41923003e-02,
        -1.14761094e+00, -3.58114075e-01]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________ test_partial_dependence_easy_target[2-est3] __________________

est = DecisionTreeRegressor(random_state=0), power = 2

    @pytest.mark.parametrize(
        "est",
        (
            LinearRegression(),
            GradientBoostingRegressor(random_state=0),
            HistGradientBoostingRegressor(
                random_state=0, min_samples_leaf=1, max_leaf_nodes=None, max_iter=1
            ),
            DecisionTreeRegressor(random_state=0),
        ),
    )
    @pytest.mark.parametrize("power", (1, 2))
    def test_partial_dependence_easy_target(est, power):
        # If the target y only depends on one feature in an obvious way (linear or
        # quadratic) then the partial dependence for that feature should reflect
        # it.
        # We here fit a linear regression_data model (with polynomial features if
        # needed) and compute r_squared to check that the partial dependence
        # correctly reflects the target.
    
        rng = np.random.RandomState(0)
        n_samples = 200
        target_variable = 2
        X = rng.normal(size=(n_samples, 5))
        y = X[:, target_variable] ** power
    
        est = clone(est).fit(X, y)
    
>       pdp = partial_dependence(
            est, features=[target_variable], X=X, grid_resolution=1000, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:436: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = DecisionTreeRegressor(random_state=0)
X = array([[ 1.76405235e+00,  4.00157208e-01,  9.78737984e-01,
         2.24089320e+00,  1.86755799e+00],
       [-9.77277...2.89120505e-01],
       [ 4.12870820e-01, -1.98398897e-01,  9.41923003e-02,
        -1.14761094e+00, -3.58114075e-01]])
features = [2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________________ test_partial_dependence_X_list[estimator0] __________________

estimator = LinearRegression()

    @pytest.mark.parametrize(
        "estimator", [LinearRegression(), GradientBoostingClassifier(random_state=0)]
    )
    def test_partial_dependence_X_list(estimator):
        # check that array-like objects are accepted
        X, y = make_classification(random_state=0)
        estimator = clone(estimator).fit(X, y)
>       partial_dependence(estimator, list(X), [0], kind="average")

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:572: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-0.039267991037953184, 0.1319117557874122, -0.21120598363152,
        ..., 1.9769890149980527, 1.0212247431671...7, -0.5643010333021604,
        ..., 1.2666139424327671, -1.3177173431407654, 1.6180542690002255]],
      dtype=object)
features = [0]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________________ test_partial_dependence_X_list[estimator1] __________________

estimator = GradientBoostingClassifier(random_state=0)

    @pytest.mark.parametrize(
        "estimator", [LinearRegression(), GradientBoostingClassifier(random_state=0)]
    )
    def test_partial_dependence_X_list(estimator):
        # check that array-like objects are accepted
        X, y = make_classification(random_state=0)
        estimator = clone(estimator).fit(X, y)
>       partial_dependence(estimator, list(X), [0], kind="average")

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:572: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(random_state=0)
X = array([[-0.039267991037953184, 0.1319117557874122, -0.21120598363152,
        ..., 1.9769890149980527, 1.0212247431671...7, -0.5643010333021604,
        ..., 1.2666139424327671, -1.3177173431407654, 1.6180542690002255]],
      dtype=object)
features = [0]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
___________________ test_warning_recursion_non_constant_init ___________________

    def test_warning_recursion_non_constant_init():
        # make sure that passing a non-constant init parameter to a GBDT and using
        # recursion method yields a warning.
    
        gbc = GradientBoostingClassifier(init=DummyClassifier(), random_state=0)
        gbc.fit(X, y)
    
        with pytest.warns(
            UserWarning, match="Using recursion method with a non-constant init predictor"
        ):
>           partial_dependence(gbc, X, [0], method="recursion", kind="average")

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:585: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingClassifier(init=DummyClassifier(), random_state=0)
X = array([[-2, -1],
       [-1, -1],
       [-1, -2],
       [1, 1],
       [1, 2],
       [2, 1]], dtype=object)
features = [0]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError

During handling of the above exception, another exception occurred:

    def test_warning_recursion_non_constant_init():
        # make sure that passing a non-constant init parameter to a GBDT and using
        # recursion method yields a warning.
    
        gbc = GradientBoostingClassifier(init=DummyClassifier(), random_state=0)
        gbc.fit(X, y)
    
        with pytest.warns(
            UserWarning, match="Using recursion method with a non-constant init predictor"
        ):
>           partial_dependence(gbc, X, [0], method="recursion", kind="average")
E           Failed: DID NOT WARN. No warnings of type (<class 'UserWarning'>,) were emitted.
E            Emitted warnings: [].

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:585: Failed
__________ test_partial_dependence_sample_weight_of_fitted_estimator ___________

    def test_partial_dependence_sample_weight_of_fitted_estimator():
        # Test near perfect correlation between partial dependence and diagonal
        # when sample weights emphasize y = x predictions
        # non-regression test for #13193
        # TODO: extend to HistGradientBoosting once sample_weight is supported
        N = 1000
        rng = np.random.RandomState(123456)
        mask = rng.randint(2, size=N, dtype=bool)
    
        x = rng.rand(N)
        # set y = x on mask and y = -x outside
        y = x.copy()
        y[~mask] = -y[~mask]
        X = np.c_[mask, x]
        # sample weights to emphasize data points where y = x
        sample_weight = np.ones(N)
        sample_weight[mask] = 1000.0
    
        clf = GradientBoostingRegressor(n_estimators=10, random_state=1)
        clf.fit(X, y, sample_weight=sample_weight)
    
>       pdp = partial_dependence(clf, X, features=[1], kind="average")

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:614: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = GradientBoostingRegressor(n_estimators=10, random_state=1)
X = array([[1.        , 0.2288873 ],
       [0.        , 0.77678375],
       [0.        , 0.59478359],
       ...,
       [0.        , 0.44831397],
       [1.        , 0.30214719],
       [0.        , 0.24147057]])
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______________________ test_hist_gbdt_sw_not_supported ________________________

    def test_hist_gbdt_sw_not_supported():
        # TODO: remove/fix when PDP supports HGBT with sample weights
        clf = HistGradientBoostingRegressor(random_state=1)
        clf.fit(X, y, sample_weight=np.ones(len(X)))
    
        with pytest.raises(
            NotImplementedError, match="does not support partial dependence"
        ):
>           partial_dependence(clf, X, features=[1])

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:627: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = HistGradientBoostingRegressor(random_state=1)
X = array([[-2, -1],
       [-1, -1],
       [-1, -2],
       [1, 1],
       [1, 2],
       [2, 1]], dtype=object)
features = [1]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_______________________ test_partial_dependence_pipeline _______________________

    def test_partial_dependence_pipeline():
        # check that the partial dependence support pipeline
        iris = load_iris()
    
        scaler = StandardScaler()
        clf = DummyClassifier(random_state=42)
        pipe = make_pipeline(scaler, clf)
    
        clf.fit(scaler.fit_transform(iris.data), iris.target)
        pipe.fit(iris.data, iris.target)
    
        features = 0
>       pdp_pipe = partial_dependence(
            pipe, iris.data, features=[features], grid_resolution=10, kind="average"
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:642: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('standardscaler', StandardScaler()),
                ('dummyclassifier', DummyClassifier(random_state=42))])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [0]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_____ test_kind_average_and_average_of_individual[LinearRegression-data0] ______

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)

    @pytest.mark.parametrize(
        "Estimator, data",
        [
            (LinearRegression, multioutput_regression_data),
            (LogisticRegression, binary_classification_data),
        ],
    )
    def test_kind_average_and_average_of_individual(Estimator, data):
        est = Estimator()
        (X, y), n_targets = data
        est.fit(X, y)
    
>       pdp_avg = partial_dependence(est, X=X, features=[1, 2], kind="average")

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:794: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
____ test_kind_average_and_average_of_individual[LogisticRegression-data1] _____

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)

    @pytest.mark.parametrize(
        "Estimator, data",
        [
            (LinearRegression, multioutput_regression_data),
            (LogisticRegression, binary_classification_data),
        ],
    )
    def test_kind_average_and_average_of_individual(Estimator, data):
        est = Estimator()
        (X, y), n_targets = data
        est.fit(X, y)
    
>       pdp_avg = partial_dependence(est, X=X, features=[1, 2], kind="average")

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:794: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_partial_dependence_kind_individual_ignores_sample_weight[LinearRegression-data0] _

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)

    @pytest.mark.parametrize(
        "Estimator, data",
        [
            (LinearRegression, multioutput_regression_data),
            (LogisticRegression, binary_classification_data),
        ],
    )
    def test_partial_dependence_kind_individual_ignores_sample_weight(Estimator, data):
        """Check that `sample_weight` does not have any effect on reported ICE."""
        est = Estimator()
        (X, y), n_targets = data
        sample_weight = np.arange(X.shape[0])
        est.fit(X, y)
    
>       pdp_nsw = partial_dependence(est, X=X, features=[1, 2], kind="individual")

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:814: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_partial_dependence_kind_individual_ignores_sample_weight[LogisticRegression-data1] _

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)

    @pytest.mark.parametrize(
        "Estimator, data",
        [
            (LinearRegression, multioutput_regression_data),
            (LogisticRegression, binary_classification_data),
        ],
    )
    def test_partial_dependence_kind_individual_ignores_sample_weight(Estimator, data):
        """Check that `sample_weight` does not have any effect on reported ICE."""
        est = Estimator()
        (X, y), n_targets = data
        sample_weight = np.arange(X.shape[0])
        est.fit(X, y)
    
>       pdp_nsw = partial_dependence(est, X=X, features=[1, 2], kind="individual")

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:814: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[0-estimator0] ___________

estimator = LinearRegression(), non_null_weight_idx = 0

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                                 RobustScaler(), [1, 3])])),
                ('linearregression', LinearRegression())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[0-estimator1] ___________

estimator = LogisticRegression(), non_null_weight_idx = 0

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                             RobustScaler(), [1, 3])])),
                ('logisticregression', LogisticRegression())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[0-estimator2] ___________

estimator = RandomForestRegressor(), non_null_weight_idx = 0

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                       RobustScaler(), [1, 3])])),
                ('randomforestregressor', RandomForestRegressor())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[0-estimator3] ___________

estimator = GradientBoostingClassifier(), non_null_weight_idx = 0

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...             RobustScaler(), [1, 3])])),
                ('gradientboostingclassifier', GradientBoostingClassifier())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[1-estimator0] ___________

estimator = LinearRegression(), non_null_weight_idx = 1

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                                 RobustScaler(), [1, 3])])),
                ('linearregression', LinearRegression())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[1-estimator1] ___________

estimator = LogisticRegression(), non_null_weight_idx = 1

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                             RobustScaler(), [1, 3])])),
                ('logisticregression', LogisticRegression())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[1-estimator2] ___________

estimator = RandomForestRegressor(), non_null_weight_idx = 1

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                       RobustScaler(), [1, 3])])),
                ('randomforestregressor', RandomForestRegressor())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[1-estimator3] ___________

estimator = GradientBoostingClassifier(), non_null_weight_idx = 1

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...             RobustScaler(), [1, 3])])),
                ('gradientboostingclassifier', GradientBoostingClassifier())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[-1-estimator0] __________

estimator = LinearRegression(), non_null_weight_idx = -1

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                                 RobustScaler(), [1, 3])])),
                ('linearregression', LinearRegression())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[-1-estimator1] __________

estimator = LogisticRegression(), non_null_weight_idx = -1

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                             RobustScaler(), [1, 3])])),
                ('logisticregression', LogisticRegression())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[-1-estimator2] __________

estimator = RandomForestRegressor(), non_null_weight_idx = -1

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...                       RobustScaler(), [1, 3])])),
                ('randomforestregressor', RandomForestRegressor())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
__________ test_partial_dependence_non_null_weight_idx[-1-estimator3] __________

estimator = GradientBoostingClassifier(), non_null_weight_idx = -1

    @pytest.mark.parametrize(
        "estimator",
        [
            LinearRegression(),
            LogisticRegression(),
            RandomForestRegressor(),
            GradientBoostingClassifier(),
        ],
    )
    @pytest.mark.parametrize("non_null_weight_idx", [0, 1, -1])
    def test_partial_dependence_non_null_weight_idx(estimator, non_null_weight_idx):
        """Check that if we pass a `sample_weight` of zeros with only one index with
        sample weight equals one, then the average `partial_dependence` with this
        `sample_weight` is equal to the individual `partial_dependence` of the
        corresponding index.
        """
        X, y = iris.data, iris.target
        preprocessor = make_column_transformer(
            (StandardScaler(), [0, 2]), (RobustScaler(), [1, 3])
        )
        pipe = make_pipeline(preprocessor, clone(estimator)).fit(X, y)
    
        sample_weight = np.zeros_like(y)
        sample_weight[non_null_weight_idx] = 1
>       pdp_sw = partial_dependence(
            pipe,
            X,
            [2, 3],
            kind="average",
            sample_weight=sample_weight,
            grid_resolution=10,
        )

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:846: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('columntransformer',
                 ColumnTransformer(transformers=[('standardscaler',
            ...             RobustScaler(), [1, 3])])),
                ('gradientboostingclassifier', GradientBoostingClassifier())])
X = array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
  ...],
       [6.3, 2.5, 5. , 1.9],
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]])
features = [2, 3]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_partial_dependence_equivalence_equal_sample_weight[LinearRegression-data0] _

Estimator = <class 'sklearn.linear_model._base.LinearRegression'>
data = ((array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.64933...    [ -77.0442537 , -106.33645293],
       [  11.72525625,   79.48897745],
       [   1.0856097 ,   58.83205573]])), 2)

    @pytest.mark.parametrize(
        "Estimator, data",
        [
            (LinearRegression, multioutput_regression_data),
            (LogisticRegression, binary_classification_data),
        ],
    )
    def test_partial_dependence_equivalence_equal_sample_weight(Estimator, data):
        """Check that `sample_weight=None` is equivalent to having equal weights."""
    
        est = Estimator()
        (X, y), n_targets = data
        est.fit(X, y)
    
        sample_weight, params = None, {"X": X, "features": [1, 2], "kind": "average"}
>       pdp_sw_none = partial_dependence(est, **params, sample_weight=sample_weight)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:878: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LinearRegression()
X = array([[-1.36555273, -1.00912403,  1.59540417, ...,  0.65121001,
        -0.1378997 , -0.41376249],
       [-0.6493379...035,  0.30198921],
       [ 0.35924916,  0.50538694, -1.40556005, ...,  0.54100822,
         0.53659652,  0.11709087]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_ test_partial_dependence_equivalence_equal_sample_weight[LogisticRegression-data1] _

Estimator = <class 'sklearn.linear_model._logistic.LogisticRegression'>
data = ((array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01... 0, 1, 0, 0, 0,
       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0,
       1, 1, 1, 1, 0, 0])), 1)

    @pytest.mark.parametrize(
        "Estimator, data",
        [
            (LinearRegression, multioutput_regression_data),
            (LogisticRegression, binary_classification_data),
        ],
    )
    def test_partial_dependence_equivalence_equal_sample_weight(Estimator, data):
        """Check that `sample_weight=None` is equivalent to having equal weights."""
    
        est = Estimator()
        (X, y), n_targets = data
        est.fit(X, y)
    
        sample_weight, params = None, {"X": X, "features": [1, 2], "kind": "average"}
>       pdp_sw_none = partial_dependence(est, **params, sample_weight=sample_weight)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:878: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = LogisticRegression()
X = array([[ 6.99380484e-01, -9.64606424e-01, -2.12523045e-01,
         5.98946831e-02, -7.62114512e-01,  4.57627354e-01,
...-2.24258934e-01,
         6.76460732e-01,  1.35126740e+00, -1.31908640e-01,
        -3.75147117e-01, -1.50699840e+00]])
features = [1, 2]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
_________________________ test_mixed_type_categorical __________________________

    def test_mixed_type_categorical():
        """Check that we raise a proper error when a column has mixed types and
        the sorting of `np.unique` will fail."""
        X = np.array(["A", "B", "C", np.nan], dtype=object).reshape(-1, 1)
        y = np.array([0, 1, 0, 1])
    
        from sklearn.preprocessing import OrdinalEncoder
    
        clf = make_pipeline(
            OrdinalEncoder(encoded_missing_value=-1),
            LogisticRegression(),
        ).fit(X, y)
        with pytest.raises(ValueError, match="The column #0 contains mixed data types"):
>           partial_dependence(clf, X, features=[0])

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py:930: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

estimator = Pipeline(steps=[('ordinalencoder', OrdinalEncoder(encoded_missing_value=-1)),
                ('logisticregression', LogisticRegression())])
X = array([['A'],
       ['B'],
       ['C'],
       [nan]], dtype=object)
features = [0]

    @validate_params({'estimator': [HasMethods(['fit', 'predict']), HasMethods(['fit', 'predict_proba']), HasMethods(['fit', 'decision_function'])], 'X': ['array-like', 'sparse matrix'], 'features': ['array-like', Integral, str], 'sample_weight': ['array-like', None], 'categorical_features': ['array-like', None], 'feature_names': ['array-like', None], 'response_method': [StrOptions({'auto', 'predict_proba', 'decision_function'})], 'percentiles': [tuple], 'grid_resolution': [Interval(Integral, 1, None, closed='left')], 'method': [StrOptions({'auto', 'recursion', 'brute'})], 'kind': [StrOptions({'average', 'individual', 'both'})]}, prefer_skip_nested_validation=True)
    def partial_dependence(estimator, X, features, *, sample_weight=None, categorical_features=None, feature_names=None, response_method='auto', percentiles=(0.05, 0.95), grid_resolution=100, method='auto', kind='average'):
        """Partial dependence of ``features``.
    
        Partial dependence of a feature (or a set of features) corresponds to
        the average response of an estimator for each possible value of the
        feature.
    
        Read more in the :ref:`User Guide <partial_dependence>`.
    
        .. warning::
    
            For :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, the
            `'recursion'` method (used by default) will not account for the `init`
            predictor of the boosting process. In practice, this will produce
            the same values as `'brute'` up to a constant offset in the target
            response, provided that `init` is a constant estimator (which is the
            default). However, if `init` is not a constant estimator, the
            partial dependence values are incorrect for `'recursion'` because the
            offset will be sample-dependent. It is preferable to use the `'brute'`
            method. Note that this only applies to
            :class:`~sklearn.ensemble.GradientBoostingClassifier` and
            :class:`~sklearn.ensemble.GradientBoostingRegressor`, not to
            :class:`~sklearn.ensemble.HistGradientBoostingClassifier` and
            :class:`~sklearn.ensemble.HistGradientBoostingRegressor`.
    
        Parameters
        ----------
        estimator : BaseEstimator
            A fitted estimator object implementing :term:`predict`,
            :term:`predict_proba`, or :term:`decision_function`.
            Multioutput-multiclass classifiers are not supported.
    
        X : {array-like, sparse matrix or dataframe} of shape (n_samples, n_features)
            ``X`` is used to generate a grid of values for the target
            ``features`` (where the partial dependence will be evaluated), and
            also to generate values for the complement features when the
            `method` is 'brute'.
    
        features : array-like of {int, str, bool} or int or str
            The feature (e.g. `[0]`) or pair of interacting features
            (e.g. `[(0, 1)]`) for which the partial dependency should be computed.
    
        sample_weight : array-like of shape (n_samples,), default=None
            Sample weights are used to calculate weighted means when averaging the
            model output. If `None`, then samples are equally weighted. If
            `sample_weight` is not `None`, then `method` will be set to `'brute'`.
            Note that `sample_weight` is ignored for `kind='individual'`.
    
            .. versionadded:: 1.3
    
        categorical_features : array-like of shape (n_features,) or shape             (n_categorical_features,), dtype={bool, int, str}, default=None
            Indicates the categorical features.
    
            - `None`: no feature will be considered categorical;
            - boolean array-like: boolean mask of shape `(n_features,)`
                indicating which features are categorical. Thus, this array has
                the same shape has `X.shape[1]`;
            - integer or string array-like: integer indices or strings
                indicating categorical features.
    
            .. versionadded:: 1.2
    
        feature_names : array-like of shape (n_features,), dtype=str, default=None
            Name of each feature; `feature_names[i]` holds the name of the feature
            with index `i`.
            By default, the name of the feature corresponds to their numerical
            index for NumPy array and their column name for pandas dataframe.
    
            .. versionadded:: 1.2
    
        response_method : {'auto', 'predict_proba', 'decision_function'},             default='auto'
            Specifies whether to use :term:`predict_proba` or
            :term:`decision_function` as the target response. For regressors
            this parameter is ignored and the response is always the output of
            :term:`predict`. By default, :term:`predict_proba` is tried first
            and we revert to :term:`decision_function` if it doesn't exist. If
            ``method`` is 'recursion', the response is always the output of
            :term:`decision_function`.
    
        percentiles : tuple of float, default=(0.05, 0.95)
            The lower and upper percentile used to create the extreme values
            for the grid. Must be in [0, 1].
    
        grid_resolution : int, default=100
            The number of equally spaced points on the grid, for each target
            feature.
    
        method : {'auto', 'recursion', 'brute'}, default='auto'
            The method used to calculate the averaged predictions:
    
            - `'recursion'` is only supported for some tree-based estimators
              (namely
              :class:`~sklearn.ensemble.GradientBoostingClassifier`,
              :class:`~sklearn.ensemble.GradientBoostingRegressor`,
              :class:`~sklearn.ensemble.HistGradientBoostingClassifier`,
              :class:`~sklearn.ensemble.HistGradientBoostingRegressor`,
              :class:`~sklearn.tree.DecisionTreeRegressor`,
              :class:`~sklearn.ensemble.RandomForestRegressor`,
              ) when `kind='average'`.
              This is more efficient in terms of speed.
              With this method, the target response of a
              classifier is always the decision function, not the predicted
              probabilities. Since the `'recursion'` method implicitly computes
              the average of the Individual Conditional Expectation (ICE) by
              design, it is not compatible with ICE and thus `kind` must be
              `'average'`.
    
            - `'brute'` is supported for any estimator, but is more
              computationally intensive.
    
            - `'auto'`: the `'recursion'` is used for estimators that support it,
              and `'brute'` is used otherwise. If `sample_weight` is not `None`,
              then `'brute'` is used regardless of the estimator.
    
            Please see :ref:`this note <pdp_method_differences>` for
            differences between the `'brute'` and `'recursion'` method.
    
        kind : {'average', 'individual', 'both'}, default='average'
            Whether to return the partial dependence averaged across all the
            samples in the dataset or one value per sample or both.
            See Returns below.
    
            Note that the fast `method='recursion'` option is only available for
            `kind='average'` and `sample_weights=None`. Computing individual
            dependencies and doing weighted averages requires using the slower
            `method='brute'`.
    
            .. versionadded:: 0.24
    
        Returns
        -------
        predictions : :class:`~sklearn.utils.Bunch`
            Dictionary-like object, with the following attributes.
    
            individual : ndarray of shape (n_outputs, n_instances,                 len(values[0]), len(values[1]), ...)
                The predictions for all the points in the grid for all
                samples in X. This is also known as Individual
                Conditional Expectation (ICE).
                Only available when `kind='individual'` or `kind='both'`.
    
            average : ndarray of shape (n_outputs, len(values[0]),                 len(values[1]), ...)
                The predictions for all the points in the grid, averaged
                over all samples in X (or over the training data if
                `method` is 'recursion').
                Only available when `kind='average'` or `kind='both'`.
    
            grid_values : seq of 1d ndarrays
                The values with which the grid has been created. The generated
                grid is a cartesian product of the arrays in `grid_values` where
                `len(grid_values) == len(features)`. The size of each array
                `grid_values[j]` is either `grid_resolution`, or the number of
                unique values in `X[:, j]`, whichever is smaller.
    
                .. versionadded:: 1.3
    
            `n_outputs` corresponds to the number of classes in a multi-class
            setting, or to the number of tasks for multi-output regression.
            For classical regression and binary classification `n_outputs==1`.
            `n_values_feature_j` corresponds to the size `grid_values[j]`.
    
        See Also
        --------
        PartialDependenceDisplay.from_estimator : Plot Partial Dependence.
        PartialDependenceDisplay : Partial Dependence visualization.
    
        Examples
        --------
        >>> X = [[0, 0, 2], [1, 0, 0]]
        >>> y = [0, 1]
        >>> from sklearn.ensemble import GradientBoostingClassifier
        >>> gb = GradientBoostingClassifier(random_state=0).fit(X, y)
        >>> partial_dependence(gb, features=[0], X=X, percentiles=(0, 1),
        ...                    grid_resolution=2) # doctest: +SKIP
        (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])])
        """
        check_is_fitted(estimator)
        if not (is_classifier(estimator) or is_regressor(estimator)):
            raise ValueError("'estimator' must be a fitted regressor or classifier.")
        if is_classifier(estimator) and isinstance(estimator.classes_[0], np.ndarray):
            raise ValueError('Multiclass-multioutput estimators are not supported')
        if not (hasattr(X, '__array__') or sparse.issparse(X)):
            X = check_array(X, ensure_all_finite='allow-nan', dtype=object)
        if is_regressor(estimator) and response_method != 'auto':
            raise ValueError("The response_method parameter is ignored for regressors and must be 'auto'.")
        if kind != 'average':
            if method == 'recursion':
                raise ValueError("The 'recursion' method only applies when 'kind' is set to 'average'")
            method = 'brute'
        if method == 'recursion' and sample_weight is not None:
            raise ValueError("The 'recursion' method can only be applied when sample_weight is None.")
        if method == 'auto':
            if sample_weight is not None:
                method = 'brute'
            elif isinstance(estimator, BaseGradientBoosting) and estimator.init is None:
                method = 'recursion'
            elif isinstance(estimator, (BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                method = 'recursion'
            else:
                method = 'brute'
        if method == 'recursion':
            if not isinstance(estimator, (BaseGradientBoosting, BaseHistGradientBoosting, DecisionTreeRegressor, RandomForestRegressor)):
                supported_classes_recursion = ('GradientBoostingClassifier', 'GradientBoostingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor', 'HistGradientBoostingRegressor', 'DecisionTreeRegressor', 'RandomForestRegressor')
                raise ValueError("Only the following estimators support the 'recursion' method: {}. Try using method='brute'.".format(', '.join(supported_classes_recursion)))
            if response_method == 'auto':
                response_method = 'decision_function'
            if response_method != 'decision_function':
                raise ValueError("With the 'recursion' method, the response_method must be 'decision_function'. Got {}.".format(response_method))
        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)
        if _determine_key_type(features, accept_slice=False) == 'int':
            if np.any(np.less(features, 0)):
                raise ValueError('all features must be in [0, {}]'.format(X.shape[1] - 1))
        features_indices = np.asarray(_get_column_indices(X, features), dtype=np.intp, order='C').ravel()
        feature_names = _check_feature_names(X, feature_names)
        n_features = X.shape[1]
        if categorical_features is None:
            is_categorical = [False] * len(features_indices)
        else:
            categorical_features = np.asarray(categorical_features)
            if categorical_features.dtype.kind == 'b':
                if categorical_features.size != n_features:
                    raise ValueError(f'When `categorical_features` is a boolean array-like, the array should be of shape (n_features,). Got {categorical_features.size} elements while `X` contains {n_features} features.')
                is_categorical = [categorical_features[idx] for idx in features_indices]
            elif categorical_features.dtype.kind in ('i', 'O', 'U'):
                categorical_features_idx = [_get_feature_index(cat, feature_names=feature_names) for cat in categorical_features]
                is_categorical = [idx in categorical_features_idx for idx in features_indices]
            else:
                raise ValueError(f'Expected `categorical_features` to be an array-like of boolean, integer, or string. Got {categorical_features.dtype} instead.')
>       grid, values = _grid_from_X(_safe_indexing(X, features_indices, axis=1), percentiles, is_categorical, grid_resolution)
E       TypeError: cannot unpack non-iterable NoneType object

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/_partial_dependence.py:468: TypeError
=========================== short test summary info ============================
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-auto-data4]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-brute-data5]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-DecisionTreeRegressor-brute-data6]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data7]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data8]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data9]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data10]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-MultiTaskLasso-brute-data11]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[2-percentiles0-percentiles are too close]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles1-'percentiles' must be a sequence of 2 elements]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-12345-'percentiles' must be a sequence of 2 elements]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles3-'percentiles' values must be in \\[0, 1\\]]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles4-'percentiles' values must be in \\[0, 1\\]]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles5-percentiles\\[0\\] must be strictly less than]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[1-percentiles6-'grid_resolution' must be strictly greater than 1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_warning_recursion_non_constant_init
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_of_fitted_estimator
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_hist_gbdt_sw_not_supported
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_pipeline
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LinearRegression-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LogisticRegression-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LinearRegression-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LogisticRegression-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator2]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator3]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LinearRegression-data0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LogisticRegression-data1]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_mixed_type_categorical
================== 197 failed, 49 passed, 23 skipped in 8.84s ==================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 269 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data0] I: Seeding RNGs with 1528021371
PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_with_categorical[2] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_with_categorical[100] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_heterogeneous_type[3] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_heterogeneous_type[100] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[2-percentiles0-percentiles are too close] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles1-'percentiles' must be a sequence of 2 elements] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-12345-'percentiles' must be a sequence of 2 elements] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles3-'percentiles' values must be in \\[0, 1\\]] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles4-'percentiles' values must be in \\[0, 1\\]] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles5-percentiles\\[0\\] must be strictly less than] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[1-percentiles6-'grid_resolution' must be strictly greater than 1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_tree_vs_forest_and_gbdt[0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[DecisionTreeClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[ExtraTreeClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[ExtraTreesClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[KNeighborsClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[RadiusNeighborsClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[RandomForestClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator0-params0-'estimator' must be a fitted regressor or classifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator1-params1-The response_method parameter is ignored for regressors] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator2-params2-'recursion' method, the response_method must be 'decision_function'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator3-params3-'recursion' method, the response_method must be 'decision_function'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator4-params4-The 'recursion' method only applies when 'kind' is set to 'average'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator5-params5-The 'recursion' method only applies when 'kind' is set to 'average'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator6-params6-Only the following estimators support the 'recursion' method:] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[-1-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[-1-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[10000-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[10000-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_string[estimator0] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_string[estimator1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_warning_recursion_non_constant_init PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_of_fitted_estimator PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_hist_gbdt_sw_not_supported PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_pipeline PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-None-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-None-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-passthrough-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-passthrough-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-None-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-None-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-passthrough-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-passthrough-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[scalar-int] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[scalar-str] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[list-int] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[list-str] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[mask] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LinearRegression-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LogisticRegression-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LinearRegression-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LogisticRegression-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LinearRegression-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LogisticRegression-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_size_error PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_with_recursion PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_mixed_type_categorical PASSED

======================= 246 passed, 23 skipped in 13.55s =======================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 269 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data0] I: Seeding RNGs with 1488196588
PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features0-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[average-features1-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features0-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[individual-features1-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features0-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-5-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-auto-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingClassifier-brute-data3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-auto-data4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-GradientBoostingRegressor-brute-data5] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-DecisionTreeRegressor-brute-data6] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data7] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LinearRegression-brute-data8] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data9] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-LogisticRegression-brute-data10] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_output_shape[both-features1-10-MultiTaskLasso-brute-data11] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_with_categorical[2] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_with_categorical[100] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_heterogeneous_type[3] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_heterogeneous_type[100] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[2-percentiles0-percentiles are too close] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles1-'percentiles' must be a sequence of 2 elements] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-12345-'percentiles' must be a sequence of 2 elements] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles3-'percentiles' values must be in \\[0, 1\\]] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles4-'percentiles' values must be in \\[0, 1\\]] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[100-percentiles5-percentiles\\[0\\] must be strictly less than] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_grid_from_X_error[1-percentiles6-'grid_resolution' must be strictly greater than 1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est0-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est1-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est2-recursion-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est3-brute-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_helpers[est4-recursion-4] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_tree_vs_forest_and_gbdt[0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[0-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[1-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[2-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[3-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[4-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_recursion_decision_function[5-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[1-est3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_easy_target[2-est3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[DecisionTreeClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[ExtraTreeClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[ExtraTreesClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[KNeighborsClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[RadiusNeighborsClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_multiclass_multioutput[RandomForestClassifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator0-params0-'estimator' must be a fitted regressor or classifier] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator1-params1-The response_method parameter is ignored for regressors] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator2-params2-'recursion' method, the response_method must be 'decision_function'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator3-params3-'recursion' method, the response_method must be 'decision_function'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator4-params4-The 'recursion' method only applies when 'kind' is set to 'average'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator5-params5-The 'recursion' method only applies when 'kind' is set to 'average'] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_error[estimator6-params6-Only the following estimators support the 'recursion' method:] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[-1-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[-1-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[10000-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_indices[10000-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_string[estimator0] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unknown_feature_string[estimator1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_X_list[estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_warning_recursion_non_constant_init PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_of_fitted_estimator PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_hist_gbdt_sw_not_supported PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_pipeline PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-None-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-None-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-passthrough-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-integer-column-transformer-passthrough-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-None-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-None-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-passthrough-estimator-brute] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_dataframe[features-string-column-transformer-passthrough-estimator-recursion] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[scalar-int] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[scalar-str] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[list-int] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[list-str] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_feature_type[mask] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_unfitted[estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LinearRegression-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_kind_average_and_average_of_individual[LogisticRegression-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LinearRegression-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_kind_individual_ignores_sample_weight[LogisticRegression-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[0-estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[1-estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator2] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_non_null_weight_idx[-1-estimator3] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LinearRegression-data0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_equivalence_equal_sample_weight[LogisticRegression-data1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_size_error PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_partial_dependence_sample_weight_with_recursion PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/inspection/tests/test_partial_dependence.py::test_mixed_type_categorical PASSED

======================= 246 passed, 23 skipped in 13.49s =======================
