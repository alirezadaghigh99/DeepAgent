output file:
processed_Laplacejacobians_naive197.json
function:
jacobians_naive
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[CurvatureInterface] FAILED', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[CurvatureInterface] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[AsdlInterface]', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[CurvatureInterface] FAILED', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[BackPackInterface] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[CurvatureInterface]', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[BackPackInterface]', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[AsdlInterface] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[CurvatureInterface]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface]', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[BackPackInterface] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[CurvatureInterface]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[BackPackInterface]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[BackPackInterface]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[CurvatureInterface]', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[AsdlInterface] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[BackPackInterface]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[AsdlInterface]', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface] FAILED', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[BackPackInterface] FAILED', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[CurvatureInterface] FAILED', '../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[BackPackInterface] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface]'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/Laplace/Laplace/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/Laplace/Laplace
configfile: pyproject.toml
plugins: mock-3.14.0, cov-6.0.0
collecting ... collected 23 items

../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[CurvatureInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[BackPackInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[CurvatureInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[BackPackInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[CurvatureInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[AsdlInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[BackPackInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[CurvatureInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[AsdlInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[BackPackInterface] FAILED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_multioutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_multioutput[BackPackInterface] PASSED

=================================== FAILURES ===================================
_______________ test_jacobians_singleoutput[CurvatureInterface] ________________

singleoutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=1, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]], requires_grad=True)
backend_cls = <class 'laplace.curvature.curvature.CurvatureInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_jacobians_singleoutput(singleoutput_model, X, backend_cls):
        model = singleoutput_model
        backend = backend_cls(model, "classification")
        Js, f = backend.jacobians(X)
        Js_naive, f_naive = jacobians_naive(model, X)
>       assert Js.shape == Js_naive.shape
E       assert torch.Size([200, 1, 101]) == torch.Size([200, 200, 3])
E         
E         At index 1 diff: 1 != 200
E         
E         Full diff:
E         - torch.Size([200, 200, 3])
E         ?                  ^ ^^^^
E         + torch.Size([200, 1, 101])
E         ?                  ^^^^ ^

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:59: AssertionError
__________________ test_jacobians_singleoutput[AsdlInterface] __________________

singleoutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=1, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]], requires_grad=True)
backend_cls = <class 'laplace.curvature.asdl.AsdlInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_jacobians_singleoutput(singleoutput_model, X, backend_cls):
        model = singleoutput_model
        backend = backend_cls(model, "classification")
        Js, f = backend.jacobians(X)
        Js_naive, f_naive = jacobians_naive(model, X)
>       assert Js.shape == Js_naive.shape
E       assert torch.Size([200, 1, 101]) == torch.Size([200, 200, 3])
E         
E         At index 1 diff: 1 != 200
E         
E         Full diff:
E         - torch.Size([200, 200, 3])
E         ?                  ^ ^^^^
E         + torch.Size([200, 1, 101])
E         ?                  ^^^^ ^

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:59: AssertionError
________________ test_jacobians_singleoutput[BackPackInterface] ________________

singleoutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=1, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]], requires_grad=True)
backend_cls = <class 'laplace.curvature.backpack.BackPackInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_jacobians_singleoutput(singleoutput_model, X, backend_cls):
        model = singleoutput_model
        backend = backend_cls(model, "classification")
        Js, f = backend.jacobians(X)
        Js_naive, f_naive = jacobians_naive(model, X)
>       assert Js.shape == Js_naive.shape
E       assert torch.Size([200, 1, 101]) == torch.Size([200, 200, 3])
E         
E         At index 1 diff: 1 != 200
E         
E         Full diff:
E         - torch.Size([200, 200, 3])
E         ?                  ^ ^^^^
E         + torch.Size([200, 1, 101])
E         ?                  ^^^^ ^

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:59: AssertionError
________________ test_jacobians_multioutput[CurvatureInterface] ________________

multioutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=2, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]], requires_grad=True)
backend_cls = <class 'laplace.curvature.curvature.CurvatureInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_jacobians_multioutput(multioutput_model, X, backend_cls):
        model = multioutput_model
        backend = backend_cls(model, "classification")
        Js, f = backend.jacobians(X)
>       Js_naive, f_naive = jacobians_naive(model, X)

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/utils.py:59: in jacobians_naive
    return jacobians_naive(model, data)
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/temp.py:28: in jacobians_naive
    f[i].backward(retain_graph=True)
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/_tensor.py:581: in backward
    torch.autograd.backward(
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:340: in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([ 0.2189, -0.4750], grad_fn=<SelectBackward0>),)
grads = (None,), is_grads_batched = False

    def _make_grads(
        outputs: Union[Sequence[torch.Tensor], Sequence[graph.GradientEdge]],
        grads: Sequence[_OptionalTensor],
        is_grads_batched: bool,
    ) -> Tuple[_OptionalTensor, ...]:
        new_grads: List[_OptionalTensor] = []
        for out, grad in zip(outputs, grads):
            out = cast(Union[torch.Tensor, graph.GradientEdge], out)
            out_size = None
            out_device = None
    
            if isinstance(out, graph.GradientEdge):
                out_metadata = out.node._input_metadata[out.output_nr]
                out_size = torch.Size(out_metadata.shape)
                out_dtype = out_metadata.dtype
                out_device = out_metadata.device
                out_is_nested = out_metadata.is_nested_tensor
                if out_metadata.is_cpp_nested_tensor:
                    raise RuntimeError(
                        "C++ NestedTensor are not supported with GradientEdge"
                    )
                out_is_cpp_nested = False
            else:
                # circular import
                from torch.nested._internal.nested_tensor import NestedTensor
    
                assert isinstance(out, torch.Tensor)
                out_dtype = out.dtype
                out_is_nested = out.is_nested
                out_is_cpp_nested = out_is_nested and not isinstance(out, NestedTensor)
                if not out_is_cpp_nested:
                    out_size = out.shape
    
            if isinstance(grad, torch.Tensor):
                from torch.fx.experimental.symbolic_shapes import expect_true, sym_eq
    
                first_grad = grad if not is_grads_batched else grad[0]
    
                # TODO: We can remove this conditional once we uniformly use
                # singleton int to represent jagged dimension, so that size() call
                # on nested tensor works.
                if out_is_cpp_nested:
                    assert isinstance(out, torch.Tensor)
                    shape_matches = torch.is_same_size(out, first_grad)
                else:
                    # We need to do a regular size check, without going through
                    # the operator, to be able to handle unbacked symints
                    # (expect_true ensures we can deal with unbacked)
                    assert out_size is not None
                    shape_matches = expect_true(sym_eq(out_size, first_grad.size()))
    
                if not shape_matches:
                    out = cast(Union[torch.Tensor, graph.GradientEdge], out)
                    out_shape, grad_shape = _calculate_shape(
                        out, first_grad, is_grads_batched
                    )
                    if is_grads_batched:
                        raise RuntimeError(
                            "If `is_grads_batched=True`, we interpret the first "
                            "dimension of each grad_output as the batch dimension. "
                            "The sizes of the remaining dimensions are expected to match "
                            "the shape of corresponding output, but a mismatch "
                            "was detected: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + ". "
                            "If you only want some tensors in `grad_output` to be considered "
                            "batched, consider using vmap."
                        )
                    else:
                        raise RuntimeError(
                            "Mismatch in shape: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + "."
                        )
                if out_dtype.is_complex != grad.dtype.is_complex:
                    raise RuntimeError(
                        "For complex Tensors, both grad_output and output"
                        " are required to have the same dtype."
                        " Mismatch in dtype: grad_output["
                        + str(grads.index(grad))
                        + "] has a dtype of "
                        + str(grad.dtype)
                        + " and output["
                        + str(outputs.index(out))
                        + "] has a dtype of "
                        + str(out_dtype)
                        + "."
                    )
                new_grads.append(grad)
            elif grad is None:
                if isinstance(out, graph.GradientEdge) or out.requires_grad:  # type: ignore[attr-defined]
                    if isinstance(out, graph.GradientEdge):
                        assert out_size is not None
                        out_numel_is_1 = all(o == 1 for o in out_size)
                    else:
                        assert isinstance(out, torch.Tensor)
                        out_numel_is_1 = out.numel() == 1
                    if not out_numel_is_1:
>                       raise RuntimeError(
                            "grad can be implicitly created only for scalar outputs"
                        )
E                       RuntimeError: grad can be implicitly created only for scalar outputs

/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:198: RuntimeError
__________________ test_jacobians_multioutput[AsdlInterface] ___________________

multioutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=2, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]], requires_grad=True)
backend_cls = <class 'laplace.curvature.asdl.AsdlInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_jacobians_multioutput(multioutput_model, X, backend_cls):
        model = multioutput_model
        backend = backend_cls(model, "classification")
        Js, f = backend.jacobians(X)
>       Js_naive, f_naive = jacobians_naive(model, X)

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/utils.py:59: in jacobians_naive
    return jacobians_naive(model, data)
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/temp.py:28: in jacobians_naive
    f[i].backward(retain_graph=True)
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/_tensor.py:581: in backward
    torch.autograd.backward(
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:340: in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([ 0.2189, -0.4750], grad_fn=<SelectBackward0>),)
grads = (None,), is_grads_batched = False

    def _make_grads(
        outputs: Union[Sequence[torch.Tensor], Sequence[graph.GradientEdge]],
        grads: Sequence[_OptionalTensor],
        is_grads_batched: bool,
    ) -> Tuple[_OptionalTensor, ...]:
        new_grads: List[_OptionalTensor] = []
        for out, grad in zip(outputs, grads):
            out = cast(Union[torch.Tensor, graph.GradientEdge], out)
            out_size = None
            out_device = None
    
            if isinstance(out, graph.GradientEdge):
                out_metadata = out.node._input_metadata[out.output_nr]
                out_size = torch.Size(out_metadata.shape)
                out_dtype = out_metadata.dtype
                out_device = out_metadata.device
                out_is_nested = out_metadata.is_nested_tensor
                if out_metadata.is_cpp_nested_tensor:
                    raise RuntimeError(
                        "C++ NestedTensor are not supported with GradientEdge"
                    )
                out_is_cpp_nested = False
            else:
                # circular import
                from torch.nested._internal.nested_tensor import NestedTensor
    
                assert isinstance(out, torch.Tensor)
                out_dtype = out.dtype
                out_is_nested = out.is_nested
                out_is_cpp_nested = out_is_nested and not isinstance(out, NestedTensor)
                if not out_is_cpp_nested:
                    out_size = out.shape
    
            if isinstance(grad, torch.Tensor):
                from torch.fx.experimental.symbolic_shapes import expect_true, sym_eq
    
                first_grad = grad if not is_grads_batched else grad[0]
    
                # TODO: We can remove this conditional once we uniformly use
                # singleton int to represent jagged dimension, so that size() call
                # on nested tensor works.
                if out_is_cpp_nested:
                    assert isinstance(out, torch.Tensor)
                    shape_matches = torch.is_same_size(out, first_grad)
                else:
                    # We need to do a regular size check, without going through
                    # the operator, to be able to handle unbacked symints
                    # (expect_true ensures we can deal with unbacked)
                    assert out_size is not None
                    shape_matches = expect_true(sym_eq(out_size, first_grad.size()))
    
                if not shape_matches:
                    out = cast(Union[torch.Tensor, graph.GradientEdge], out)
                    out_shape, grad_shape = _calculate_shape(
                        out, first_grad, is_grads_batched
                    )
                    if is_grads_batched:
                        raise RuntimeError(
                            "If `is_grads_batched=True`, we interpret the first "
                            "dimension of each grad_output as the batch dimension. "
                            "The sizes of the remaining dimensions are expected to match "
                            "the shape of corresponding output, but a mismatch "
                            "was detected: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + ". "
                            "If you only want some tensors in `grad_output` to be considered "
                            "batched, consider using vmap."
                        )
                    else:
                        raise RuntimeError(
                            "Mismatch in shape: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + "."
                        )
                if out_dtype.is_complex != grad.dtype.is_complex:
                    raise RuntimeError(
                        "For complex Tensors, both grad_output and output"
                        " are required to have the same dtype."
                        " Mismatch in dtype: grad_output["
                        + str(grads.index(grad))
                        + "] has a dtype of "
                        + str(grad.dtype)
                        + " and output["
                        + str(outputs.index(out))
                        + "] has a dtype of "
                        + str(out_dtype)
                        + "."
                    )
                new_grads.append(grad)
            elif grad is None:
                if isinstance(out, graph.GradientEdge) or out.requires_grad:  # type: ignore[attr-defined]
                    if isinstance(out, graph.GradientEdge):
                        assert out_size is not None
                        out_numel_is_1 = all(o == 1 for o in out_size)
                    else:
                        assert isinstance(out, torch.Tensor)
                        out_numel_is_1 = out.numel() == 1
                    if not out_numel_is_1:
>                       raise RuntimeError(
                            "grad can be implicitly created only for scalar outputs"
                        )
E                       RuntimeError: grad can be implicitly created only for scalar outputs

/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:198: RuntimeError
________________ test_jacobians_multioutput[BackPackInterface] _________________

multioutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=2, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]], requires_grad=True)
backend_cls = <class 'laplace.curvature.backpack.BackPackInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_jacobians_multioutput(multioutput_model, X, backend_cls):
        model = multioutput_model
        backend = backend_cls(model, "classification")
        Js, f = backend.jacobians(X)
>       Js_naive, f_naive = jacobians_naive(model, X)

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:72: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/utils.py:59: in jacobians_naive
    return jacobians_naive(model, data)
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/temp.py:28: in jacobians_naive
    f[i].backward(retain_graph=True)
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/_tensor.py:581: in backward
    torch.autograd.backward(
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:340: in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([ 0.2189, -0.4750], grad_fn=<SelectBackward0>),)
grads = (None,), is_grads_batched = False

    def _make_grads(
        outputs: Union[Sequence[torch.Tensor], Sequence[graph.GradientEdge]],
        grads: Sequence[_OptionalTensor],
        is_grads_batched: bool,
    ) -> Tuple[_OptionalTensor, ...]:
        new_grads: List[_OptionalTensor] = []
        for out, grad in zip(outputs, grads):
            out = cast(Union[torch.Tensor, graph.GradientEdge], out)
            out_size = None
            out_device = None
    
            if isinstance(out, graph.GradientEdge):
                out_metadata = out.node._input_metadata[out.output_nr]
                out_size = torch.Size(out_metadata.shape)
                out_dtype = out_metadata.dtype
                out_device = out_metadata.device
                out_is_nested = out_metadata.is_nested_tensor
                if out_metadata.is_cpp_nested_tensor:
                    raise RuntimeError(
                        "C++ NestedTensor are not supported with GradientEdge"
                    )
                out_is_cpp_nested = False
            else:
                # circular import
                from torch.nested._internal.nested_tensor import NestedTensor
    
                assert isinstance(out, torch.Tensor)
                out_dtype = out.dtype
                out_is_nested = out.is_nested
                out_is_cpp_nested = out_is_nested and not isinstance(out, NestedTensor)
                if not out_is_cpp_nested:
                    out_size = out.shape
    
            if isinstance(grad, torch.Tensor):
                from torch.fx.experimental.symbolic_shapes import expect_true, sym_eq
    
                first_grad = grad if not is_grads_batched else grad[0]
    
                # TODO: We can remove this conditional once we uniformly use
                # singleton int to represent jagged dimension, so that size() call
                # on nested tensor works.
                if out_is_cpp_nested:
                    assert isinstance(out, torch.Tensor)
                    shape_matches = torch.is_same_size(out, first_grad)
                else:
                    # We need to do a regular size check, without going through
                    # the operator, to be able to handle unbacked symints
                    # (expect_true ensures we can deal with unbacked)
                    assert out_size is not None
                    shape_matches = expect_true(sym_eq(out_size, first_grad.size()))
    
                if not shape_matches:
                    out = cast(Union[torch.Tensor, graph.GradientEdge], out)
                    out_shape, grad_shape = _calculate_shape(
                        out, first_grad, is_grads_batched
                    )
                    if is_grads_batched:
                        raise RuntimeError(
                            "If `is_grads_batched=True`, we interpret the first "
                            "dimension of each grad_output as the batch dimension. "
                            "The sizes of the remaining dimensions are expected to match "
                            "the shape of corresponding output, but a mismatch "
                            "was detected: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + ". "
                            "If you only want some tensors in `grad_output` to be considered "
                            "batched, consider using vmap."
                        )
                    else:
                        raise RuntimeError(
                            "Mismatch in shape: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + "."
                        )
                if out_dtype.is_complex != grad.dtype.is_complex:
                    raise RuntimeError(
                        "For complex Tensors, both grad_output and output"
                        " are required to have the same dtype."
                        " Mismatch in dtype: grad_output["
                        + str(grads.index(grad))
                        + "] has a dtype of "
                        + str(grad.dtype)
                        + " and output["
                        + str(outputs.index(out))
                        + "] has a dtype of "
                        + str(out_dtype)
                        + "."
                    )
                new_grads.append(grad)
            elif grad is None:
                if isinstance(out, graph.GradientEdge) or out.requires_grad:  # type: ignore[attr-defined]
                    if isinstance(out, graph.GradientEdge):
                        assert out_size is not None
                        out_numel_is_1 = all(o == 1 for o in out_size)
                    else:
                        assert isinstance(out, torch.Tensor)
                        out_numel_is_1 = out.numel() == 1
                    if not out_numel_is_1:
>                       raise RuntimeError(
                            "grad can be implicitly created only for scalar outputs"
                        )
E                       RuntimeError: grad can be implicitly created only for scalar outputs

/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:198: RuntimeError
__________ test_last_layer_jacobians_singleoutput[CurvatureInterface] __________

singleoutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=1, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...4e+00, -5.7150e-01],
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]])
backend_cls = <class 'laplace.curvature.curvature.CurvatureInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_last_layer_jacobians_singleoutput(singleoutput_model, X, backend_cls):
        model = FeatureExtractor(singleoutput_model)
        backend = backend_cls(model, "classification")
        Js, f = backend.last_layer_jacobians(X)
        _, phi = model.forward_with_features(X)
        Js_naive, f_naive = jacobians_naive(model.last_layer, phi)
>       assert Js.shape == Js_naive.shape
E       assert torch.Size([200, 1, 21]) == torch.Size([200, 200, 20])
E         
E         At index 1 diff: 1 != 200
E         
E         Full diff:
E         - torch.Size([200, 200, 20])
E         ?                   ^^^^^^
E         + torch.Size([200, 1, 21])
E         ?                  +++ ^

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:88: AssertionError
____________ test_last_layer_jacobians_singleoutput[AsdlInterface] _____________

singleoutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=1, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...4e+00, -5.7150e-01],
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]])
backend_cls = <class 'laplace.curvature.asdl.AsdlInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_last_layer_jacobians_singleoutput(singleoutput_model, X, backend_cls):
        model = FeatureExtractor(singleoutput_model)
        backend = backend_cls(model, "classification")
        Js, f = backend.last_layer_jacobians(X)
        _, phi = model.forward_with_features(X)
        Js_naive, f_naive = jacobians_naive(model.last_layer, phi)
>       assert Js.shape == Js_naive.shape
E       assert torch.Size([200, 1, 21]) == torch.Size([200, 200, 20])
E         
E         At index 1 diff: 1 != 200
E         
E         Full diff:
E         - torch.Size([200, 200, 20])
E         ?                   ^^^^^^
E         + torch.Size([200, 1, 21])
E         ?                  +++ ^

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:88: AssertionError
__________ test_last_layer_jacobians_singleoutput[BackPackInterface] ___________

singleoutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=1, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...4e+00, -5.7150e-01],
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]])
backend_cls = <class 'laplace.curvature.backpack.BackPackInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_last_layer_jacobians_singleoutput(singleoutput_model, X, backend_cls):
        model = FeatureExtractor(singleoutput_model)
        backend = backend_cls(model, "classification")
        Js, f = backend.last_layer_jacobians(X)
        _, phi = model.forward_with_features(X)
        Js_naive, f_naive = jacobians_naive(model.last_layer, phi)
>       assert Js.shape == Js_naive.shape
E       assert torch.Size([200, 1, 21]) == torch.Size([200, 200, 20])
E         
E         At index 1 diff: 1 != 200
E         
E         Full diff:
E         - torch.Size([200, 200, 20])
E         ?                   ^^^^^^
E         + torch.Size([200, 1, 21])
E         ?                  +++ ^

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:88: AssertionError
__________ test_last_layer_jacobians_multioutput[CurvatureInterface] ___________

multioutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=2, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...4e+00, -5.7150e-01],
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]])
backend_cls = <class 'laplace.curvature.curvature.CurvatureInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_last_layer_jacobians_multioutput(multioutput_model, X, backend_cls):
        model = FeatureExtractor(multioutput_model)
        backend = backend_cls(model, "classification")
        Js, f = backend.last_layer_jacobians(X)
        _, phi = model.forward_with_features(X)
>       Js_naive, f_naive = jacobians_naive(model.last_layer, phi)

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/utils.py:59: in jacobians_naive
    return jacobians_naive(model, data)
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/temp.py:28: in jacobians_naive
    f[i].backward(retain_graph=True)
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/_tensor.py:581: in backward
    torch.autograd.backward(
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:340: in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([ 0.2189, -0.4750], grad_fn=<SelectBackward0>),)
grads = (None,), is_grads_batched = False

    def _make_grads(
        outputs: Union[Sequence[torch.Tensor], Sequence[graph.GradientEdge]],
        grads: Sequence[_OptionalTensor],
        is_grads_batched: bool,
    ) -> Tuple[_OptionalTensor, ...]:
        new_grads: List[_OptionalTensor] = []
        for out, grad in zip(outputs, grads):
            out = cast(Union[torch.Tensor, graph.GradientEdge], out)
            out_size = None
            out_device = None
    
            if isinstance(out, graph.GradientEdge):
                out_metadata = out.node._input_metadata[out.output_nr]
                out_size = torch.Size(out_metadata.shape)
                out_dtype = out_metadata.dtype
                out_device = out_metadata.device
                out_is_nested = out_metadata.is_nested_tensor
                if out_metadata.is_cpp_nested_tensor:
                    raise RuntimeError(
                        "C++ NestedTensor are not supported with GradientEdge"
                    )
                out_is_cpp_nested = False
            else:
                # circular import
                from torch.nested._internal.nested_tensor import NestedTensor
    
                assert isinstance(out, torch.Tensor)
                out_dtype = out.dtype
                out_is_nested = out.is_nested
                out_is_cpp_nested = out_is_nested and not isinstance(out, NestedTensor)
                if not out_is_cpp_nested:
                    out_size = out.shape
    
            if isinstance(grad, torch.Tensor):
                from torch.fx.experimental.symbolic_shapes import expect_true, sym_eq
    
                first_grad = grad if not is_grads_batched else grad[0]
    
                # TODO: We can remove this conditional once we uniformly use
                # singleton int to represent jagged dimension, so that size() call
                # on nested tensor works.
                if out_is_cpp_nested:
                    assert isinstance(out, torch.Tensor)
                    shape_matches = torch.is_same_size(out, first_grad)
                else:
                    # We need to do a regular size check, without going through
                    # the operator, to be able to handle unbacked symints
                    # (expect_true ensures we can deal with unbacked)
                    assert out_size is not None
                    shape_matches = expect_true(sym_eq(out_size, first_grad.size()))
    
                if not shape_matches:
                    out = cast(Union[torch.Tensor, graph.GradientEdge], out)
                    out_shape, grad_shape = _calculate_shape(
                        out, first_grad, is_grads_batched
                    )
                    if is_grads_batched:
                        raise RuntimeError(
                            "If `is_grads_batched=True`, we interpret the first "
                            "dimension of each grad_output as the batch dimension. "
                            "The sizes of the remaining dimensions are expected to match "
                            "the shape of corresponding output, but a mismatch "
                            "was detected: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + ". "
                            "If you only want some tensors in `grad_output` to be considered "
                            "batched, consider using vmap."
                        )
                    else:
                        raise RuntimeError(
                            "Mismatch in shape: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + "."
                        )
                if out_dtype.is_complex != grad.dtype.is_complex:
                    raise RuntimeError(
                        "For complex Tensors, both grad_output and output"
                        " are required to have the same dtype."
                        " Mismatch in dtype: grad_output["
                        + str(grads.index(grad))
                        + "] has a dtype of "
                        + str(grad.dtype)
                        + " and output["
                        + str(outputs.index(out))
                        + "] has a dtype of "
                        + str(out_dtype)
                        + "."
                    )
                new_grads.append(grad)
            elif grad is None:
                if isinstance(out, graph.GradientEdge) or out.requires_grad:  # type: ignore[attr-defined]
                    if isinstance(out, graph.GradientEdge):
                        assert out_size is not None
                        out_numel_is_1 = all(o == 1 for o in out_size)
                    else:
                        assert isinstance(out, torch.Tensor)
                        out_numel_is_1 = out.numel() == 1
                    if not out_numel_is_1:
>                       raise RuntimeError(
                            "grad can be implicitly created only for scalar outputs"
                        )
E                       RuntimeError: grad can be implicitly created only for scalar outputs

/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:198: RuntimeError
_____________ test_last_layer_jacobians_multioutput[AsdlInterface] _____________

multioutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=2, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...4e+00, -5.7150e-01],
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]])
backend_cls = <class 'laplace.curvature.asdl.AsdlInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_last_layer_jacobians_multioutput(multioutput_model, X, backend_cls):
        model = FeatureExtractor(multioutput_model)
        backend = backend_cls(model, "classification")
        Js, f = backend.last_layer_jacobians(X)
        _, phi = model.forward_with_features(X)
>       Js_naive, f_naive = jacobians_naive(model.last_layer, phi)

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/utils.py:59: in jacobians_naive
    return jacobians_naive(model, data)
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/temp.py:28: in jacobians_naive
    f[i].backward(retain_graph=True)
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/_tensor.py:581: in backward
    torch.autograd.backward(
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:340: in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([ 0.2189, -0.4750], grad_fn=<SelectBackward0>),)
grads = (None,), is_grads_batched = False

    def _make_grads(
        outputs: Union[Sequence[torch.Tensor], Sequence[graph.GradientEdge]],
        grads: Sequence[_OptionalTensor],
        is_grads_batched: bool,
    ) -> Tuple[_OptionalTensor, ...]:
        new_grads: List[_OptionalTensor] = []
        for out, grad in zip(outputs, grads):
            out = cast(Union[torch.Tensor, graph.GradientEdge], out)
            out_size = None
            out_device = None
    
            if isinstance(out, graph.GradientEdge):
                out_metadata = out.node._input_metadata[out.output_nr]
                out_size = torch.Size(out_metadata.shape)
                out_dtype = out_metadata.dtype
                out_device = out_metadata.device
                out_is_nested = out_metadata.is_nested_tensor
                if out_metadata.is_cpp_nested_tensor:
                    raise RuntimeError(
                        "C++ NestedTensor are not supported with GradientEdge"
                    )
                out_is_cpp_nested = False
            else:
                # circular import
                from torch.nested._internal.nested_tensor import NestedTensor
    
                assert isinstance(out, torch.Tensor)
                out_dtype = out.dtype
                out_is_nested = out.is_nested
                out_is_cpp_nested = out_is_nested and not isinstance(out, NestedTensor)
                if not out_is_cpp_nested:
                    out_size = out.shape
    
            if isinstance(grad, torch.Tensor):
                from torch.fx.experimental.symbolic_shapes import expect_true, sym_eq
    
                first_grad = grad if not is_grads_batched else grad[0]
    
                # TODO: We can remove this conditional once we uniformly use
                # singleton int to represent jagged dimension, so that size() call
                # on nested tensor works.
                if out_is_cpp_nested:
                    assert isinstance(out, torch.Tensor)
                    shape_matches = torch.is_same_size(out, first_grad)
                else:
                    # We need to do a regular size check, without going through
                    # the operator, to be able to handle unbacked symints
                    # (expect_true ensures we can deal with unbacked)
                    assert out_size is not None
                    shape_matches = expect_true(sym_eq(out_size, first_grad.size()))
    
                if not shape_matches:
                    out = cast(Union[torch.Tensor, graph.GradientEdge], out)
                    out_shape, grad_shape = _calculate_shape(
                        out, first_grad, is_grads_batched
                    )
                    if is_grads_batched:
                        raise RuntimeError(
                            "If `is_grads_batched=True`, we interpret the first "
                            "dimension of each grad_output as the batch dimension. "
                            "The sizes of the remaining dimensions are expected to match "
                            "the shape of corresponding output, but a mismatch "
                            "was detected: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + ". "
                            "If you only want some tensors in `grad_output` to be considered "
                            "batched, consider using vmap."
                        )
                    else:
                        raise RuntimeError(
                            "Mismatch in shape: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + "."
                        )
                if out_dtype.is_complex != grad.dtype.is_complex:
                    raise RuntimeError(
                        "For complex Tensors, both grad_output and output"
                        " are required to have the same dtype."
                        " Mismatch in dtype: grad_output["
                        + str(grads.index(grad))
                        + "] has a dtype of "
                        + str(grad.dtype)
                        + " and output["
                        + str(outputs.index(out))
                        + "] has a dtype of "
                        + str(out_dtype)
                        + "."
                    )
                new_grads.append(grad)
            elif grad is None:
                if isinstance(out, graph.GradientEdge) or out.requires_grad:  # type: ignore[attr-defined]
                    if isinstance(out, graph.GradientEdge):
                        assert out_size is not None
                        out_numel_is_1 = all(o == 1 for o in out_size)
                    else:
                        assert isinstance(out, torch.Tensor)
                        out_numel_is_1 = out.numel() == 1
                    if not out_numel_is_1:
>                       raise RuntimeError(
                            "grad can be implicitly created only for scalar outputs"
                        )
E                       RuntimeError: grad can be implicitly created only for scalar outputs

/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:198: RuntimeError
___________ test_last_layer_jacobians_multioutput[BackPackInterface] ___________

multioutput_model = Sequential(
  (0): Linear(in_features=3, out_features=20, bias=True)
  (1): Linear(in_features=20, out_features=2, bias=True)
)
X = tensor([[-8.3996e-01,  1.4799e-01, -7.6018e-01],
        [-7.5407e-01, -8.8915e-01, -1.0300e+00],
        [ 1.1966e+00...4e+00, -5.7150e-01],
        [ 3.5002e-01,  6.7761e-01,  2.2537e+00],
        [ 1.6888e+00, -8.5980e-01, -9.0062e-01]])
backend_cls = <class 'laplace.curvature.backpack.BackPackInterface'>

    @pytest.mark.parametrize(
        "backend_cls", [CurvatureInterface, AsdlInterface, BackPackInterface]
    )
    def test_last_layer_jacobians_multioutput(multioutput_model, X, backend_cls):
        model = FeatureExtractor(multioutput_model)
        backend = backend_cls(model, "classification")
        Js, f = backend.last_layer_jacobians(X)
        _, phi = model.forward_with_features(X)
>       Js_naive, f_naive = jacobians_naive(model.last_layer, phi)

/local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py:102: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/utils.py:59: in jacobians_naive
    return jacobians_naive(model, data)
/local/data0/moved_data/publishablew/Laplace/Laplace/tests/temp.py:28: in jacobians_naive
    f[i].backward(retain_graph=True)
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/_tensor.py:581: in backward
    torch.autograd.backward(
/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:340: in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

outputs = (tensor([ 0.2189, -0.4750], grad_fn=<SelectBackward0>),)
grads = (None,), is_grads_batched = False

    def _make_grads(
        outputs: Union[Sequence[torch.Tensor], Sequence[graph.GradientEdge]],
        grads: Sequence[_OptionalTensor],
        is_grads_batched: bool,
    ) -> Tuple[_OptionalTensor, ...]:
        new_grads: List[_OptionalTensor] = []
        for out, grad in zip(outputs, grads):
            out = cast(Union[torch.Tensor, graph.GradientEdge], out)
            out_size = None
            out_device = None
    
            if isinstance(out, graph.GradientEdge):
                out_metadata = out.node._input_metadata[out.output_nr]
                out_size = torch.Size(out_metadata.shape)
                out_dtype = out_metadata.dtype
                out_device = out_metadata.device
                out_is_nested = out_metadata.is_nested_tensor
                if out_metadata.is_cpp_nested_tensor:
                    raise RuntimeError(
                        "C++ NestedTensor are not supported with GradientEdge"
                    )
                out_is_cpp_nested = False
            else:
                # circular import
                from torch.nested._internal.nested_tensor import NestedTensor
    
                assert isinstance(out, torch.Tensor)
                out_dtype = out.dtype
                out_is_nested = out.is_nested
                out_is_cpp_nested = out_is_nested and not isinstance(out, NestedTensor)
                if not out_is_cpp_nested:
                    out_size = out.shape
    
            if isinstance(grad, torch.Tensor):
                from torch.fx.experimental.symbolic_shapes import expect_true, sym_eq
    
                first_grad = grad if not is_grads_batched else grad[0]
    
                # TODO: We can remove this conditional once we uniformly use
                # singleton int to represent jagged dimension, so that size() call
                # on nested tensor works.
                if out_is_cpp_nested:
                    assert isinstance(out, torch.Tensor)
                    shape_matches = torch.is_same_size(out, first_grad)
                else:
                    # We need to do a regular size check, without going through
                    # the operator, to be able to handle unbacked symints
                    # (expect_true ensures we can deal with unbacked)
                    assert out_size is not None
                    shape_matches = expect_true(sym_eq(out_size, first_grad.size()))
    
                if not shape_matches:
                    out = cast(Union[torch.Tensor, graph.GradientEdge], out)
                    out_shape, grad_shape = _calculate_shape(
                        out, first_grad, is_grads_batched
                    )
                    if is_grads_batched:
                        raise RuntimeError(
                            "If `is_grads_batched=True`, we interpret the first "
                            "dimension of each grad_output as the batch dimension. "
                            "The sizes of the remaining dimensions are expected to match "
                            "the shape of corresponding output, but a mismatch "
                            "was detected: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + ". "
                            "If you only want some tensors in `grad_output` to be considered "
                            "batched, consider using vmap."
                        )
                    else:
                        raise RuntimeError(
                            "Mismatch in shape: grad_output["
                            + str(grads.index(grad))
                            + "] has a shape of "
                            + str(grad_shape)
                            + " and output["
                            + str(outputs.index(out))
                            + "] has a shape of "
                            + str(out_shape)
                            + "."
                        )
                if out_dtype.is_complex != grad.dtype.is_complex:
                    raise RuntimeError(
                        "For complex Tensors, both grad_output and output"
                        " are required to have the same dtype."
                        " Mismatch in dtype: grad_output["
                        + str(grads.index(grad))
                        + "] has a dtype of "
                        + str(grad.dtype)
                        + " and output["
                        + str(outputs.index(out))
                        + "] has a dtype of "
                        + str(out_dtype)
                        + "."
                    )
                new_grads.append(grad)
            elif grad is None:
                if isinstance(out, graph.GradientEdge) or out.requires_grad:  # type: ignore[attr-defined]
                    if isinstance(out, graph.GradientEdge):
                        assert out_size is not None
                        out_numel_is_1 = all(o == 1 for o in out_size)
                    else:
                        assert isinstance(out, torch.Tensor)
                        out_numel_is_1 = out.numel() == 1
                    if not out_numel_is_1:
>                       raise RuntimeError(
                            "grad can be implicitly created only for scalar outputs"
                        )
E                       RuntimeError: grad can be implicitly created only for scalar outputs

/local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:198: RuntimeError
=============================== warnings summary ===============================
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:18
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:18: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    TORCH_VERSION = LooseVersion(version("torch"))

../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:19
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:19: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    TORCH_VERSION_AT_LEAST_1_12_0 = TORCH_VERSION >= LooseVersion("1.12.0")

tests/test_jacobians.py::test_linear_jacobians[AsdlInterface]
tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface]
tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface]
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
    self._maybe_warn_non_full_backward_hook(args, result, grad_fn)

tests/test_jacobians.py::test_backprop_jacobians_singleoutput[BackPackInterface]
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1201.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[CurvatureInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[BackPackInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[CurvatureInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[BackPackInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[CurvatureInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[AsdlInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[BackPackInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[CurvatureInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[AsdlInterface]
FAILED ../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[BackPackInterface]
================== 12 failed, 11 passed, 6 warnings in 2.49s ===================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/Laplace/Laplace/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/Laplace/Laplace
configfile: pyproject.toml
plugins: mock-3.14.0, cov-6.0.0
collecting ... collected 23 items

../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_multioutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_multioutput[BackPackInterface] PASSED

=============================== warnings summary ===============================
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:18
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:18: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    TORCH_VERSION = LooseVersion(version("torch"))

../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:19
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:19: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    TORCH_VERSION_AT_LEAST_1_12_0 = TORCH_VERSION >= LooseVersion("1.12.0")

tests/test_jacobians.py::test_linear_jacobians[AsdlInterface]
tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface]
tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface]
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
    self._maybe_warn_non_full_backward_hook(args, result, grad_fn)

tests/test_jacobians.py::test_backprop_jacobians_singleoutput[BackPackInterface]
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1201.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================== 23 passed, 6 warnings in 2.51s ========================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.11.10, pytest-8.3.4, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/Laplace/Laplace/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/Laplace/Laplace
configfile: pyproject.toml
plugins: mock-3.14.0, cov-6.0.0
collecting ... collected 23 items

../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_linear_jacobians[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_jacobians_multioutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[AsdlInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_last_layer_jacobians_multioutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_jacobians_multioutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_singleoutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_singleoutput[BackPackInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_multioutput[CurvatureInterface] PASSED
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/tests/test_jacobians.py::test_backprop_last_layer_jacobians_multioutput[BackPackInterface] PASSED

=============================== warnings summary ===============================
../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:18
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:18: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    TORCH_VERSION = LooseVersion(version("torch"))

../../../../../../local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:19
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/unfoldNd/utils.py:19: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    TORCH_VERSION_AT_LEAST_1_12_0 = TORCH_VERSION >= LooseVersion("1.12.0")

tests/test_jacobians.py::test_linear_jacobians[AsdlInterface]
tests/test_jacobians.py::test_jacobians_singleoutput[AsdlInterface]
tests/test_jacobians.py::test_jacobians_multioutput[AsdlInterface]
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
    self._maybe_warn_non_full_backward_hook(args, result, grad_fn)

tests/test_jacobians.py::test_backprop_jacobians_singleoutput[BackPackInterface]
  /local/data0/moved_data/publishablew/Laplace/Laplace/venv/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at ../torch/csrc/autograd/engine.cpp:1201.)
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================== 23 passed, 6 warnings in 2.60s ========================
