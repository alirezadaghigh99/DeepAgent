output file:
processed_scikit-learnscale58.json
function:
scale
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler0] FAILED', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler0] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler0]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler0]', '../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler0] FAILED'}

All Test Cases On Generated code:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 120 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-None-scaler0] I: Seeding RNGs with 1680810078
PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler0] FAILED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_array-scaler1] SKIPPED

=================================== FAILURES ===================================
__ test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler0] __

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_matrix'>
dtype = <class 'numpy.float32'>, constant = 0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float32'>'
	with 0 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
__ test_standard_scaler_constant_features[0-float32-False-csc_array-scaler0] ___

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_array'>
dtype = <class 'numpy.float32'>, constant = 0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float32'>'
	with 0 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
__ test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler0] __

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_matrix'>
dtype = <class 'numpy.float32'>, constant = 0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float32'>'
	with 0 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
__ test_standard_scaler_constant_features[0-float32-False-csr_array-scaler0] ___

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_array'>
dtype = <class 'numpy.float32'>, constant = 0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float32'>'
	with 0 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
__ test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler0] __

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_matrix'>
dtype = <class 'numpy.float64'>, constant = 0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float64'>'
	with 0 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
__ test_standard_scaler_constant_features[0-float64-False-csc_array-scaler0] ___

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_array'>
dtype = <class 'numpy.float64'>, constant = 0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float64'>'
	with 0 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
__ test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler0] __

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_matrix'>
dtype = <class 'numpy.float64'>, constant = 0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float64'>'
	with 0 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
__ test_standard_scaler_constant_features[0-float64-False-csr_array-scaler0] ___

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_array'>
dtype = <class 'numpy.float64'>, constant = 0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float64'>'
	with 0 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
____ test_standard_scaler_constant_features[1.0-float32-False-None-scaler0] ____

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = None, dtype = <class 'numpy.float32'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
            X_scaled_2 = scale(X, with_mean=scaler.with_mean)
            assert X_scaled_2 is not X  # make sure we did a copy
>           assert_allclose_dense_sparse(X_scaled_2, X)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:269: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:285: in assert_allclose_dense_sparse
    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:239: in assert_allclose
    np_assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<function assert_allclose.<locals>.compare at 0x77e650969670>, array([[0.],
       [0.],
       [0.],
       [0.],
  ...      [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.]], dtype=float32))
kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-07, atol=1e-09', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Not equal to tolerance rtol=1e-07, atol=1e-09
E           
E           Mismatched elements: 100 / 100 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: 1.
E            ACTUAL: array([[0.],
E                  [0.],
E                  [0.],...
E            DESIRED: array([[1.],
E                  [1.],
E                  [1.],...

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
_ test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_matrix'>
dtype = <class 'numpy.float32'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float32'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler0] __

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_array'>
dtype = <class 'numpy.float32'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float32'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_matrix'>
dtype = <class 'numpy.float32'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float32'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler0] __

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_array'>
dtype = <class 'numpy.float32'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float32'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
____ test_standard_scaler_constant_features[1.0-float64-False-None-scaler0] ____

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = None, dtype = <class 'numpy.float64'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
            X_scaled_2 = scale(X, with_mean=scaler.with_mean)
            assert X_scaled_2 is not X  # make sure we did a copy
>           assert_allclose_dense_sparse(X_scaled_2, X)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:269: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:285: in assert_allclose_dense_sparse
    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:239: in assert_allclose
    np_assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<function assert_allclose.<locals>.compare at 0x77e650a9ad30>, array([[0.],
       [0.],
       [0.],
       [0.],
  ...
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.],
       [1.]]))
kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-07, atol=1e-09', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Not equal to tolerance rtol=1e-07, atol=1e-09
E           
E           Mismatched elements: 100 / 100 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: 1.
E            ACTUAL: array([[0.],
E                  [0.],
E                  [0.],...
E            DESIRED: array([[1.],
E                  [1.],
E                  [1.],...

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
_ test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_matrix'>
dtype = <class 'numpy.float64'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float64'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler0] __

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_array'>
dtype = <class 'numpy.float64'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float64'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_matrix'>
dtype = <class 'numpy.float64'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float64'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler0] __

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_array'>
dtype = <class 'numpy.float64'>, constant = 1.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float64'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
___ test_standard_scaler_constant_features[100.0-float32-False-None-scaler0] ___

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = None, dtype = <class 'numpy.float32'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
            X_scaled_2 = scale(X, with_mean=scaler.with_mean)
            assert X_scaled_2 is not X  # make sure we did a copy
>           assert_allclose_dense_sparse(X_scaled_2, X)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:269: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:285: in assert_allclose_dense_sparse
    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:239: in assert_allclose
    np_assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<function assert_allclose.<locals>.compare at 0x77e6504a1ca0>, array([[0.],
       [0.],
       [0.],
       [0.],
  ...     [100.],
       [100.],
       [100.],
       [100.],
       [100.],
       [100.],
       [100.]], dtype=float32))
kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-07, atol=1e-09', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Not equal to tolerance rtol=1e-07, atol=1e-09
E           
E           Mismatched elements: 100 / 100 (100%)
E           Max absolute difference among violations: 100.
E           Max relative difference among violations: 1.
E            ACTUAL: array([[0.],
E                  [0.],
E                  [0.],...
E            DESIRED: array([[100.],
E                  [100.],
E                  [100.],...

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
_ test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_matrix'>
dtype = <class 'numpy.float32'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float32'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_array'>
dtype = <class 'numpy.float32'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float32'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_matrix'>
dtype = <class 'numpy.float32'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float32'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_array'>
dtype = <class 'numpy.float32'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float32'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
___ test_standard_scaler_constant_features[100.0-float64-False-None-scaler0] ___

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = None, dtype = <class 'numpy.float64'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
            X_scaled_2 = scale(X, with_mean=scaler.with_mean)
            assert X_scaled_2 is not X  # make sure we did a copy
>           assert_allclose_dense_sparse(X_scaled_2, X)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:269: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:285: in assert_allclose_dense_sparse
    assert_allclose(x, y, rtol=rtol, atol=atol, err_msg=err_msg)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_testing.py:239: in assert_allclose
    np_assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = (<function assert_allclose.<locals>.compare at 0x77e650a9a4c0>, array([[0.],
       [0.],
       [0.],
       [0.],
  ...     [100.],
       [100.],
       [100.],
       [100.],
       [100.],
       [100.],
       [100.],
       [100.]]))
kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-07, atol=1e-09', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError: 
E           Not equal to tolerance rtol=1e-07, atol=1e-09
E           
E           Mismatched elements: 100 / 100 (100%)
E           Max absolute difference among violations: 100.
E           Max relative difference among violations: 1.
E            ACTUAL: array([[0.],
E                  [0.],
E                  [0.],...
E            DESIRED: array([[100.],
E                  [100.],
E                  [100.],...

/usr/local/lib/python3.9/contextlib.py:79: AssertionError
_ test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_matrix'>
dtype = <class 'numpy.float64'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float64'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csc.csc_array'>
dtype = <class 'numpy.float64'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float64'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_matrix'>
dtype = <class 'numpy.float64'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse matrix of type '<class 'numpy.float64'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
_ test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler0] _

scaler = StandardScaler(with_mean=False), add_sample_weight = False
sparse_container = <class 'scipy.sparse._csr.csr_array'>
dtype = <class 'numpy.float64'>, constant = 100.0

    @pytest.mark.parametrize(
        "scaler",
        [
            StandardScaler(with_mean=False),
            RobustScaler(with_centering=False),
        ],
    )
    @pytest.mark.parametrize("sparse_container", [None] + CSC_CONTAINERS + CSR_CONTAINERS)
    @pytest.mark.parametrize("add_sample_weight", [False, True])
    @pytest.mark.parametrize("dtype", [np.float32, np.float64])
    @pytest.mark.parametrize("constant", [0, 1.0, 100.0])
    def test_standard_scaler_constant_features(
        scaler, add_sample_weight, sparse_container, dtype, constant
    ):
        if isinstance(scaler, RobustScaler) and add_sample_weight:
            pytest.skip(f"{scaler.__class__.__name__} does not yet support sample_weight")
    
        rng = np.random.RandomState(0)
        n_samples = 100
        n_features = 1
        if add_sample_weight:
            fit_params = dict(sample_weight=rng.uniform(size=n_samples) * 2)
        else:
            fit_params = {}
        X_array = np.full(shape=(n_samples, n_features), fill_value=constant, dtype=dtype)
        X = X_array if sparse_container is None else sparse_container(X_array)
        X_scaled = scaler.fit(X, **fit_params).transform(X)
    
        if isinstance(scaler, StandardScaler):
            # The variance info should be close to zero for constant features.
            assert_allclose(scaler.var_, np.zeros(X.shape[1]), atol=1e-7)
    
        # Constant features should not be scaled (scale of 1.):
        assert_allclose(scaler.scale_, np.ones(X.shape[1]))
    
        assert X_scaled is not X  # make sure we make a copy
        assert_allclose_dense_sparse(X_scaled, X)
    
        if isinstance(scaler, StandardScaler) and not add_sample_weight:
            # Also check consistency with the standard scale function.
>           X_scaled_2 = scale(X, with_mean=scaler.with_mean)

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:267: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/utils/_param_validation.py:216: in wrapper
    return func(*args, **kwargs)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/_data.py:62: in scale
    return scale(X)
/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/temp.py:25: in scale
    mean = np.mean(X, axis=axis, keepdims=True)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

a = <100x1 sparse array of type '<class 'numpy.float64'>'
	with 100 stored elements in Compressed Sparse Column format>
axis = 0, dtype = None, out = None, keepdims = True

    @array_function_dispatch(_mean_dispatcher)
    def mean(a, axis=None, dtype=None, out=None, keepdims=np._NoValue, *,
             where=np._NoValue):
        """
        Compute the arithmetic mean along the specified axis.
    
        Returns the average of the array elements.  The average is taken over
        the flattened array by default, otherwise over the specified axis.
        `float64` intermediate and return values are used for integer inputs.
    
        Parameters
        ----------
        a : array_like
            Array containing numbers whose mean is desired. If `a` is not an
            array, a conversion is attempted.
        axis : None or int or tuple of ints, optional
            Axis or axes along which the means are computed. The default is to
            compute the mean of the flattened array.
    
            .. versionadded:: 1.7.0
    
            If this is a tuple of ints, a mean is performed over multiple axes,
            instead of a single axis or all the axes as before.
        dtype : data-type, optional
            Type to use in computing the mean.  For integer inputs, the default
            is `float64`; for floating point inputs, it is the same as the
            input dtype.
        out : ndarray, optional
            Alternate output array in which to place the result.  The default
            is ``None``; if provided, it must have the same shape as the
            expected output, but the type will be cast if necessary.
            See :ref:`ufuncs-output-type` for more details.
            See :ref:`ufuncs-output-type` for more details.
    
        keepdims : bool, optional
            If this is set to True, the axes which are reduced are left
            in the result as dimensions with size one. With this option,
            the result will broadcast correctly against the input array.
    
            If the default value is passed, then `keepdims` will not be
            passed through to the `mean` method of sub-classes of
            `ndarray`, however any non-default value will be.  If the
            sub-class' method does not implement `keepdims` any
            exceptions will be raised.
    
        where : array_like of bool, optional
            Elements to include in the mean. See `~numpy.ufunc.reduce` for details.
    
            .. versionadded:: 1.20.0
    
        Returns
        -------
        m : ndarray, see dtype parameter above
            If `out=None`, returns a new array containing the mean values,
            otherwise a reference to the output array is returned.
    
        See Also
        --------
        average : Weighted average
        std, var, nanmean, nanstd, nanvar
    
        Notes
        -----
        The arithmetic mean is the sum of the elements along the axis divided
        by the number of elements.
    
        Note that for floating-point input, the mean is computed using the
        same precision the input has.  Depending on the input data, this can
        cause the results to be inaccurate, especially for `float32` (see
        example below).  Specifying a higher-precision accumulator using the
        `dtype` keyword can alleviate this issue.
    
        By default, `float16` results are computed using `float32` intermediates
        for extra precision.
    
        Examples
        --------
        >>> a = np.array([[1, 2], [3, 4]])
        >>> np.mean(a)
        2.5
        >>> np.mean(a, axis=0)
        array([2., 3.])
        >>> np.mean(a, axis=1)
        array([1.5, 3.5])
    
        In single precision, `mean` can be inaccurate:
    
        >>> a = np.zeros((2, 512*512), dtype=np.float32)
        >>> a[0, :] = 1.0
        >>> a[1, :] = 0.1
        >>> np.mean(a)
        0.54999924
    
        Computing the mean in float64 is more accurate:
    
        >>> np.mean(a, dtype=np.float64)
        0.55000000074505806 # may vary
    
        Specifying a where argument:
    
        >>> a = np.array([[5, 9, 13], [14, 10, 12], [11, 15, 19]])
        >>> np.mean(a)
        12.0
        >>> np.mean(a, where=[[True], [False], [False]])
        9.0
    
        """
        kwargs = {}
        if keepdims is not np._NoValue:
            kwargs['keepdims'] = keepdims
        if where is not np._NoValue:
            kwargs['where'] = where
        if type(a) is not mu.ndarray:
            try:
                mean = a.mean
            except AttributeError:
                pass
            else:
>               return mean(axis=axis, dtype=dtype, out=out, **kwargs)
E               TypeError: mean() got an unexpected keyword argument 'keepdims'

/local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/lib/python3.9/site-packages/numpy/_core/fromnumeric.py:3594: TypeError
=========================== short test summary info ============================
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler0]
FAILED ../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler0]
================== 28 failed, 62 passed, 30 skipped in 2.57s ===================


Final Test Result:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 120 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-None-scaler0] I: Seeding RNGs with 19979935
PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_array-scaler1] SKIPPED

======================== 90 passed, 30 skipped in 0.60s ========================


Initial Result:
============================= test session starts ==============================
platform linux -- Python 3.9.0, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/scikit-learn/scikit-learn/venv/bin/python
cachedir: .pytest_cache
rootdir: /local/data0/moved_data/publishablew/scikit-learn/scikit-learn
configfile: setup.cfg
plugins: cov-6.0.0
collecting ... collected 120 items

../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-None-scaler0] I: Seeding RNGs with 1648706187
PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[0-float64-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[1.0-float64-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float32-True-csr_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-None-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csc_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_matrix-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-False-csr_array-scaler1] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-None-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-None-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csc_array-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_matrix-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_matrix-scaler1] SKIPPED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_array-scaler0] PASSED
../../../../../../local/data0/moved_data/publishablew/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py::test_standard_scaler_constant_features[100.0-float64-True-csr_array-scaler1] SKIPPED

======================== 90 passed, 30 skipped in 0.63s ========================
