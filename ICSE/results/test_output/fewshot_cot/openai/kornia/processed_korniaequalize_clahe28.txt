output file:
processed_korniaequalize_clahe28.json
function:
equalize_clahe
Error Cases:

Pass or Failed: 0

Related Failed Test Cases:
{'../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_he[cpu-float32] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-None-grid1]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_clahe[cpu-float32]', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_gradcheck[cpu]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_clahe[cpu-float32] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-None-grid1] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-2.0-grid2]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_ahe[cpu-float32] FAILED', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-2.0-grid2] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_he[cpu-float32]', '../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_gradcheck[cpu] FAILED', 'FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_ahe[cpu-float32]'}

All Test Cases On Generated code:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'jit', 'onnxrt', 'cudagraphs', 'openxla', 'tvm', 'inductor', None}
model weights cached: []

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1, jaxtyping-0.2.38
collecting ... collected 25 items

../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_smoke[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-None-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-None-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-1-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-1-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-4-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-0.0-None] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-None-grid1] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-2.0-grid2] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[0-1.0-grid0-ValueError-Invalid input tensor, it is empty.] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-1-grid1-TypeError-Input clip_limit type is not float. Got] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-2-TypeError-Input grid_size type is not Tuple. Got] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid3-TypeError-Input grid_size is not a Tuple with 2 elements. Got 3] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid4-TypeError-Input grid_size type is not valid, must be a Tuple[int, int]] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid5-ValueError-Input grid_size elements must be positive. Got] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_dims[dims0] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_dims[dims1] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_type PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_gradcheck[cpu] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_jit[cpu-float32] SKIPPED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_module PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_he[cpu-float32] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_ahe[cpu-float32] FAILED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_clahe[cpu-float32] FAILED

=================================== FAILURES ===================================
________ TestEqualization.test_optional_params[cpu-float32-None-grid1] _________

self = <test_equalization.TestEqualization object at 0x73ebcc33a380>
clip = None, grid = (2, 2), device = device(type='cpu'), dtype = torch.float32

    @pytest.mark.parametrize("clip, grid", [(0.0, None), (None, (2, 2)), (2.0, (2, 2))])
    def test_optional_params(self, clip, grid, device, dtype):
        C, H, W = 1, 10, 20
        img = torch.rand(C, H, W, device=device, dtype=dtype)
        if clip is None:
>           res = enhance.equalize_clahe(img, grid_size=grid)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/utils/image.py:272: in _wrapper
    output = f(input, *args, **kwargs)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/equalization.py:205: in equalize_clahe
    return equalize_clahe(input, clip_limit, grid_size, slow_and_differentiable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[3.9009e-01, 2.3420e-01, 9.2511e-01, 2.6824e-01, 7.0799e-01,
           1.3820e-01, 4.3413e-01, 4.3716e-01, ...117e-01, 9.7103e-01, 5.1711e-01, 7.5489e-01,
           5.7929e-01, 6.2947e-01, 6.8820e-02, 2.7813e-01, 2.8874e-01]]]])
clip_limit = 40.0, grid_size = (2, 2), slow_and_differentiable = False

    def equalize_clahe(input: torch.Tensor, clip_limit: float, grid_size: Tuple[int, int], slow_and_differentiable: bool=False) -> torch.Tensor:
        """
        Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to an input image tensor.
    
        Args:
            input (Tensor): Input image tensor with shape (*, C, H, W) and values in [0, 1].
            clip_limit (float): Threshold for contrast limiting. If 0, clipping is disabled.
            grid_size (Tuple[int, int]): Number of tiles to divide the image into (rows, cols).
            slow_and_differentiable (bool): Use a slow but differentiable implementation if True.
    
        Returns:
            Tensor: Image tensor with the same shape as input after applying CLAHE.
        """
        if not isinstance(clip_limit, float):
            raise TypeError('clip_limit must be a float.')
        if not (isinstance(grid_size, tuple) and len(grid_size) == 2 and all((isinstance(x, int) for x in grid_size))):
            raise TypeError('grid_size must be a tuple of two integers.')
        if any((x <= 0 for x in grid_size)):
            raise ValueError('All elements of grid_size must be positive.')
        if input.min() < 0 or input.max() > 1:
            raise ValueError('Input tensor values must be in the range [0, 1].')
        original_shape = input.shape
        if input.dim() == 3:
            input = input.unsqueeze(0)
        batch_size, channels, height, width = input.shape
        grid_rows, grid_cols = grid_size
        tile_height = height // grid_rows
        tile_width = width // grid_cols
        output = torch.zeros_like(input)
        for b in range(batch_size):
            for c in range(channels):
                for i in range(grid_rows):
                    for j in range(grid_cols):
                        h_start = i * tile_height
                        w_start = j * tile_width
                        h_end = h_start + tile_height
                        w_end = w_start + tile_width
                        tile = input[b, c, h_start:h_end, w_start:w_end]
                        hist = torch.histc(tile, bins=256, min=0, max=1)
                        if clip_limit > 0:
                            excess = hist - clip_limit
                            excess[excess < 0] = 0
                            hist = hist + excess.sum() / 256
                        cdf = hist.cumsum(0)
                        cdf = (cdf - cdf.min()) / (cdf.max() - cdf.min())
>                       tile_flat = tile.view(-1)
E                       RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/temp.py:56: RuntimeError
_________ TestEqualization.test_optional_params[cpu-float32-2.0-grid2] _________

self = <test_equalization.TestEqualization object at 0x73ebcc33a6e0>, clip = 2.0
grid = (2, 2), device = device(type='cpu'), dtype = torch.float32

    @pytest.mark.parametrize("clip, grid", [(0.0, None), (None, (2, 2)), (2.0, (2, 2))])
    def test_optional_params(self, clip, grid, device, dtype):
        C, H, W = 1, 10, 20
        img = torch.rand(C, H, W, device=device, dtype=dtype)
        if clip is None:
            res = enhance.equalize_clahe(img, grid_size=grid)
        elif grid is None:
            res = enhance.equalize_clahe(img, clip_limit=clip)
        else:
>           res = enhance.equalize_clahe(img, clip, grid)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/utils/image.py:272: in _wrapper
    output = f(input, *args, **kwargs)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/equalization.py:205: in equalize_clahe
    return equalize_clahe(input, clip_limit, grid_size, slow_and_differentiable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[1.4772e-01, 1.6022e-01, 2.7243e-01, 8.6802e-01, 3.5756e-01,
           1.3870e-01, 9.5618e-01, 1.6126e-01, ...220e-01, 7.9292e-01, 3.3922e-01, 6.6645e-01,
           8.9751e-01, 2.0817e-01, 3.7356e-01, 6.2608e-01, 9.3465e-01]]]])
clip_limit = 2.0, grid_size = (2, 2), slow_and_differentiable = False

    def equalize_clahe(input: torch.Tensor, clip_limit: float, grid_size: Tuple[int, int], slow_and_differentiable: bool=False) -> torch.Tensor:
        """
        Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to an input image tensor.
    
        Args:
            input (Tensor): Input image tensor with shape (*, C, H, W) and values in [0, 1].
            clip_limit (float): Threshold for contrast limiting. If 0, clipping is disabled.
            grid_size (Tuple[int, int]): Number of tiles to divide the image into (rows, cols).
            slow_and_differentiable (bool): Use a slow but differentiable implementation if True.
    
        Returns:
            Tensor: Image tensor with the same shape as input after applying CLAHE.
        """
        if not isinstance(clip_limit, float):
            raise TypeError('clip_limit must be a float.')
        if not (isinstance(grid_size, tuple) and len(grid_size) == 2 and all((isinstance(x, int) for x in grid_size))):
            raise TypeError('grid_size must be a tuple of two integers.')
        if any((x <= 0 for x in grid_size)):
            raise ValueError('All elements of grid_size must be positive.')
        if input.min() < 0 or input.max() > 1:
            raise ValueError('Input tensor values must be in the range [0, 1].')
        original_shape = input.shape
        if input.dim() == 3:
            input = input.unsqueeze(0)
        batch_size, channels, height, width = input.shape
        grid_rows, grid_cols = grid_size
        tile_height = height // grid_rows
        tile_width = width // grid_cols
        output = torch.zeros_like(input)
        for b in range(batch_size):
            for c in range(channels):
                for i in range(grid_rows):
                    for j in range(grid_cols):
                        h_start = i * tile_height
                        w_start = j * tile_width
                        h_end = h_start + tile_height
                        w_end = w_start + tile_width
                        tile = input[b, c, h_start:h_end, w_start:w_end]
                        hist = torch.histc(tile, bins=256, min=0, max=1)
                        if clip_limit > 0:
                            excess = hist - clip_limit
                            excess[excess < 0] = 0
                            hist = hist + excess.sum() / 256
                        cdf = hist.cumsum(0)
                        cdf = (cdf - cdf.min()) / (cdf.max() - cdf.min())
>                       tile_flat = tile.view(-1)
E                       RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/temp.py:56: RuntimeError
_ TestEqualization.test_exception[1-1-grid1-TypeError-Input clip_limit type is not float. Got] _

self = <test_equalization.TestEqualization object at 0x73ebcc33ab30>, B = 1
clip = 1, grid = (2, 2), exception_type = <class 'TypeError'>
expected_error_msg = 'Input clip_limit type is not float. Got'

    @pytest.mark.parametrize(
        "B, clip, grid, exception_type, expected_error_msg",
        [
            (0, 1.0, (2, 2), ValueError, "Invalid input tensor, it is empty."),  # from perform_keep_shape_image
            (1, 1, (2, 2), TypeError, "Input clip_limit type is not float. Got"),
            (1, 2.0, 2, TypeError, "Input grid_size type is not Tuple. Got"),
            (1, 2.0, (2, 2, 2), TypeError, "Input grid_size is not a Tuple with 2 elements. Got 3"),
            (1, 2.0, (2, 2.0), TypeError, "Input grid_size type is not valid, must be a Tuple[int, int]"),
            (1, 2.0, (2, 0), ValueError, "Input grid_size elements must be positive. Got"),
        ],
    )
    def test_exception(self, B, clip, grid, exception_type, expected_error_msg):
        C, H, W = 1, 10, 20
        img = torch.rand(B, C, H, W)
        with pytest.raises(exception_type) as errinfo:
            enhance.equalize_clahe(img, clip, grid)
>       assert expected_error_msg in str(errinfo)
E       assert 'Input clip_limit type is not float. Got' in "<ExceptionInfo TypeError('clip_limit must be a float.') tblen=4>"
E        +  where "<ExceptionInfo TypeError('clip_limit must be a float.') tblen=4>" = str(<ExceptionInfo TypeError('clip_limit must be a float.') tblen=4>)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:61: AssertionError
_ TestEqualization.test_exception[1-2.0-2-TypeError-Input grid_size type is not Tuple. Got] _

self = <test_equalization.TestEqualization object at 0x73ebcc339480>, B = 1
clip = 2.0, grid = 2, exception_type = <class 'TypeError'>
expected_error_msg = 'Input grid_size type is not Tuple. Got'

    @pytest.mark.parametrize(
        "B, clip, grid, exception_type, expected_error_msg",
        [
            (0, 1.0, (2, 2), ValueError, "Invalid input tensor, it is empty."),  # from perform_keep_shape_image
            (1, 1, (2, 2), TypeError, "Input clip_limit type is not float. Got"),
            (1, 2.0, 2, TypeError, "Input grid_size type is not Tuple. Got"),
            (1, 2.0, (2, 2, 2), TypeError, "Input grid_size is not a Tuple with 2 elements. Got 3"),
            (1, 2.0, (2, 2.0), TypeError, "Input grid_size type is not valid, must be a Tuple[int, int]"),
            (1, 2.0, (2, 0), ValueError, "Input grid_size elements must be positive. Got"),
        ],
    )
    def test_exception(self, B, clip, grid, exception_type, expected_error_msg):
        C, H, W = 1, 10, 20
        img = torch.rand(B, C, H, W)
        with pytest.raises(exception_type) as errinfo:
            enhance.equalize_clahe(img, clip, grid)
>       assert expected_error_msg in str(errinfo)
E       assert 'Input grid_size type is not Tuple. Got' in "<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>"
E        +  where "<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>" = str(<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:61: AssertionError
_ TestEqualization.test_exception[1-2.0-grid3-TypeError-Input grid_size is not a Tuple with 2 elements. Got 3] _

self = <test_equalization.TestEqualization object at 0x73ebcc33ad40>, B = 1
clip = 2.0, grid = (2, 2, 2), exception_type = <class 'TypeError'>
expected_error_msg = 'Input grid_size is not a Tuple with 2 elements. Got 3'

    @pytest.mark.parametrize(
        "B, clip, grid, exception_type, expected_error_msg",
        [
            (0, 1.0, (2, 2), ValueError, "Invalid input tensor, it is empty."),  # from perform_keep_shape_image
            (1, 1, (2, 2), TypeError, "Input clip_limit type is not float. Got"),
            (1, 2.0, 2, TypeError, "Input grid_size type is not Tuple. Got"),
            (1, 2.0, (2, 2, 2), TypeError, "Input grid_size is not a Tuple with 2 elements. Got 3"),
            (1, 2.0, (2, 2.0), TypeError, "Input grid_size type is not valid, must be a Tuple[int, int]"),
            (1, 2.0, (2, 0), ValueError, "Input grid_size elements must be positive. Got"),
        ],
    )
    def test_exception(self, B, clip, grid, exception_type, expected_error_msg):
        C, H, W = 1, 10, 20
        img = torch.rand(B, C, H, W)
        with pytest.raises(exception_type) as errinfo:
            enhance.equalize_clahe(img, clip, grid)
>       assert expected_error_msg in str(errinfo)
E       assert 'Input grid_size is not a Tuple with 2 elements. Got 3' in "<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>"
E        +  where "<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>" = str(<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:61: AssertionError
_ TestEqualization.test_exception[1-2.0-grid4-TypeError-Input grid_size type is not valid, must be a Tuple[int, int]] _

self = <test_equalization.TestEqualization object at 0x73ebcc33ae00>, B = 1
clip = 2.0, grid = (2, 2.0), exception_type = <class 'TypeError'>
expected_error_msg = 'Input grid_size type is not valid, must be a Tuple[int, int]'

    @pytest.mark.parametrize(
        "B, clip, grid, exception_type, expected_error_msg",
        [
            (0, 1.0, (2, 2), ValueError, "Invalid input tensor, it is empty."),  # from perform_keep_shape_image
            (1, 1, (2, 2), TypeError, "Input clip_limit type is not float. Got"),
            (1, 2.0, 2, TypeError, "Input grid_size type is not Tuple. Got"),
            (1, 2.0, (2, 2, 2), TypeError, "Input grid_size is not a Tuple with 2 elements. Got 3"),
            (1, 2.0, (2, 2.0), TypeError, "Input grid_size type is not valid, must be a Tuple[int, int]"),
            (1, 2.0, (2, 0), ValueError, "Input grid_size elements must be positive. Got"),
        ],
    )
    def test_exception(self, B, clip, grid, exception_type, expected_error_msg):
        C, H, W = 1, 10, 20
        img = torch.rand(B, C, H, W)
        with pytest.raises(exception_type) as errinfo:
            enhance.equalize_clahe(img, clip, grid)
>       assert expected_error_msg in str(errinfo)
E       assert 'Input grid_size type is not valid, must be a Tuple[int, int]' in "<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>"
E        +  where "<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>" = str(<ExceptionInfo TypeError('grid_size must be a tuple of two integers.') tblen=4>)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:61: AssertionError
_ TestEqualization.test_exception[1-2.0-grid5-ValueError-Input grid_size elements must be positive. Got] _

self = <test_equalization.TestEqualization object at 0x73ebcc33aec0>, B = 1
clip = 2.0, grid = (2, 0), exception_type = <class 'ValueError'>
expected_error_msg = 'Input grid_size elements must be positive. Got'

    @pytest.mark.parametrize(
        "B, clip, grid, exception_type, expected_error_msg",
        [
            (0, 1.0, (2, 2), ValueError, "Invalid input tensor, it is empty."),  # from perform_keep_shape_image
            (1, 1, (2, 2), TypeError, "Input clip_limit type is not float. Got"),
            (1, 2.0, 2, TypeError, "Input grid_size type is not Tuple. Got"),
            (1, 2.0, (2, 2, 2), TypeError, "Input grid_size is not a Tuple with 2 elements. Got 3"),
            (1, 2.0, (2, 2.0), TypeError, "Input grid_size type is not valid, must be a Tuple[int, int]"),
            (1, 2.0, (2, 0), ValueError, "Input grid_size elements must be positive. Got"),
        ],
    )
    def test_exception(self, B, clip, grid, exception_type, expected_error_msg):
        C, H, W = 1, 10, 20
        img = torch.rand(B, C, H, W)
        with pytest.raises(exception_type) as errinfo:
            enhance.equalize_clahe(img, clip, grid)
>       assert expected_error_msg in str(errinfo)
E       assert 'Input grid_size elements must be positive. Got' in "<ExceptionInfo ValueError('All elements of grid_size must be positive.') tblen=4>"
E        +  where "<ExceptionInfo ValueError('All elements of grid_size must be positive.') tblen=4>" = str(<ExceptionInfo ValueError('All elements of grid_size must be positive.') tblen=4>)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:61: AssertionError
______________ TestEqualization.test_exception_tensor_dims[dims0] ______________

self = <test_equalization.TestEqualization object at 0x73ebcc33b0d0>
dims = (1, 1, 1, 1, 1)

    @pytest.mark.parametrize("dims", [(1, 1, 1, 1, 1), (1, 1)])
    def test_exception_tensor_dims(self, dims):
        img = torch.rand(dims)
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:66: Failed
______________ TestEqualization.test_exception_tensor_dims[dims1] ______________

self = <test_equalization.TestEqualization object at 0x73ebcc33b190>
dims = (1, 1)

    @pytest.mark.parametrize("dims", [(1, 1, 1, 1, 1), (1, 1)])
    def test_exception_tensor_dims(self, dims):
        img = torch.rand(dims)
>       with pytest.raises(ValueError):
E       Failed: DID NOT RAISE <class 'ValueError'>

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:66: Failed
_____________________ TestEqualization.test_gradcheck[cpu] _____________________

self = <test_equalization.TestEqualization object at 0x73ebcc33b6a0>
device = device(type='cpu')

    def test_gradcheck(self, device):
        torch.random.manual_seed(4)
        bs, channels, height, width = 1, 1, 11, 11
        inputs = torch.rand(bs, channels, height, width, device=device, dtype=torch.float64)
    
        def grad_rot(data, a, b, c):
            rot = rotate(data, torch.tensor(30.0, dtype=data.dtype, device=device))
            return enhance.equalize_clahe(rot, a, b, c)
    
>       self.gradcheck(grad_rot, (inputs, 40.0, (2, 2), True), nondet_tol=1e-4)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/testing/base.py:143: in gradcheck
    return gradcheck(func, inputs, raise_exception=raise_exception, fast_mode=fast_mode, **kwargs)
/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:2052: in gradcheck
    return _gradcheck_helper(**args)
/local/data0/moved_data/publishablew/kornia/kornia/venv/lib/python3.10/site-packages/torch/autograd/gradcheck.py:2074: in _gradcheck_helper
    func_out = func(*tupled_inputs)
/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:80: in grad_rot
    return enhance.equalize_clahe(rot, a, b, c)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/utils/image.py:272: in _wrapper
    output = f(input, *args, **kwargs)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/equalization.py:205: in equalize_clahe
    return equalize_clahe(input, clip_limit, grid_size, slow_and_differentiable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[0.0000, 0.0000, 0.1003, 0.2967, 0.6151, 0.3659, 0.2140, 0.2394,
           0.4072, 0.0181, 0.0000],
       ...7, 0.7032, 0.4179,
           0.1644, 0.0000, 0.0000]]]], dtype=torch.float64,
       grad_fn=<GridSampler2DBackward0>)
clip_limit = 40.0, grid_size = (2, 2), slow_and_differentiable = True

    def equalize_clahe(input: torch.Tensor, clip_limit: float, grid_size: Tuple[int, int], slow_and_differentiable: bool=False) -> torch.Tensor:
        """
        Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to an input image tensor.
    
        Args:
            input (Tensor): Input image tensor with shape (*, C, H, W) and values in [0, 1].
            clip_limit (float): Threshold for contrast limiting. If 0, clipping is disabled.
            grid_size (Tuple[int, int]): Number of tiles to divide the image into (rows, cols).
            slow_and_differentiable (bool): Use a slow but differentiable implementation if True.
    
        Returns:
            Tensor: Image tensor with the same shape as input after applying CLAHE.
        """
        if not isinstance(clip_limit, float):
            raise TypeError('clip_limit must be a float.')
        if not (isinstance(grid_size, tuple) and len(grid_size) == 2 and all((isinstance(x, int) for x in grid_size))):
            raise TypeError('grid_size must be a tuple of two integers.')
        if any((x <= 0 for x in grid_size)):
            raise ValueError('All elements of grid_size must be positive.')
        if input.min() < 0 or input.max() > 1:
            raise ValueError('Input tensor values must be in the range [0, 1].')
        original_shape = input.shape
        if input.dim() == 3:
            input = input.unsqueeze(0)
        batch_size, channels, height, width = input.shape
        grid_rows, grid_cols = grid_size
        tile_height = height // grid_rows
        tile_width = width // grid_cols
        output = torch.zeros_like(input)
        for b in range(batch_size):
            for c in range(channels):
                for i in range(grid_rows):
                    for j in range(grid_cols):
                        h_start = i * tile_height
                        w_start = j * tile_width
                        h_end = h_start + tile_height
                        w_end = w_start + tile_width
                        tile = input[b, c, h_start:h_end, w_start:w_end]
                        hist = torch.histc(tile, bins=256, min=0, max=1)
                        if clip_limit > 0:
                            excess = hist - clip_limit
                            excess[excess < 0] = 0
                            hist = hist + excess.sum() / 256
                        cdf = hist.cumsum(0)
                        cdf = (cdf - cdf.min()) / (cdf.max() - cdf.min())
>                       tile_flat = tile.view(-1)
E                       RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/temp.py:56: RuntimeError
____________________ TestEqualization.test_he[cpu-float32] _____________________

self = <test_equalization.TestEqualization object at 0x73ebcc33bee0>
img = tensor([[[[0.0000, 0.0526, 0.1053, 0.1579, 0.2105, 0.2632, 0.3158, 0.3684,
           0.4211, 0.4737, 0.5263, 0.5789, ...         0.4211, 0.4737, 0.5263, 0.5789, 0.6316, 0.6842, 0.7368, 0.7895,
           0.8421, 0.8947, 0.9474, 1.0000]]]])

    def test_he(self, img):
        # should be similar to enhance.equalize but slower. Similar because the lut is computed in a different way.
        clip_limit: float = 0.0
        grid_size: Tuple = (1, 1)
>       res = enhance.equalize_clahe(img, clip_limit=clip_limit, grid_size=grid_size)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:107: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/utils/image.py:272: in _wrapper
    output = f(input, *args, **kwargs)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/equalization.py:205: in equalize_clahe
    return equalize_clahe(input, clip_limit, grid_size, slow_and_differentiable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[0.0000, 0.0526, 0.1053, 0.1579, 0.2105, 0.2632, 0.3158, 0.3684,
           0.4211, 0.4737, 0.5263, 0.5789, ...         0.4211, 0.4737, 0.5263, 0.5789, 0.6316, 0.6842, 0.7368, 0.7895,
           0.8421, 0.8947, 0.9474, 1.0000]]]])
clip_limit = 0.0, grid_size = (1, 1), slow_and_differentiable = False

    def equalize_clahe(input: torch.Tensor, clip_limit: float, grid_size: Tuple[int, int], slow_and_differentiable: bool=False) -> torch.Tensor:
        """
        Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to an input image tensor.
    
        Args:
            input (Tensor): Input image tensor with shape (*, C, H, W) and values in [0, 1].
            clip_limit (float): Threshold for contrast limiting. If 0, clipping is disabled.
            grid_size (Tuple[int, int]): Number of tiles to divide the image into (rows, cols).
            slow_and_differentiable (bool): Use a slow but differentiable implementation if True.
    
        Returns:
            Tensor: Image tensor with the same shape as input after applying CLAHE.
        """
        if not isinstance(clip_limit, float):
            raise TypeError('clip_limit must be a float.')
        if not (isinstance(grid_size, tuple) and len(grid_size) == 2 and all((isinstance(x, int) for x in grid_size))):
            raise TypeError('grid_size must be a tuple of two integers.')
        if any((x <= 0 for x in grid_size)):
            raise ValueError('All elements of grid_size must be positive.')
        if input.min() < 0 or input.max() > 1:
            raise ValueError('Input tensor values must be in the range [0, 1].')
        original_shape = input.shape
        if input.dim() == 3:
            input = input.unsqueeze(0)
        batch_size, channels, height, width = input.shape
        grid_rows, grid_cols = grid_size
        tile_height = height // grid_rows
        tile_width = width // grid_cols
        output = torch.zeros_like(input)
        for b in range(batch_size):
            for c in range(channels):
                for i in range(grid_rows):
                    for j in range(grid_cols):
                        h_start = i * tile_height
                        w_start = j * tile_width
                        h_end = h_start + tile_height
                        w_end = w_start + tile_width
                        tile = input[b, c, h_start:h_end, w_start:w_end]
                        hist = torch.histc(tile, bins=256, min=0, max=1)
                        if clip_limit > 0:
                            excess = hist - clip_limit
                            excess[excess < 0] = 0
                            hist = hist + excess.sum() / 256
                        cdf = hist.cumsum(0)
                        cdf = (cdf - cdf.min()) / (cdf.max() - cdf.min())
>                       tile_flat = tile.view(-1)
E                       RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/temp.py:56: RuntimeError
____________________ TestEqualization.test_ahe[cpu-float32] ____________________

self = <test_equalization.TestEqualization object at 0x73ebcc358280>
img = tensor([[[[0.0000, 0.0526, 0.1053, 0.1579, 0.2105, 0.2632, 0.3158, 0.3684,
           0.4211, 0.4737, 0.5263, 0.5789, ...         0.4211, 0.4737, 0.5263, 0.5789, 0.6316, 0.6842, 0.7368, 0.7895,
           0.8421, 0.8947, 0.9474, 1.0000]]]])

    def test_ahe(self, img):
        clip_limit: float = 0.0
        grid_size: Tuple = (8, 8)
>       res = enhance.equalize_clahe(img, clip_limit=clip_limit, grid_size=grid_size)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:148: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/utils/image.py:272: in _wrapper
    output = f(input, *args, **kwargs)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/equalization.py:205: in equalize_clahe
    return equalize_clahe(input, clip_limit, grid_size, slow_and_differentiable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[0.0000, 0.0526, 0.1053, 0.1579, 0.2105, 0.2632, 0.3158, 0.3684,
           0.4211, 0.4737, 0.5263, 0.5789, ...         0.4211, 0.4737, 0.5263, 0.5789, 0.6316, 0.6842, 0.7368, 0.7895,
           0.8421, 0.8947, 0.9474, 1.0000]]]])
clip_limit = 0.0, grid_size = (8, 8), slow_and_differentiable = False

    def equalize_clahe(input: torch.Tensor, clip_limit: float, grid_size: Tuple[int, int], slow_and_differentiable: bool=False) -> torch.Tensor:
        """
        Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to an input image tensor.
    
        Args:
            input (Tensor): Input image tensor with shape (*, C, H, W) and values in [0, 1].
            clip_limit (float): Threshold for contrast limiting. If 0, clipping is disabled.
            grid_size (Tuple[int, int]): Number of tiles to divide the image into (rows, cols).
            slow_and_differentiable (bool): Use a slow but differentiable implementation if True.
    
        Returns:
            Tensor: Image tensor with the same shape as input after applying CLAHE.
        """
        if not isinstance(clip_limit, float):
            raise TypeError('clip_limit must be a float.')
        if not (isinstance(grid_size, tuple) and len(grid_size) == 2 and all((isinstance(x, int) for x in grid_size))):
            raise TypeError('grid_size must be a tuple of two integers.')
        if any((x <= 0 for x in grid_size)):
            raise ValueError('All elements of grid_size must be positive.')
        if input.min() < 0 or input.max() > 1:
            raise ValueError('Input tensor values must be in the range [0, 1].')
        original_shape = input.shape
        if input.dim() == 3:
            input = input.unsqueeze(0)
        batch_size, channels, height, width = input.shape
        grid_rows, grid_cols = grid_size
        tile_height = height // grid_rows
        tile_width = width // grid_cols
        output = torch.zeros_like(input)
        for b in range(batch_size):
            for c in range(channels):
                for i in range(grid_rows):
                    for j in range(grid_cols):
                        h_start = i * tile_height
                        w_start = j * tile_width
                        h_end = h_start + tile_height
                        w_end = w_start + tile_width
                        tile = input[b, c, h_start:h_end, w_start:w_end]
                        hist = torch.histc(tile, bins=256, min=0, max=1)
                        if clip_limit > 0:
                            excess = hist - clip_limit
                            excess[excess < 0] = 0
                            hist = hist + excess.sum() / 256
                        cdf = hist.cumsum(0)
                        cdf = (cdf - cdf.min()) / (cdf.max() - cdf.min())
>                       tile_flat = tile.view(-1)
E                       RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/temp.py:56: RuntimeError
___________________ TestEqualization.test_clahe[cpu-float32] ___________________

self = <test_equalization.TestEqualization object at 0x73ebcc3585e0>
img = tensor([[[[0.0000, 0.0526, 0.1053, 0.1579, 0.2105, 0.2632, 0.3158, 0.3684,
           0.4211, 0.4737, 0.5263, 0.5789, ...         0.4211, 0.4737, 0.5263, 0.5789, 0.6316, 0.6842, 0.7368, 0.7895,
           0.8421, 0.8947, 0.9474, 1.0000]]]])

    def test_clahe(self, img):
        clip_limit: float = 2.0
        grid_size: Tuple = (8, 8)
>       res = enhance.equalize_clahe(img, clip_limit=clip_limit, grid_size=grid_size)

/local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py:189: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/local/data0/moved_data/publishablew/kornia/kornia/kornia/utils/image.py:272: in _wrapper
    output = f(input, *args, **kwargs)
/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/equalization.py:205: in equalize_clahe
    return equalize_clahe(input, clip_limit, grid_size, slow_and_differentiable)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[[[0.0000, 0.0526, 0.1053, 0.1579, 0.2105, 0.2632, 0.3158, 0.3684,
           0.4211, 0.4737, 0.5263, 0.5789, ...         0.4211, 0.4737, 0.5263, 0.5789, 0.6316, 0.6842, 0.7368, 0.7895,
           0.8421, 0.8947, 0.9474, 1.0000]]]])
clip_limit = 2.0, grid_size = (8, 8), slow_and_differentiable = False

    def equalize_clahe(input: torch.Tensor, clip_limit: float, grid_size: Tuple[int, int], slow_and_differentiable: bool=False) -> torch.Tensor:
        """
        Apply Contrast Limited Adaptive Histogram Equalization (CLAHE) to an input image tensor.
    
        Args:
            input (Tensor): Input image tensor with shape (*, C, H, W) and values in [0, 1].
            clip_limit (float): Threshold for contrast limiting. If 0, clipping is disabled.
            grid_size (Tuple[int, int]): Number of tiles to divide the image into (rows, cols).
            slow_and_differentiable (bool): Use a slow but differentiable implementation if True.
    
        Returns:
            Tensor: Image tensor with the same shape as input after applying CLAHE.
        """
        if not isinstance(clip_limit, float):
            raise TypeError('clip_limit must be a float.')
        if not (isinstance(grid_size, tuple) and len(grid_size) == 2 and all((isinstance(x, int) for x in grid_size))):
            raise TypeError('grid_size must be a tuple of two integers.')
        if any((x <= 0 for x in grid_size)):
            raise ValueError('All elements of grid_size must be positive.')
        if input.min() < 0 or input.max() > 1:
            raise ValueError('Input tensor values must be in the range [0, 1].')
        original_shape = input.shape
        if input.dim() == 3:
            input = input.unsqueeze(0)
        batch_size, channels, height, width = input.shape
        grid_rows, grid_cols = grid_size
        tile_height = height // grid_rows
        tile_width = width // grid_cols
        output = torch.zeros_like(input)
        for b in range(batch_size):
            for c in range(channels):
                for i in range(grid_rows):
                    for j in range(grid_cols):
                        h_start = i * tile_height
                        w_start = j * tile_width
                        h_end = h_start + tile_height
                        w_end = w_start + tile_width
                        tile = input[b, c, h_start:h_end, w_start:w_end]
                        hist = torch.histc(tile, bins=256, min=0, max=1)
                        if clip_limit > 0:
                            excess = hist - clip_limit
                            excess[excess < 0] = 0
                            hist = hist + excess.sum() / 256
                        cdf = hist.cumsum(0)
                        cdf = (cdf - cdf.min()) / (cdf.max() - cdf.min())
>                       tile_flat = tile.view(-1)
E                       RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.

/local/data0/moved_data/publishablew/kornia/kornia/kornia/enhance/temp.py:56: RuntimeError
=========================== short test summary info ============================
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-None-grid1]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-2.0-grid2]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-1-grid1-TypeError-Input clip_limit type is not float. Got]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-2-TypeError-Input grid_size type is not Tuple. Got]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid3-TypeError-Input grid_size is not a Tuple with 2 elements. Got 3]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid4-TypeError-Input grid_size type is not valid, must be a Tuple[int, int]]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid5-ValueError-Input grid_size elements must be positive. Got]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_dims[dims0]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_dims[dims1]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_gradcheck[cpu]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_he[cpu-float32]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_ahe[cpu-float32]
FAILED ../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_clahe[cpu-float32]
=================== 13 failed, 11 passed, 1 skipped in 0.54s ===================


Final Test Result:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'cudagraphs', 'onnxrt', 'jit', 'tvm', 'openxla', 'inductor', None}
model weights cached: []

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1, jaxtyping-0.2.38
collecting ... collected 25 items

../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_smoke[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-None-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-None-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-1-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-1-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-4-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-0.0-None] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-None-grid1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-2.0-grid2] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[0-1.0-grid0-ValueError-Invalid input tensor, it is empty.] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-1-grid1-TypeError-Input clip_limit type is not float. Got] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-2-TypeError-Input grid_size type is not Tuple. Got] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid3-TypeError-Input grid_size is not a Tuple with 2 elements. Got 3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid4-TypeError-Input grid_size type is not valid, must be a Tuple[int, int]] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid5-ValueError-Input grid_size elements must be positive. Got] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_dims[dims0] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_dims[dims1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_type PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_gradcheck[cpu] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_jit[cpu-float32] SKIPPED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_module PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_he[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_ahe[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_clahe[cpu-float32] PASSED

======================== 24 passed, 1 skipped in 0.22s =========================


Initial Result:
Setting up torch compile...
============================= test session starts ==============================
platform linux -- Python 3.10.12, pytest-8.3.3, pluggy-1.5.0 -- /local/data0/moved_data/publishablew/kornia/kornia/venv/bin/python
cachedir: .pytest_cache

cpu info:
	- Model name: AMD Ryzen 7 PRO 5845 8-Core Processor
	- Architecture: x86_64
	- CPU(s): 16
	- Thread(s) per core: 2
	- CPU max MHz: 4661.7178
	- CPU min MHz: 2200.0000
gpu info: {'GPU 0': 'NVIDIA GeForce RTX 3060'}
main deps:
    - kornia-0.7.4
    - torch-2.5.1+cu124
        - commit: a8d6afb511a69687bbb2b7e88a3cf67917e1697e
        - cuda: 12.4
        - nvidia-driver: 555.42.02
x deps:
    - accelerate-1.1.1
dev deps:
    - kornia_rs-0.1.7
    - onnx-1.17.0
gcc info: (Ubuntu 10.5.0-1ubuntu1~22.04) 10.5.0
available optimizers: {'', 'tvm', 'inductor', 'jit', 'onnxrt', 'cudagraphs', 'openxla', None}
model weights cached: []

rootdir: /local/data0/moved_data/publishablew/kornia/kornia
configfile: pyproject.toml
plugins: timeout-2.3.1, jaxtyping-0.2.38
collecting ... collected 25 items

../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_smoke[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-None-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-None-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-1-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-1-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-4-1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_cardinality[cpu-float32-4-3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-0.0-None] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-None-grid1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_optional_params[cpu-float32-2.0-grid2] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[0-1.0-grid0-ValueError-Invalid input tensor, it is empty.] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-1-grid1-TypeError-Input clip_limit type is not float. Got] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-2-TypeError-Input grid_size type is not Tuple. Got] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid3-TypeError-Input grid_size is not a Tuple with 2 elements. Got 3] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid4-TypeError-Input grid_size type is not valid, must be a Tuple[int, int]] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception[1-2.0-grid5-ValueError-Input grid_size elements must be positive. Got] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_dims[dims0] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_dims[dims1] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_exception_tensor_type PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_gradcheck[cpu] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_jit[cpu-float32] SKIPPED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_module PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_he[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_ahe[cpu-float32] PASSED
../../../../../../local/data0/moved_data/publishablew/kornia/kornia/tests/enhance/test_equalization.py::TestEqualization::test_clahe[cpu-float32] PASSED

======================== 24 passed, 1 skipped in 0.21s =========================
