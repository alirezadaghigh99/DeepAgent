{
    "result": "Here's a Python function `binary_focal_loss_with_logits` that computes the Binary Focal Loss as described:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\ndef binary_focal_loss_with_logits(pred, target, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None, weight=None):\n    \"\"\"\n    Compute the Binary Focal Loss between `pred` and `target`.\n\n    Args:\n        pred (Tensor): Logits tensor with shape (N, C, *) where C = number of classes.\n        target (Tensor): Labels tensor with the same shape as pred (N, C, *) where each value is between 0 and 1.\n        alpha (float): Weighting factor alpha in [0, 1].\n        gamma (float): Focusing parameter gamma >= 0.\n        reduction (str): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n        pos_weight (Tensor, optional): A weight of positive examples with shape (num_of_classes,).\n        weight (Tensor, optional): Weights for classes with shape (num_of_classes,).\n\n    Returns:\n        Tensor: The computed loss.\n    \"\"\"\n    # Compute the binary cross entropy with logits\n    bce_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none', pos_weight=pos_weight, weight=weight)\n\n    # Convert logits to probabilities\n    pred_prob = torch.sigmoid(pred)\n\n    # Compute the focal loss factor\n    p_t = target * pred_prob + (1 - target) * (1 - pred_prob)\n    focal_factor = (1 - p_t) ** gamma\n\n    # Apply the focal loss factor and alpha\n    loss = alpha * focal_factor * bce_loss\n\n    # Apply reduction\n    if reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        return loss\n\n# Example usage\nif __name__ == \"__main__\":\n    C = 3  # num_classes\n    pred = torch.randn(1, C, 5, requires_grad=True)\n    target = torch.randint(2, (1, C, 5)).float()\n    kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n    output = binary_focal_loss_with_logits(pred, target, **kwargs)\n    output.backward()\n```\n\n### Explanation:\n- **Logits to Probabilities**: The function first converts the logits to probabilities using the sigmoid function.\n- **Binary Cross Entropy**: It computes the binary cross-entropy loss using `F.binary_cross_entropy_with_logits`.\n- **Focal Loss Factor**: The focal loss factor is computed as \\((1 - p_t)^\\gamma\\), where \\(p_t\\) is the probability of the true class.\n- **Alpha Weighting**: The loss is scaled by the alpha factor.\n- **Reduction**: The function supports three types of reduction: 'none', 'mean', and 'sum'.",
    "prompt": "Generate a Python function called binary_focal_loss_with_logits for Criterion that computes Binary Focal loss.\n\nFocal loss is computed as follows:\n\n    .. math::\n\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n\n    where:\n       - :math:`p_t` is the model's estimated probability for each class.\n\n    Args:\n        pred: logits tensor with shape :math:`(N, C, *)` where C = number of classes.\n        target: labels tensor with the same shape as pred :math:`(N, C, *)`\n          where each value is between 0 and 1.\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.\n        reduction: Specifies the reduction to apply to the\n          output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n          will be applied, ``'mean'``: the sum of the output will be divided by\n          the number of elements in the output, ``'sum'``: the output will be\n          summed.\n        pos_weight: a weight of positive examples with shape :math:`(num\\_of\\_classes,)`.\n          It is possible to trade off recall and precision by adding weights to positive examples.\n        weight: weights for classes with shape :math:`(num\\_of\\_classes,)`.\n\n    Returns:\n        the computed loss.\n\n    Examples:\n        >>> C = 3  # num_classes\n        >>> pred = torch.randn(1, C, 5, requires_grad=True)\n        >>> target = torch.randint(2, (1, C, 5))\n        >>> kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n        >>> output = binary_focal_loss_with_logits(pred, target, **kwargs)\n        >>> output.backward()",
    "function_name": "binary_focal_loss_with_logits",
    "ground_truth": "kornia/losses/focal.py",
    "test": "tests/losses/test_focal_loss.py::TestBinaryFocalLossWithLogits",
    "class": "",
    "stage": "Model Construction",
    "task": "Classification",
    "data": "Image"
}