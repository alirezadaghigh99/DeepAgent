{
    "result": "To implement the `_clamp_bounding_boxes` function, we need to handle the conversion between different bounding box formats and ensure that the bounding boxes are clamped within the specified canvas size. Here's how you can implement this function:\n\n```python\nimport torch\nfrom enum import Enum\n\nclass BoundingBoxFormat(Enum):\n    XYXY = \"xyxy\"\n    XYWH = \"xywh\"\n    CXCYWH = \"cxcywh\"\n\ndef _convert_to_xyxy(bounding_boxes, format):\n    if format == BoundingBoxFormat.XYXY:\n        return bounding_boxes\n    elif format == BoundingBoxFormat.XYWH:\n        x, y, w, h = bounding_boxes.unbind(-1)\n        return torch.stack((x, y, x + w, y + h), dim=-1)\n    elif format == BoundingBoxFormat.CXCYWH:\n        cx, cy, w, h = bounding_boxes.unbind(-1)\n        return torch.stack((cx - w / 2, cy - h / 2, cx + w / 2, cy + h / 2), dim=-1)\n    else:\n        raise ValueError(f\"Unsupported format: {format}\")\n\ndef _convert_from_xyxy(bounding_boxes, format):\n    if format == BoundingBoxFormat.XYXY:\n        return bounding_boxes\n    elif format == BoundingBoxFormat.XYWH:\n        x1, y1, x2, y2 = bounding_boxes.unbind(-1)\n        return torch.stack((x1, y1, x2 - x1, y2 - y1), dim=-1)\n    elif format == BoundingBoxFormat.CXCYWH:\n        x1, y1, x2, y2 = bounding_boxes.unbind(-1)\n        return torch.stack(((x1 + x2) / 2, (y1 + y2) / 2, x2 - x1, y2 - y1), dim=-1)\n    else:\n        raise ValueError(f\"Unsupported format: {format}\")\n\ndef _clamp_bounding_boxes(bounding_boxes, format, canvas_size):\n    # Convert to XYXY format for clamping\n    boxes_xyxy = _convert_to_xyxy(bounding_boxes, format)\n    \n    # Clamp the coordinates\n    height, width = canvas_size\n    x1, y1, x2, y2 = boxes_xyxy.unbind(-1)\n    x1 = x1.clamp(min=0, max=width)\n    y1 = y1.clamp(min=0, max=height)\n    x2 = x2.clamp(min=0, max=width)\n    y2 = y2.clamp(min=0, max=height)\n    \n    clamped_boxes_xyxy = torch.stack((x1, y1, x2, y2), dim=-1)\n    \n    # Convert back to the original format\n    clamped_boxes = _convert_from_xyxy(clamped_boxes_xyxy, format)\n    \n    return clamped_boxes\n\n# Example usage:\n# bounding_boxes = torch.tensor([[10, 10, 50, 50], [60, 60, 120, 120]], dtype=torch.float32)\n# format = BoundingBoxFormat.XYXY\n# canvas_size = (100, 100)\n# clamped_boxes = _clamp_bounding_boxes(bounding_boxes, format, canvas_size)\n# print(clamped_boxes)\n```\n\n### Explanation:\n1. **Conversion Functions**: We define two helper functions, `_convert_to_xyxy` and `_convert_from_xyxy`, to handle the conversion between different bounding box formats and the `XYXY` format.\n\n2. **Clamping**: The `_clamp_bounding_boxes` function first converts the bounding boxes to the `XYXY` format, clamps the coordinates to ensure they fit within the canvas size, and then converts them back to the original format.\n\n3. **Enum for Formats**: We use an `Enum` to define the possible formats for bounding boxes, which makes the code more readable and less error-prone.\n\nThis implementation ensures that the bounding boxes are adjusted to fit within the specified canvas size while maintaining their original format and data type.",
    "prompt": "Implement a function named `_clamp_bounding_boxes` that adjusts bounding boxes to fit within a given canvas size. The function takes three inputs: `bounding_boxes`, a PyTorch tensor representing the bounding boxes; `format`, an enum of type `BoundingBoxFormat`, which can be one of the following: `XYXY`, `XYWH`, or `CXCYWH`; and `canvas_size`, a tuple representing the height and width of the canvas. The function ensures that the bounding boxes do not exceed the canvas boundaries by clamping their coordinates. It first converts the bounding boxes to the `XYXY` format for clamping, then converts them back to their original format before returning them as a PyTorch tensor in the same data type as the input.",
    "function_name": "_clamp_bounding_boxes",
    "ground_truth": "torchvision/transforms/v2/functional/_meta.py",
    "test": "test/test_transforms_v2.py::TestResize::test_bounding_boxes_correctness",
    "class": "",
    "stage": "Inference",
    "task": "",
    "data": "Image"
}