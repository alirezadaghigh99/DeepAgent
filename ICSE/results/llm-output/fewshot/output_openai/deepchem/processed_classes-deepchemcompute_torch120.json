{
    "result": "To create a Python function that calculates the categorical cross-entropy loss using PyTorch, you can define a method within a class that inherits from `torch.nn.Module` or `torch.nn.Loss`. Below is an example of how you might implement the `_create_pytorch_loss` method within a class called `CategoricalCrossEntropy`. This method will handle both 2D and 3D input tensors for probabilities and labels.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CategoricalCrossEntropy(nn.Module):\n    def __init__(self):\n        super(CategoricalCrossEntropy, self).__init__()\n\n    def _create_pytorch_loss(self, probabilities: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculate the categorical cross-entropy loss between output probabilities and labels.\n\n        Args:\n            probabilities (torch.Tensor): Tensor of shape (batch_size, classes) or (batch_size, tasks, classes)\n                                          containing the predicted probabilities.\n            labels (torch.Tensor): Tensor of shape (batch_size,) or (batch_size, tasks) containing the true labels.\n\n        Returns:\n            torch.Tensor: Tensor containing the loss values.\n        \"\"\"\n        # Ensure probabilities are in log form for numerical stability\n        log_probs = torch.log(probabilities)\n\n        # If the input is 3D, we need to reshape it to apply the loss function\n        if probabilities.dim() == 3:\n            # Reshape to (batch_size * tasks, classes) and (batch_size * tasks)\n            batch_size, tasks, classes = probabilities.shape\n            log_probs = log_probs.view(batch_size * tasks, classes)\n            labels = labels.view(batch_size * tasks)\n\n        # Use PyTorch's built-in cross-entropy loss function\n        loss = F.nll_loss(log_probs, labels, reduction='none')\n\n        # If the input was 3D, reshape the loss back to (batch_size, tasks)\n        if probabilities.dim() == 3:\n            loss = loss.view(batch_size, tasks)\n\n        return loss\n```\n\n### Explanation:\n- **Log Probabilities**: The method first converts the probabilities to log probabilities using `torch.log` for numerical stability.\n- **Handling 3D Inputs**: If the input tensors are 3D, they are reshaped to 2D to apply the loss function, and then reshaped back to the original dimensions.\n- **Loss Calculation**: The method uses `F.nll_loss`, which is the negative log likelihood loss function, suitable for categorical cross-entropy when the input is in log form.\n- **Reshape Loss**: If the input was 3D, the loss is reshaped back to match the original input dimensions.\n\nThis implementation assumes that the input probabilities are already normalized (i.e., they sum to 1 across the classes dimension). If they are not, you should apply a softmax function before passing them to this method.",
    "prompt": "Generate a Python function called `_create_pytorch_loss` that calculates the categorical cross entropy loss between output probabilities and labels using PyTorch. The inputs are tensors of probabilities and labels with shapes `(batch_size, classes)` or `(batch_size, tasks, classes)`, and the output is a tensor of loss values.\n\n```python\nclass CategoricalCrossEntropy(Loss):\n    def _create_pytorch_loss(self):\n```",
    "function_name": "_create_pytorch_loss",
    "ground_truth": "deepchem/models/losses.py",
    "test": "deepchem/models/tests/test_losses.py::TestLosses",
    "class": "CategoricalCrossEntropy",
    "stage": "",
    "task": "Classification",
    "data": "Evaluation/metrics"
}