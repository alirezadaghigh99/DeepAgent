{
    "result": "To implement the `_compute_tf_loss` function for calculating the softmax cross-entropy loss using TensorFlow, you can use the `tf.nn.softmax_cross_entropy_with_logits` function. This function is designed to compute the softmax cross-entropy loss between logits and labels. Here's how you can implement it:\n\n```python\nimport tensorflow as tf\nfrom tensorflow.keras.losses import Loss\n\nclass SoftmaxCrossEntropy(Loss):\n    def _compute_tf_loss(self, output, labels):\n        \"\"\"\n        Computes the softmax cross-entropy loss between logits and labels.\n\n        Args:\n            output (tf.Tensor): Logits tensor with shape (batch_size, classes) or (batch_size, tasks, classes).\n            labels (tf.Tensor): Labels tensor with the same shape as output.\n\n        Returns:\n            tf.Tensor: Tensor of loss values.\n        \"\"\"\n        # Ensure the labels are in the same shape as the output\n        labels = tf.cast(labels, dtype=output.dtype)\n\n        # Compute the softmax cross-entropy loss\n        if len(output.shape) == 2:\n            # Case for shape (batch_size, classes)\n            loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=output)\n        elif len(output.shape) == 3:\n            # Case for shape (batch_size, tasks, classes)\n            # Reshape to combine batch_size and tasks into a single dimension\n            output_reshaped = tf.reshape(output, [-1, output.shape[-1]])\n            labels_reshaped = tf.reshape(labels, [-1, labels.shape[-1]])\n            loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_reshaped, logits=output_reshaped)\n            # Reshape back to (batch_size, tasks)\n            loss = tf.reshape(loss, [output.shape[0], output.shape[1]])\n        else:\n            raise ValueError(\"Output tensor must have shape (batch_size, classes) or (batch_size, tasks, classes).\")\n\n        return loss\n```\n\n### Explanation:\n- **Input Shapes**: The function handles two possible input shapes: `(batch_size, classes)` and `(batch_size, tasks, classes)`.\n- **Casting Labels**: The labels are cast to the same data type as the output logits to ensure compatibility.\n- **Loss Calculation**:\n  - For 2D inputs, the function directly computes the loss using `tf.nn.softmax_cross_entropy_with_logits`.\n  - For 3D inputs, the function reshapes the tensors to combine the batch size and tasks into a single dimension, computes the loss, and then reshapes the loss back to the original dimensions.\n- **Error Handling**: If the input tensor does not match the expected shapes, a `ValueError` is raised.",
    "prompt": "Generate a Python function called `_compute_tf_loss` that calculates the softmax cross entropy loss between output logits and labels using TensorFlow. The inputs are tensors of logits and labels with shapes `(batch_size, classes)` or `(batch_size, tasks, classes)`, and the output is a tensor of loss values.\n\n```python\nclass SoftmaxCrossEntropy(Loss):\n    def _compute_tf_loss(self, output, labels):\n```",
    "function_name": "_compute_tf_loss",
    "ground_truth": "deepchem/models/losses.py",
    "test": "deepchem/models/tests/test_losses.py::TestLosses",
    "class": "SoftmaxCrossEntropy",
    "stage": "",
    "task": "Classification",
    "data": "Training"
}