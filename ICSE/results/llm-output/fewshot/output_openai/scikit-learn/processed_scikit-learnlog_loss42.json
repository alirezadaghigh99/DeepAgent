{
    "result": "Certainly! Below is a Python function called `log_loss` that calculates the log loss (also known as logistic loss or cross-entropy loss). The function includes parameters for ground truth labels, predicted probabilities, normalization, sample weights, and optional class labels. Additionally, I've included examples, notes, and references for further information.\n\n```python\nimport numpy as np\n\ndef log_loss(y_true, y_pred, normalize=True, sample_weight=None, labels=None):\n    \"\"\"\n    Calculate the log loss, also known as logistic loss or cross-entropy loss.\n\n    Args:\n        y_true (array-like): Ground truth labels for n_samples samples.\n        y_pred (array-like): Predicted probabilities from a classifier's predict_proba method.\n        normalize (bool): Whether to return the mean loss per sample or the sum of per-sample losses.\n        sample_weight (array-like, optional): Optional sample weights.\n        labels (array-like, optional): Optional labels for the classes.\n\n    Returns:\n        float: The log loss value.\n\n    Examples:\n        >>> y_true = [0, 1, 1]\n        >>> y_pred = [0.1, 0.9, 0.8]\n        >>> log_loss(y_true, y_pred)\n        0.164252033486018\n\n    Notes:\n        - Log loss is a measure of the performance of a classification model where the prediction is a probability value between 0 and 1.\n        - The log loss increases as the predicted probability diverges from the actual label.\n        - This function assumes binary classification. For multi-class classification, y_pred should be a 2D array with probabilities for each class.\n\n    References:\n        - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n        - https://en.wikipedia.org/wiki/Cross_entropy\n\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    if labels is not None:\n        y_true = np.searchsorted(labels, y_true)\n\n    if sample_weight is None:\n        sample_weight = np.ones_like(y_true, dtype=float)\n\n    # Clip y_pred to prevent log(0)\n    eps = np.finfo(float).eps\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n\n    # Calculate log loss\n    loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    loss *= sample_weight\n\n    if normalize:\n        return np.average(loss)\n    else:\n        return np.sum(loss)\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = [0, 1, 1]\n    y_pred = [0.1, 0.9, 0.8]\n    print(log_loss(y_true, y_pred))\n```\n\n### Key Points:\n- **Clipping**: The predicted probabilities are clipped to avoid taking the logarithm of zero, which would result in an undefined value.\n- **Normalization**: The function can return either the mean log loss per sample or the total log loss, depending on the `normalize` parameter.\n- **Sample Weights**: The function supports optional sample weights to account for different importance of samples.\n- **Binary Classification**: This implementation assumes binary classification. For multi-class classification, `y_pred` should be a 2D array with probabilities for each class.",
    "prompt": "Generate a Python function called log_loss that calculates the log loss, also known as logistic loss or cross-entropy loss. The function takes in the following parameters:\n\n- y_true: Ground truth labels for n_samples samples.\n- y_pred: Predicted probabilities from a classifier's predict_proba method.\n- normalize: A boolean indicating whether to return the mean loss per sample or the sum of per-sample losses.\n- sample_weight: Optional sample weights.\n- labels: Optional labels for the classes.\n\nThe function calculates the log loss based on the predicted probabilities and true labels. It returns the log loss as a float value. The function also includes examples, notes, and references for further information.",
    "function_name": "log_loss",
    "ground_truth": "sklearn/metrics/_classification.py",
    "test": "sklearn/metrics/tests/test_classification.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Classification",
    "data": "Table"
}