{
    "result": "Certainly! Below is a Python function `top_k_accuracy_score` that calculates the top-k accuracy classification score. This function includes parameters for true labels, predicted scores, the value of k, normalization, sample weights, and class labels. Additionally, I've included a docstring with examples and information about the metric.\n\n```python\nimport numpy as np\nfrom sklearn.utils import check_array, check_consistent_length\n\ndef top_k_accuracy_score(y_true, y_score, k=1, normalize=True, sample_weight=None, labels=None):\n    \"\"\"\n    Calculate the top-k accuracy classification score.\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,) representing the true labels.\n    - y_score: array-like of shape (n_samples,) or (n_samples, n_classes) representing the target scores.\n    - k: int, number of most likely outcomes considered to find the correct label.\n    - normalize: bool, whether to return the fraction of correctly classified samples or the number of correctly classified samples.\n    - sample_weight: array-like of shape (n_samples,), sample weights.\n    - labels: array-like of shape (n_classes,), list of labels that index the classes in y_score.\n\n    Returns:\n    - score: float, top-k accuracy score. Best performance is 1 with normalize == True and the number of samples with normalize == False.\n\n    Examples:\n    >>> y_true = [0, 1, 2, 2]\n    >>> y_score = [[0.2, 0.3, 0.5], [0.1, 0.8, 0.1], [0.6, 0.2, 0.2], [0.3, 0.4, 0.3]]\n    >>> top_k_accuracy_score(y_true, y_score, k=2)\n    0.75\n\n    >>> top_k_accuracy_score(y_true, y_score, k=1, normalize=False)\n    3\n    \"\"\"\n    y_true = check_array(y_true, ensure_2d=False)\n    y_score = check_array(y_score, ensure_2d=False)\n    check_consistent_length(y_true, y_score, sample_weight)\n\n    if labels is not None:\n        labels = np.asarray(labels)\n        y_score = y_score[:, labels]\n\n    if y_score.ndim == 1:\n        y_score = y_score[:, np.newaxis]\n\n    n_samples = y_score.shape[0]\n    top_k_preds = np.argsort(y_score, axis=1)[:, -k:]\n\n    correct = np.any(top_k_preds == y_true[:, np.newaxis], axis=1)\n\n    if sample_weight is not None:\n        sample_weight = check_array(sample_weight, ensure_2d=False)\n        check_consistent_length(y_true, sample_weight)\n        correct = correct * sample_weight\n\n    if normalize:\n        return np.average(correct)\n    else:\n        return np.sum(correct)\n\n```\n\n### Explanation:\n- **Parameters**:\n  - `y_true`: True labels for the samples.\n  - `y_score`: Predicted scores or probabilities for each class.\n  - `k`: Number of top predictions to consider for accuracy.\n  - `normalize`: If `True`, returns the fraction of correctly classified samples; otherwise, returns the count.\n  - `sample_weight`: Weights for each sample.\n  - `labels`: Specific labels to consider in the score array.\n\n- **Returns**:\n  - The top-k accuracy score, either as a fraction or a count, depending on the `normalize` parameter.\n\n- **Examples**:\n  - The function includes examples demonstrating how to use it with different values of `k` and `normalize`.\n\nThis function is useful for evaluating models where the top-k predictions are important, such as in multi-class classification tasks.",
    "prompt": "Generate a Python function called top_k_accuracy_score that calculates the top-k accuracy classification score. The function takes in the following parameters:\n\n- y_true: array-like of shape (n_samples,) representing the true labels.\n- y_score: array-like of shape (n_samples,) or (n_samples, n_classes) representing the target scores.\n- k: an integer representing the number of most likely outcomes considered to find the correct label.\n- normalize: a boolean indicating whether to return the fraction of correctly classified samples or the number of correctly classified samples.\n- sample_weight: array-like of shape (n_samples,) representing sample weights.\n- labels: array-like of shape (n_classes,) representing the list of labels that index the classes in y_score.\n\nThe function returns a float value representing the top-k accuracy score. The best performance is 1 with normalize == True and the number of samples with normalize == False. The function also includes examples and additional information about the metric.",
    "function_name": "top_k_accuracy_score",
    "ground_truth": "sklearn/metrics/_ranking.py",
    "test": "sklearn/metrics/tests/test_ranking.py::test_top_k_accuracy_score_binary",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Classification",
    "data": "Table"
}