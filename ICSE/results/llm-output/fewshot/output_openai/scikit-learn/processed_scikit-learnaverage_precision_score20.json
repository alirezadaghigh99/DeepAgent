{
    "result": "To create a function `average_precision_score` that computes the average precision (AP) from prediction scores, we can follow the structure and style of the examples you provided. This function will handle binary, multilabel-indicator, and multiclass data, and will include a helper function `_binary_uninterpolated_average_precision` for binary classification. Here's how you can implement it:\n\n```python\nimport numpy as np\nfrom sklearn.utils import check_array, column_or_1d\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import auc\n\ndef _binary_uninterpolated_average_precision(y_true, y_score, sample_weight=None):\n    \"\"\"\n    Calculate the average precision for binary classification without interpolation.\n    \"\"\"\n    # Sort scores and corresponding truth values\n    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n    y_score = y_score[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n    if sample_weight is not None:\n        sample_weight = sample_weight[desc_score_indices]\n\n    # Calculate precision and recall\n    distinct_value_indices = np.where(np.diff(y_score))[0]\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n    tps = np.cumsum(y_true * sample_weight) if sample_weight is not None else np.cumsum(y_true)\n    fps = np.cumsum((1 - y_true) * sample_weight) if sample_weight is not None else np.cumsum(1 - y_true)\n\n    tps = tps[threshold_idxs]\n    fps = fps[threshold_idxs]\n\n    precisions = tps / (tps + fps)\n    recalls = tps / tps[-1]\n\n    # Calculate average precision\n    return np.sum((recalls[1:] - recalls[:-1]) * precisions[1:])\n\ndef average_precision_score(y_true, y_score, average='macro', pos_label=1, sample_weight=None):\n    \"\"\"\n    Compute the average precision (AP) from prediction scores.\n    \n    Parameters:\n        y_true: array-like of shape (n_samples,) or (n_samples, n_classes)\n            True binary labels or binary label indicators.\n        y_score: array-like of shape (n_samples,) or (n_samples, n_classes)\n            Target scores.\n        average: {'micro', 'samples', 'weighted', 'macro'} or None, default='macro'\n            Determines the type of averaging performed on the data.\n        pos_label: int, float, bool, or str, default=1\n            The label of the positive class.\n        sample_weight: array-like of shape (n_samples,), default=None\n            Sample weights.\n    \n    Returns:\n        average_precision: float\n            The average precision score.\n    \"\"\"\n    y_true = check_array(y_true, ensure_2d=False)\n    y_score = check_array(y_score, ensure_2d=False)\n    \n    if y_true.ndim == 1:\n        y_true = column_or_1d(y_true)\n        y_score = column_or_1d(y_score)\n        return _binary_uninterpolated_average_precision(y_true, y_score, sample_weight)\n    \n    # Multiclass or multilabel-indicator case\n    n_classes = y_score.shape[1]\n    average_precision = np.zeros(n_classes)\n    \n    for i in range(n_classes):\n        y_true_i = y_true[:, i]\n        y_score_i = y_score[:, i]\n        average_precision[i] = _binary_uninterpolated_average_precision(y_true_i, y_score_i, sample_weight)\n    \n    if average == 'macro':\n        return np.mean(average_precision)\n    elif average == 'weighted':\n        weights = np.sum(y_true, axis=0)\n        return np.average(average_precision, weights=weights)\n    elif average == 'micro':\n        y_true = y_true.ravel()\n        y_score = y_score.ravel()\n        return _binary_uninterpolated_average_precision(y_true, y_score, sample_weight)\n    elif average is None:\n        return average_precision\n    else:\n        raise ValueError(f\"Unknown average type: {average}\")\n\n# Example usage\ny_true = np.array([0, 1, 1, 0])\ny_score = np.array([0.1, 0.4, 0.35, 0.8])\nprint(average_precision_score(y_true, y_score))\n```\n\n### Explanation:\n- **_binary_uninterpolated_average_precision**: This helper function calculates the average precision for binary classification without interpolation.\n- **average_precision_score**: This function handles different input types (binary, multilabel-indicator, multiclass) and supports different averaging methods (`macro`, `micro`, `weighted`, `samples`, or `None`).\n- **Example Usage**: Demonstrates how to use the function with binary labels and scores.\n\nThis implementation uses NumPy and Scikit-learn utilities to ensure compatibility with common data structures and to handle input validation.",
    "prompt": "Generate a Python function called average_precision_score that computes the average precision (AP) from prediction scores. The function takes in the following parameters:\n\n- y_true: array-like of shape (n_samples,) or (n_samples, n_classes) representing true binary labels or binary label indicators.\n- y_score: array-like of shape (n_samples,) or (n_samples, n_classes) representing target scores.\n- average: {'micro', 'samples', 'weighted', 'macro'} or None, default='macro' determining the type of averaging performed on the data.\n- pos_label: int, float, bool, or str, default=1 representing the label of the positive class.\n- sample_weight: array-like of shape (n_samples,), default=None representing sample weights.\n\nThe function returns the average precision score as a float value. It also includes a helper function called _binary_uninterpolated_average_precision to calculate the average precision for binary classification. The function handles different types of input data such as binary, multilabel-indicator, and multiclass.\n\nAdditionally, the function provides examples of how to use it with input arrays for y_true and y_score.",
    "function_name": "average_precision_score",
    "ground_truth": "sklearn/metrics/_ranking.py",
    "test": "sklearn/metrics/tests/test_ranking.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}