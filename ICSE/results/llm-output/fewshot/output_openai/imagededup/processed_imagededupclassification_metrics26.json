{
    "result": "To generate a Python function that calculates precision, recall, and F1 score for each class given ground truth and retrieved dictionaries, you can follow the structure below. This function will compute these metrics for class 1 (duplicate file pairs) and class 0 (non-duplicate file pairs):\n\n```python\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom typing import Dict, List\n\ndef evaluate_duplicates(ground_truth: Dict[str, List[str]], retrieved: Dict[str, List[str]]) -> Dict[str, Dict[str, float]]:\n    \"\"\"\n    Given ground truth dictionary and retrieved dictionary, return per class precision, recall, and f1 score.\n    Class 1 is assigned to duplicate file pairs while class 0 is for non-duplicate file pairs.\n\n    Args:\n        ground_truth: A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value.\n        retrieved: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value.\n\n    Returns:\n        Dictionary of precision, recall, and f1 score for both classes.\n    \"\"\"\n    y_true = []\n    y_pred = []\n\n    # Create a set of all files to ensure we evaluate all possible pairs\n    all_files = set(ground_truth.keys()).union(set(retrieved.keys()))\n\n    for file in all_files:\n        gt_duplicates = set(ground_truth.get(file, []))\n        retrieved_duplicates = set(retrieved.get(file, []))\n\n        # For each file, determine if it is a duplicate or not\n        for other_file in all_files:\n            if other_file == file:\n                continue\n\n            # Determine ground truth and prediction for this pair\n            is_duplicate_gt = other_file in gt_duplicates\n            is_duplicate_pred = other_file in retrieved_duplicates\n\n            y_true.append(1 if is_duplicate_gt else 0)\n            y_pred.append(1 if is_duplicate_pred else 0)\n\n    # Calculate precision, recall, and f1 score for each class\n    precision = precision_score(y_true, y_pred, average=None, labels=[0, 1])\n    recall = recall_score(y_true, y_pred, average=None, labels=[0, 1])\n    f1 = f1_score(y_true, y_pred, average=None, labels=[0, 1])\n\n    return {\n        'class_0': {'precision': precision[0], 'recall': recall[0], 'f1_score': f1[0]},\n        'class_1': {'precision': precision[1], 'recall': recall[1], 'f1_score': f1[1]}\n    }\n```\n\n### Explanation:\n- **Inputs**: The function takes two dictionaries: `ground_truth` and `retrieved`, where each key is a filename and the value is a list of filenames considered duplicates.\n- **Outputs**: It returns a dictionary with precision, recall, and F1 score for both class 0 (non-duplicates) and class 1 (duplicates).\n- **Logic**:\n  - It iterates over all possible file pairs.\n  - For each pair, it checks if the pair is considered a duplicate in both the ground truth and retrieved dictionaries.\n  - It uses these checks to build `y_true` and `y_pred` lists, which are then used to calculate precision, recall, and F1 score using `sklearn` metrics functions.",
    "prompt": "Generate a python function that Given ground truth dictionary and retrieved dictionary, return per class precision, recall and f1 score. Class 1 is\n    assigned to duplicate file pairs while class 0 is for non-duplicate file pairs.\n\n    Args:\n        ground_truth: A dictionary representing ground truth with filenames as key and a list of duplicate filenames\n        as value.\n        retrieved: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved\n        duplicate filenames as value.\n\n    Returns:\n        Dictionary of precision, recall and f1 score for both classes.",
    "function_name": "classification_metrics",
    "ground_truth": "imagededup/handlers/metrics/classification.py#L69",
    "test": "tests/test_evaluator.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Classification",
    "data": "image"
}