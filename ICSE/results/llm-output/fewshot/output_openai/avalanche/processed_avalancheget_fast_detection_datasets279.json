{
    "result": "To create a function `get_fast_detection_datasets` that generates synthetic datasets for object detection tasks, we need to consider several aspects such as generating random images, creating bounding boxes, and assigning class labels. The function will also need to ensure that the generated datasets meet the specified requirements. Here's a possible implementation:\n\n```python\nimport numpy as np\nimport torch\nfrom typing import Tuple, List, Dict\nfrom sklearn.utils import check_random_state\n\ndef get_fast_detection_datasets(\n    n_images: int,\n    max_elements_per_image: int,\n    n_samples_per_class: int,\n    n_classes: int,\n    seed: int,\n    image_size: Tuple[int, int],\n    n_test_images: int\n) -> Tuple[Dict[str, List], Dict[str, List]]:\n    \"\"\"\n    Generate synthetic datasets for object detection tasks.\n\n    Args:\n        n_images (int): Total number of images.\n        max_elements_per_image (int): Maximum number of objects per image.\n        n_samples_per_class (int): Number of samples per class.\n        n_classes (int): Number of different classes.\n        seed (int): Random seed for reproducibility.\n        image_size (Tuple[int, int]): Size of each image (height, width).\n        n_test_images (int): Number of images to be used for testing.\n\n    Returns:\n        Tuple[Dict[str, List], Dict[str, List]]: Training and testing datasets.\n    \"\"\"\n    # Check assertions\n    assert n_images > 0, \"Number of images must be greater than zero.\"\n    assert n_test_images > 0, \"Number of test images must be greater than zero.\"\n    assert n_test_images < n_images, \"Number of test images must be less than total number of images.\"\n    assert n_samples_per_class * n_classes <= n_images * max_elements_per_image, (\n        \"Total number of samples must be greater than or equal to the required samples per class.\"\n    )\n\n    # Initialize random state\n    rs = check_random_state(seed)\n\n    # Generate synthetic images and targets\n    def generate_dataset(num_images: int) -> Dict[str, List]:\n        images = []\n        targets = []\n\n        for _ in range(num_images):\n            # Create a random image\n            image = rs.randint(0, 256, size=(3, *image_size), dtype=np.uint8)\n            images.append(torch.tensor(image, dtype=torch.uint8))\n\n            # Determine number of elements in this image\n            num_elements = rs.randint(1, max_elements_per_image + 1)\n\n            # Create random bounding boxes and labels\n            boxes = []\n            labels = []\n            for _ in range(num_elements):\n                xmin = rs.randint(0, image_size[1] - 1)\n                ymin = rs.randint(0, image_size[0] - 1)\n                xmax = rs.randint(xmin + 1, image_size[1])\n                ymax = rs.randint(ymin + 1, image_size[0])\n                boxes.append([xmin, ymin, xmax, ymax])\n                labels.append(rs.randint(0, n_classes))\n\n            targets.append({\n                'boxes': torch.tensor(boxes, dtype=torch.float32),\n                'labels': torch.tensor(labels, dtype=torch.int64)\n            })\n\n        return {'images': images, 'targets': targets}\n\n    # Split into training and testing datasets\n    train_dataset = generate_dataset(n_images - n_test_images)\n    test_dataset = generate_dataset(n_test_images)\n\n    return train_dataset, test_dataset\n```\n\n### Explanation:\n- **Assertions**: The function checks that the number of test images is valid and that the total number of samples is sufficient to meet the requirements.\n- **Random State**: A random state is used for reproducibility.\n- **Image and Target Generation**: For each image, random pixel values are generated, and random bounding boxes and labels are assigned.\n- **Dataset Splitting**: The function generates separate datasets for training and testing based on the specified number of test images.",
    "prompt": "Write a Python function `get_fast_detection_datasets` that generates synthetic datasets for object detection tasks. The function should accept the following inputs: `n_images` (number of total images), `max_elements_per_image` (maximum number of objects per image), `n_samples_per_class` (number of samples per class), `n_classes` (number of different classes), `seed` (random seed for reproducibility), `image_size` (size of each image), and `n_test_images` (number of images to be used for testing). The function should ensure that the number of generated images and elements per image are sufficient to cover the required samples per class and that the number of test images is within valid bounds. Use assertions to check that the total number of samples is greater than or equal to the required samples per class and that the number of test images is less than the total number of images and greater than zero. The function should return two datasets, one for training and one for testing, each containing images and corresponding detection targets.",
    "function_name": "get_fast_detection_datasets",
    "ground_truth": "tests/unit_tests_utils.py",
    "test": "tests/benchmarks/utils/test_transformations.py",
    "class": "",
    "stage": "Pre-Post processing",
    "task": "Detection",
    "data": "Image"
}