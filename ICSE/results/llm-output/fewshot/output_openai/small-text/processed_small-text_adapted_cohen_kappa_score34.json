{
    "result": "To create a Python function called `_adapted_cohen_kappa_score` that extends Cohen's kappa by handling the special case of perfect agreement and preventing a division by zero error, you can follow the structure of the existing `cohen_kappa_score` function from libraries like `sklearn.metrics`. Here's an example implementation:\n\n```python\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\ndef _adapted_cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None):\n    \"\"\"\n    Calculate Cohen's kappa score with an adaptation to handle perfect agreement.\n\n    Cohen's kappa is a statistic that measures inter-annotator agreement for categorical items.\n    This function extends the standard Cohen's kappa by handling the special case of perfect\n    agreement to prevent a division by zero error.\n\n    Args:\n        y1 (array-like): First set of labels.\n        y2 (array-like): Second set of labels.\n        labels (array-like, optional): List of labels to index the matrix. This may be used to\n            select a subset of labels. If None, all labels in y1 and y2 are used.\n        weights (str or array-like, optional): Weighting type to calculate the score. Options are\n            'linear', 'quadratic', or None. If None, no weighting is applied.\n        sample_weight (array-like, optional): Sample weights.\n\n    Returns:\n        float: The kappa score, which ranges from -1 (complete disagreement) to 1 (complete agreement).\n    \"\"\"\n    # Compute the confusion matrix\n    cm = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)\n    \n    # Number of samples\n    n = np.sum(cm)\n    \n    # Handle the special case of perfect agreement\n    if np.array_equal(y1, y2):\n        return 1.0\n\n    # Calculate the observed agreement\n    po = np.trace(cm) / n\n\n    # Calculate the expected agreement\n    sum0 = np.sum(cm, axis=0)\n    sum1 = np.sum(cm, axis=1)\n    pe = np.dot(sum0, sum1) / (n ** 2)\n\n    # Calculate Cohen's kappa\n    kappa = (po - pe) / (1 - pe)\n\n    return kappa\n```\n\n### Key Points:\n- **Perfect Agreement Handling**: The function checks if `y1` and `y2` are identical, which indicates perfect agreement, and directly returns a kappa score of 1.0.\n- **Confusion Matrix**: The function uses `confusion_matrix` from `sklearn.metrics` to compute the confusion matrix, which is essential for calculating observed and expected agreements.\n- **Observed and Expected Agreement**: The observed agreement (`po`) is the proportion of times the raters agree, and the expected agreement (`pe`) is the agreement expected by chance.\n- **Kappa Calculation**: The kappa score is calculated using the formula \\((po - pe) / (1 - pe)\\).\n\nThis implementation ensures that the function handles the special case of perfect agreement without encountering a division by zero error.",
    "prompt": "Create a Python function called _adapted_cohen_kappa_score that extends Cohen's kappa by handling the special case of perfect agreement, preventing a division by zero error. \ndef _adapted_cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None):",
    "function_name": "_adapted_cohen_kappa_score",
    "ground_truth": "small_text/stopping_criteria/kappa.py",
    "test": "tests/unit/small_text/stopping_criteria/test_kappa.py::TestAdaptedCohenKappScore",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}