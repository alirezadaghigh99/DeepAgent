{
    "result": "Here's a Python function `get_label_quality_scores` that computes label quality scores for a standard multi-class classification dataset. This function uses the specified method to calculate scores, which indicate the likelihood of a label being correct, with scores ranging from 0 (likely incorrect) to 1 (likely correct).\n\n```python\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef get_label_quality_scores(labels, pred_probs, method=\"self_confidence\", adjust_pred_probs=False):\n    \"\"\"\n    Compute label quality scores for a multi-class classification dataset.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A discrete vector of noisy labels, i.e., some labels may be erroneous.\n        Format: for a dataset with K classes, labels must be in 0, 1, ..., K-1.\n\n    pred_probs : np.ndarray\n        An array of shape (N, K) of model-predicted probabilities, P(label=k|x).\n\n    method : {\"self_confidence\", \"normalized_margin\", \"confidence_weighted_entropy\"}, default=\"self_confidence\"\n        Label quality scoring method.\n\n    adjust_pred_probs : bool, optional\n        Account for class imbalance in the label-quality scoring by adjusting predicted probabilities.\n\n    Returns\n    -------\n    label_quality_scores : np.ndarray\n        Contains one score (between 0 and 1) per example.\n        Lower scores indicate more likely mislabeled examples.\n    \"\"\"\n    if adjust_pred_probs:\n        class_counts = np.bincount(labels, minlength=pred_probs.shape[1])\n        class_probs = class_counts / class_counts.sum()\n        pred_probs = pred_probs / class_probs\n        pred_probs = pred_probs / pred_probs.sum(axis=1, keepdims=True)\n\n    scores = np.zeros(labels.shape[0])\n\n    for i, (label, probs) in enumerate(zip(labels, pred_probs)):\n        if method == \"self_confidence\":\n            score = probs[label]\n        elif method == \"normalized_margin\":\n            max_other_prob = np.max(np.delete(probs, label))\n            score = probs[label] - max_other_prob\n        elif method == \"confidence_weighted_entropy\":\n            score = entropy(probs) / probs[label]\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        # Normalize score to be between 0 and 1\n        scores[i] = np.clip(score, 0, 1)\n\n    return scores\n```\n\n### Explanation:\n- **Parameters:**\n  - `labels`: A numpy array of true labels for the dataset.\n  - `pred_probs`: A numpy array of predicted probabilities for each class.\n  - `method`: The method used to compute the label quality score. Options include \"self_confidence\", \"normalized_margin\", and \"confidence_weighted_entropy\".\n  - `adjust_pred_probs`: A boolean indicating whether to adjust predicted probabilities for class imbalance.\n\n- **Returns:**\n  - `label_quality_scores`: A numpy array containing a score for each example, where lower scores indicate more likely mislabeled examples.\n\n- **Methods:**\n  - **Self Confidence**: Uses the predicted probability of the true label.\n  - **Normalized Margin**: Computes the difference between the predicted probability of the true label and the highest predicted probability of any other label.\n  - **Confidence Weighted Entropy**: Computes the entropy of the predicted probabilities, weighted by the self-confidence score.\n\n- **Adjustments for Class Imbalance:**\n  - If `adjust_pred_probs` is `True`, the predicted probabilities are adjusted to account for class imbalance by normalizing with class probabilities.",
    "prompt": "Generate a Python function called get_label_quality_scores that Returns a label quality score for each datapoint.\n\n    This is a function to compute label quality scores for standard (multi-class) classification datasets,\n    where lower scores indicate labels less likely to be correct.\n\n    Score is between 0 and 1.\n\n    1 - clean label (given label is likely correct).\n    0 - dirty label (given label is likely incorrect).\n\n    Parameters\n    ----------\n    labels : np.ndarray\n      A discrete vector of noisy labels, i.e. some labels may be erroneous.\n      *Format requirements*: for dataset with K classes, labels must be in 0, 1, ..., K-1.\n      Note: multi-label classification is not supported by this method, each example must belong to a single class, e.g. format: ``labels = np.ndarray([1,0,2,1,1,0...])``.\n\n    pred_probs : np.ndarray, optional\n      An array of shape ``(N, K)`` of model-predicted probabilities,\n      ``P(label=k|x)``. Each row of this matrix corresponds\n      to an example `x` and contains the model-predicted probabilities that\n      `x` belongs to each possible class, for each of the K classes. The\n      columns must be ordered such that these probabilities correspond to\n      class 0, 1, ..., K-1.\n\n      **Note**: Returned label issues are most accurate when they are computed based on out-of-sample `pred_probs` from your model.\n      To obtain out-of-sample predicted probabilities for every datapoint in your dataset, you can use :ref:`cross-validation <pred_probs_cross_val>`.\n      This is encouraged to get better results.\n\n    method : {\"self_confidence\", \"normalized_margin\", \"confidence_weighted_entropy\"}, default=\"self_confidence\"\n      Label quality scoring method.\n\n      Letting ``k = labels[i]`` and ``P = pred_probs[i]`` denote the given label and predicted class-probabilities\n      for datapoint *i*, its score can either be:\n\n      - ``'normalized_margin'``: ``P[k] - max_{k' != k}[ P[k'] ]``\n      - ``'self_confidence'``: ``P[k]``\n      - ``'confidence_weighted_entropy'``: ``entropy(P) / self_confidence``\n\n      Note: the actual label quality scores returned by this method\n      may be transformed versions of the above, in order to ensure\n      their values lie between 0-1 with lower values indicating more likely mislabeled data.\n\n      Let ``C = {0, 1, ..., K-1}`` be the set of classes specified for our classification task.\n\n      The `normalized_margin` score works better for identifying class conditional label errors,\n      i.e. examples for which another label in ``C`` is appropriate but the given label is not.\n\n      The `self_confidence` score works better for identifying alternative label issues\n      corresponding to bad examples that are: not from any of the classes in ``C``,\n      well-described by 2 or more labels in ``C``,\n      or generally just out-of-distribution (i.e. anomalous outliers).\n\n    adjust_pred_probs : bool, optional\n      Account for class imbalance in the label-quality scoring by adjusting predicted probabilities\n      via subtraction of class confident thresholds and renormalization.\n      Set this to ``True`` if you prefer to account for class-imbalance.\n      See `Northcutt et al., 2021 <https://jair.org/index.php/jair/article/view/12125>`_.\n\n    Returns\n    -------\n    label_quality_scores : np.ndarray\n      Contains one score (between 0 and 1) per example.\n      Lower scores indicate more likely mislabeled examples.",
    "function_name": "get_label_quality_scores",
    "ground_truth": "cleanlab/rank.py",
    "test": "tests/test_object_detection.py",
    "class": "",
    "stage": "Inference",
    "task": "Classification",
    "data": ""
}