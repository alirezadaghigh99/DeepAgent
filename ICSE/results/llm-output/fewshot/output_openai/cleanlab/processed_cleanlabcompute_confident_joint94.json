{
    "result": "To create a function called `compute_confident_joint` that estimates the confident counts of latent true vs observed noisy labels, we can follow the specifications you've provided. This function will handle both single-label and multi-label classification scenarios, and it will include options for calibration and returning indices of off-diagonal elements. Here's a possible implementation:\n\n```python\nimport numpy as np\n\ndef compute_confident_joint(\n    labels,\n    pred_probs,\n    thresholds=None,\n    calibrate=False,\n    multi_label=False,\n    return_indices_of_off_diagonals=False\n):\n    \"\"\"\n    Estimate the confident counts of latent true vs observed noisy labels.\n\n    Args:\n        labels (array-like): Class labels for each example in the dataset.\n        pred_probs (array-like): Model-predicted class probabilities for each example.\n        thresholds (array-like, optional): Per-class threshold probabilities. Defaults to None.\n        calibrate (bool): Whether to calibrate the confident joint estimate. Defaults to False.\n        multi_label (bool, optional): If the dataset is multi-label classification. Defaults to False.\n        return_indices_of_off_diagonals (bool, optional): Whether to return indices of examples counted in off-diagonals. Defaults to False.\n\n    Returns:\n        np.ndarray: Array representing counts of examples for which we are confident about their given and true label.\n    \"\"\"\n    labels = np.asarray(labels)\n    pred_probs = np.asarray(pred_probs)\n    num_classes = pred_probs.shape[1]\n\n    if thresholds is None:\n        thresholds = np.full(num_classes, 0.5)\n\n    confident_joint = np.zeros((num_classes, num_classes), dtype=int)\n    off_diagonal_indices = []\n\n    for i, (label, probs) in enumerate(zip(labels, pred_probs)):\n        if multi_label:\n            confident_labels = np.where(probs >= thresholds)[0]\n            for cl in confident_labels:\n                confident_joint[cl, label] += 1\n                if cl != label:\n                    off_diagonal_indices.append(i)\n        else:\n            predicted_label = np.argmax(probs)\n            if probs[predicted_label] >= thresholds[predicted_label]:\n                confident_joint[predicted_label, label] += 1\n                if predicted_label != label:\n                    off_diagonal_indices.append(i)\n\n    if calibrate:\n        # Placeholder for calibration logic\n        # Implement calibration logic if needed\n        pass\n\n    if return_indices_of_off_diagonals:\n        return confident_joint, off_diagonal_indices\n    else:\n        return confident_joint\n\n```\n\n### Explanation:\n\n- **Inputs:**\n  - `labels`: The true labels for each example.\n  - `pred_probs`: The predicted probabilities for each class.\n  - `thresholds`: Optional thresholds for each class to determine confidence.\n  - `calibrate`: A flag to indicate if calibration is needed (not implemented here, but a placeholder is provided).\n  - `multi_label`: A flag to indicate if the dataset is multi-label.\n  - `return_indices_of_off_diagonals`: A flag to indicate if indices of off-diagonal elements should be returned.\n\n- **Outputs:**\n  - The function returns a matrix (`confident_joint`) representing the confident counts of true vs. observed labels.\n  - If `return_indices_of_off_diagonals` is `True`, it also returns the indices of examples counted in off-diagonals.\n\n- **Logic:**\n  - The function iterates over each example, checking if the predicted probabilities exceed the specified thresholds.\n  - It updates the `confident_joint` matrix based on whether the predictions are confident.\n  - For multi-label classification, it checks each class independently.\n  - Off-diagonal indices are collected if requested.\n\nThis implementation is vectorized to the extent possible, but further optimizations could be made depending on specific use cases and data structures.",
    "prompt": "Generate a Python function called compute_confident_joint that estimates the confident counts of latent true vs observed noisy labels for the examples in a dataset. The function takes in the following parameters:\n\n- labels: An array or list of class labels for each example in the dataset.\n- pred_probs: An array of model-predicted class probabilities for each example in the dataset.\n- thresholds: An optional array of per-class threshold probabilities.\n- calibrate: A boolean flag indicating whether to calibrate the confident joint estimate.\n- multi_label: An optional boolean flag indicating if the dataset is multi-label classification.\n- return_indices_of_off_diagonals: An optional boolean flag indicating whether to return indices of examples counted in off-diagonals of the confident joint.\n\nThe function returns an array representing counts of examples for which we are confident about their given and true label. If multi_label is True, the array has a different shape and represents a one-vs-rest format for the confident joint.\n\nNote that the function provides a vectorized implementation of the confident joint calculation for efficiency.",
    "function_name": "compute_confident_joint",
    "ground_truth": "cleanlab/count.py",
    "test": "tests/test_filter_count.py",
    "class": "",
    "stage": "Model Construction",
    "task": "Classification",
    "data": ""
}