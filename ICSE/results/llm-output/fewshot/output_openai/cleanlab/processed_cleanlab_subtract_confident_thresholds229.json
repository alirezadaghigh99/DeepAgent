{
    "result": "To implement the `_subtract_confident_thresholds` function, we need to follow these steps:\n\n1. **Check Inputs**: Ensure that either labels or pre-calculated thresholds are provided. If not, raise a `ValueError`.\n2. **Calculate or Use Provided Thresholds**: If thresholds are not provided, calculate them using the `get_confident_thresholds` method.\n3. **Adjust Probabilities**: Subtract the class-specific thresholds from the predicted probabilities.\n4. **Ensure Non-Negative Values**: Shift the probabilities to ensure no negative values.\n5. **Re-normalize Probabilities**: Normalize the adjusted probabilities so that they sum to 1 for each sample.\n6. **Return Adjusted Probabilities**: Return the adjusted probabilities as a NumPy array.\n\nHere's the implementation:\n\n```python\nimport numpy as np\n\ndef get_confident_thresholds(labels, predicted_probs):\n    # Placeholder for the actual implementation of threshold calculation\n    # This function should return an array of thresholds for each class\n    # For simplicity, let's assume it returns a fixed threshold for demonstration\n    num_classes = predicted_probs.shape[1]\n    return np.full(num_classes, 0.1)  # Example: fixed threshold of 0.1 for each class\n\ndef _subtract_confident_thresholds(labels=None, predicted_probs=None, multi_label=False, thresholds=None):\n    if thresholds is None:\n        if labels is None or predicted_probs is None:\n            raise ValueError(\"Either labels and predicted_probs or pre-calculated thresholds must be provided.\")\n        thresholds = get_confident_thresholds(labels, predicted_probs)\n\n    # Subtract thresholds from predicted probabilities\n    adjusted_probs = predicted_probs - thresholds\n\n    # Ensure no negative values by shifting\n    adjusted_probs = np.maximum(adjusted_probs, 0)\n\n    # Re-normalize probabilities\n    if multi_label:\n        # For multi-label, normalize each row independently\n        row_sums = adjusted_probs.sum(axis=1, keepdims=True)\n        # Avoid division by zero\n        row_sums[row_sums == 0] = 1\n        adjusted_probs /= row_sums\n    else:\n        # For single-label, normalize to ensure sum to 1\n        adjusted_probs /= adjusted_probs.sum(axis=1, keepdims=True)\n\n    return adjusted_probs\n\n# Example usage:\n# labels = np.array([...])  # Your labels here\n# predicted_probs = np.array([...])  # Your predicted probabilities here\n# adjusted_probs = _subtract_confident_thresholds(labels=labels, predicted_probs=predicted_probs)\n```\n\n### Explanation:\n- **Threshold Calculation**: The `get_confident_thresholds` function is a placeholder and should be implemented to calculate thresholds based on the dataset.\n- **Adjustment and Normalization**: The function subtracts the thresholds, ensures non-negative values, and normalizes the probabilities. For multi-label settings, each row is normalized independently, while for single-label settings, the probabilities are normalized to sum to 1.\n- **Error Handling**: A `ValueError` is raised if neither labels nor thresholds are provided, ensuring that the function has the necessary information to operate.",
    "prompt": "You need to implement a function named `_subtract_confident_thresholds` that adjusts predicted probabilities by subtracting class-specific confidence thresholds and then re-normalizing the probabilities. This adjustment aims to handle class imbalance in classification tasks. The function accepts labels, predicted probabilities, an optional flag for multi-label settings, and pre-calculated confidence thresholds. If confidence thresholds are not provided, they will be calculated from the labels and predicted probabilities using the `get_confident_thresholds` method. After subtracting the thresholds, the function ensures no negative values by shifting and then re-normalizing the probabilities. The function returns the adjusted predicted probabilities as a NumPy array. If neither labels nor pre-calculated thresholds are provided, a `ValueError` is raised.",
    "function_name": "_subtract_confident_thresholds",
    "ground_truth": "cleanlab/internal/label_quality_utils.py#L26",
    "test": "tests/test_rank.py",
    "class": "",
    "stage": "Inference",
    "task": "",
    "data": "Table"
}