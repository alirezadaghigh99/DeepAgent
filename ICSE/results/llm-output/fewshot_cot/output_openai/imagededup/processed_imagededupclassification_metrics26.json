{
    "result": "To generate a Python function that calculates precision, recall, and F1 score for each class given a ground truth dictionary and a retrieved dictionary, we can follow a step-by-step approach similar to the examples provided. Here's how you can implement this function:\n\n```python\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom typing import Dict, List\n\ndef evaluate_duplicates(ground_truth: Dict[str, List[str]], retrieved: Dict[str, List[str]]) -> Dict[str, Dict[str, float]]:\n    \"\"\"\n    Calculate precision, recall, and F1 score for duplicate and non-duplicate file pairs.\n\n    Args:\n        ground_truth: A dictionary representing ground truth with filenames as key and a list of duplicate filenames as value.\n        retrieved: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved duplicate filenames as value.\n\n    Returns:\n        Dictionary of precision, recall, and F1 score for both classes.\n    \"\"\"\n    # 1. Initialize lists to store true labels and predicted labels\n    y_true = []\n    y_pred = []\n\n    # 2. Iterate over each file in the ground truth\n    for file, true_duplicates in ground_truth.items():\n        # 3. Get the retrieved duplicates for the current file\n        retrieved_duplicates = retrieved.get(file, [])\n\n        # 4. Create a set of all files to consider (union of true and retrieved duplicates)\n        all_files = set(true_duplicates) | set(retrieved_duplicates)\n\n        # 5. For each file in the union, determine true and predicted labels\n        for other_file in all_files:\n            # 6. Append 1 if the file is a true duplicate, else 0\n            y_true.append(1 if other_file in true_duplicates else 0)\n            # 7. Append 1 if the file is a retrieved duplicate, else 0\n            y_pred.append(1 if other_file in retrieved_duplicates else 0)\n\n    # 8. Calculate precision, recall, and F1 score for both classes\n    precision = precision_score(y_true, y_pred, average=None, labels=[0, 1])\n    recall = recall_score(y_true, y_pred, average=None, labels=[0, 1])\n    f1 = f1_score(y_true, y_pred, average=None, labels=[0, 1])\n\n    # 9. Return the results in a dictionary format\n    return {\n        'class_0': {'precision': precision[0], 'recall': recall[0], 'f1_score': f1[0]},\n        'class_1': {'precision': precision[1], 'recall': recall[1], 'f1_score': f1[1]}\n    }\n```\n\n### Explanation:\n\n1. **Initialization**: We start by initializing two lists, `y_true` and `y_pred`, to store the true and predicted labels for each file pair.\n\n2. **Iterate Over Ground Truth**: We iterate over each file in the ground truth dictionary. For each file, we retrieve the list of true duplicates and the list of retrieved duplicates.\n\n3. **Union of Files**: We create a set of all files to consider by taking the union of true and retrieved duplicates. This ensures that we evaluate all possible pairs.\n\n4. **Determine Labels**: For each file in the union, we determine whether it is a true duplicate and whether it is a retrieved duplicate, appending the appropriate labels to `y_true` and `y_pred`.\n\n5. **Calculate Metrics**: We use `precision_score`, `recall_score`, and `f1_score` from `sklearn.metrics` to calculate the precision, recall, and F1 score for both classes (0 and 1).\n\n6. **Return Results**: Finally, we return the results in a dictionary format, with separate entries for class 0 (non-duplicates) and class 1 (duplicates).",
    "prompt": "Generate a python function that Given ground truth dictionary and retrieved dictionary, return per class precision, recall and f1 score. Class 1 is\n    assigned to duplicate file pairs while class 0 is for non-duplicate file pairs.\n\n    Args:\n        ground_truth: A dictionary representing ground truth with filenames as key and a list of duplicate filenames\n        as value.\n        retrieved: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved\n        duplicate filenames as value.\n\n    Returns:\n        Dictionary of precision, recall and f1 score for both classes.",
    "function_name": "classification_metrics",
    "ground_truth": "imagededup/handlers/metrics/classification.py#L69",
    "test": "tests/test_evaluator.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Classification",
    "data": "image"
}