{
    "result": "To create the `depth_to_3d` function, we will follow a step-by-step approach similar to the examples provided. This function will convert depth values into 3D points using camera intrinsics. Here's how you can implement it:\n\n```python\nimport torch\n\ndef depth_to_3d(depth: torch.Tensor, camera_matrix: torch.Tensor, normalize: bool = False) -> torch.Tensor:\n    \"\"\"\n    Convert depth values to 3D points using camera intrinsics.\n\n    Args:\n        depth (Tensor): Depth tensor of shape (B, 1, H, W).\n        camera_matrix (Tensor): Camera intrinsics tensor of shape (B, 3, 3).\n        normalize (bool): Whether to normalize the 3D points.\n\n    Returns:\n        Tensor: 3D points tensor of shape (B, 3, H, W).\n    \"\"\"\n    # ---------------------------------------------------------------\n    # 1) Validate input types and shapes\n    # ---------------------------------------------------------------\n    if not isinstance(depth, torch.Tensor):\n        raise TypeError(\"depth must be a torch.Tensor\")\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise TypeError(\"camera_matrix must be a torch.Tensor\")\n    if depth.ndim != 4 or depth.shape[1] != 1:\n        raise ValueError(\"depth must have shape (B, 1, H, W)\")\n    if camera_matrix.ndim != 3 or camera_matrix.shape[1:] != (3, 3):\n        raise ValueError(\"camera_matrix must have shape (B, 3, 3)\")\n\n    B, _, H, W = depth.shape\n\n    # ---------------------------------------------------------------\n    # 2) Create a meshgrid of pixel coordinates\n    # ---------------------------------------------------------------\n    y_coords, x_coords = torch.meshgrid(torch.arange(H, device=depth.device), torch.arange(W, device=depth.device))\n    y_coords = y_coords.expand(B, -1, -1)\n    x_coords = x_coords.expand(B, -1, -1)\n\n    # ---------------------------------------------------------------\n    # 3) Flatten the pixel coordinates and depth values\n    # ---------------------------------------------------------------\n    x_flat = x_coords.reshape(B, -1)\n    y_flat = y_coords.reshape(B, -1)\n    depth_flat = depth.reshape(B, -1)\n\n    # ---------------------------------------------------------------\n    # 4) Compute the 3D points using the camera intrinsics\n    # ---------------------------------------------------------------\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n\n    X = (x_flat - cx.unsqueeze(1)) * depth_flat / fx.unsqueeze(1)\n    Y = (y_flat - cy.unsqueeze(1)) * depth_flat / fy.unsqueeze(1)\n    Z = depth_flat\n\n    # ---------------------------------------------------------------\n    # 5) Stack the 3D coordinates\n    # ---------------------------------------------------------------\n    points_3d = torch.stack((X, Y, Z), dim=1).reshape(B, 3, H, W)\n\n    # ---------------------------------------------------------------\n    # 6) Normalize the 3D points if required\n    # ---------------------------------------------------------------\n    if normalize:\n        norm = torch.norm(points_3d, dim=1, keepdim=True)\n        points_3d = points_3d / (norm + 1e-8)\n\n    # ---------------------------------------------------------------\n    # 7) Return the 3D points tensor\n    # ---------------------------------------------------------------\n    return points_3d\n```\n\n### Explanation:\n\n1. **Input Validation**: We first check that the inputs are tensors and have the correct shapes.\n\n2. **Meshgrid Creation**: We create a meshgrid of pixel coordinates for the height and width of the image.\n\n3. **Flattening**: We flatten the pixel coordinates and depth values to simplify calculations.\n\n4. **3D Point Calculation**: Using the camera intrinsics, we compute the 3D coordinates for each pixel.\n\n5. **Stacking**: We stack the computed X, Y, and Z coordinates to form the 3D points.\n\n6. **Normalization**: If the `normalize` flag is set, we normalize the 3D points.\n\n7. **Return**: Finally, we return the 3D points tensor.",
    "prompt": "Generate a Python function called depth_to_3d that takes in a tensor representing depth values per pixel, a tensor containing camera intrinsics, and a boolean flag to normalize points. The function computes a 3D point per pixel based on the depth value and camera intrinsics. The input depth tensor should have a shape of (B, 1, H, W) and the camera_matrix tensor should have a shape of (B, 3, 3). The function returns a tensor with a 3D point per pixel of the same resolution as the input, with a shape of (B, 3, H, W). The function also includes error handling for input types and shapes.",
    "function_name": "depth_to_3d",
    "ground_truth": "kornia/geometry/depth.py",
    "test": "tests/geometry/test_depth.py::TestDepthTo3d::test_shapes_broadcast",
    "class": "",
    "stage": "Model Construction",
    "task": "",
    "data": "Image"
}