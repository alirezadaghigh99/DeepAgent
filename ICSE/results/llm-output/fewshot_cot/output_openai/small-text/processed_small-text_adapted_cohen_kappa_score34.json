{
    "result": "To create a Python function called `_adapted_cohen_kappa_score` that extends Cohen's kappa by handling the special case of perfect agreement and preventing a division by zero error, you can follow these steps:\n\n1. **Import Necessary Libraries**: You'll need `numpy` for numerical operations and `sklearn.metrics` for the original Cohen's kappa implementation.\n\n2. **Define the Function**: Start by defining the function with the appropriate parameters.\n\n3. **Handle Input Validation**: Ensure that the input arrays `y1` and `y2` are valid and have the same length.\n\n4. **Compute the Confusion Matrix**: Use the confusion matrix to calculate observed and expected agreements.\n\n5. **Check for Perfect Agreement**: Before computing the kappa score, check if the observed agreement is perfect.\n\n6. **Calculate Kappa Score**: If not perfect, calculate the kappa score using the standard formula.\n\n7. **Return the Result**: Return the kappa score, ensuring no division by zero occurs.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\ndef _adapted_cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None):\n    \"\"\"\n    Calculate Cohen's kappa score with an adaptation for perfect agreement.\n\n    Args:\n        y1 (array-like): First set of labels.\n        y2 (array-like): Second set of labels.\n        labels (array-like, optional): List of labels to index the matrix.\n        weights (str or array-like, optional): Weighting type to calculate the score.\n        sample_weight (array-like, optional): Sample weights.\n\n    Returns:\n        float: The kappa score.\n    \"\"\"\n    # 1. Validate input lengths\n    if len(y1) != len(y2):\n        raise ValueError(\"Input arrays must have the same length.\")\n\n    # 2. Compute the confusion matrix\n    cm = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)\n\n    # 3. Calculate observed agreement\n    observed_agreement = np.trace(cm)\n\n    # 4. Calculate expected agreement\n    sum0 = np.sum(cm, axis=0)\n    sum1 = np.sum(cm, axis=1)\n    expected_agreement = np.dot(sum0, sum1) / np.sum(cm)\n\n    # 5. Check for perfect agreement\n    if observed_agreement == np.sum(cm):\n        return 1.0\n\n    # 6. Calculate kappa score\n    total = np.sum(cm)\n    po = observed_agreement / total\n    pe = expected_agreement / total\n    kappa = (po - pe) / (1 - pe)\n\n    # 7. Return the kappa score\n    return kappa\n```\n\n### Explanation:\n\n- **Input Validation**: The function checks if the input arrays `y1` and `y2` have the same length.\n- **Confusion Matrix**: The confusion matrix is computed to determine the observed and expected agreements.\n- **Perfect Agreement Check**: If the observed agreement equals the total number of samples, the function returns a kappa score of 1.0, indicating perfect agreement.\n- **Kappa Calculation**: If not perfect, the kappa score is calculated using the standard formula, ensuring no division by zero occurs.\n\nThis function extends the standard Cohen's kappa by handling the special case of perfect agreement, thus preventing a division by zero error.",
    "prompt": "Create a Python function called _adapted_cohen_kappa_score that extends Cohen's kappa by handling the special case of perfect agreement, preventing a division by zero error. \ndef _adapted_cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None):",
    "function_name": "_adapted_cohen_kappa_score",
    "ground_truth": "small_text/stopping_criteria/kappa.py",
    "test": "tests/unit/small_text/stopping_criteria/test_kappa.py::TestAdaptedCohenKappScore",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}