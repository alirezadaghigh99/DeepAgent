{
    "result": "To create a function `get_fast_detection_datasets` that generates synthetic datasets for object detection tasks, we need to consider several steps. The function will generate random images and corresponding bounding boxes with class labels. Here's a step-by-step implementation:\n\n```python\nimport numpy as np\nfrom typing import Tuple, List, Dict\n\ndef get_fast_detection_datasets(\n    n_images: int,\n    max_elements_per_image: int,\n    n_samples_per_class: int,\n    n_classes: int,\n    seed: int,\n    image_size: Tuple[int, int],\n    n_test_images: int\n) -> Tuple[List[Dict[str, np.ndarray]], List[Dict[str, np.ndarray]]]:\n    \"\"\"\n    Generate synthetic datasets for object detection tasks.\n\n    Args:\n        n_images (int): Total number of images.\n        max_elements_per_image (int): Maximum number of objects per image.\n        n_samples_per_class (int): Number of samples per class.\n        n_classes (int): Number of different classes.\n        seed (int): Random seed for reproducibility.\n        image_size (Tuple[int, int]): Size of each image (height, width).\n        n_test_images (int): Number of images to be used for testing.\n\n    Returns:\n        Tuple containing two lists of dictionaries: training and testing datasets.\n        Each dictionary contains 'image' and 'targets' (bounding boxes and labels).\n    \"\"\"\n    # Set random seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # Assertions to ensure valid input\n    total_samples_needed = n_samples_per_class * n_classes\n    assert n_images * max_elements_per_image >= total_samples_needed, (\n        \"Total number of samples is less than required samples per class.\"\n    )\n    assert 0 < n_test_images < n_images, (\n        \"Number of test images must be less than total images and greater than zero.\"\n    )\n\n    # Initialize datasets\n    train_dataset = []\n    test_dataset = []\n\n    # Generate images and targets\n    for i in range(n_images):\n        # Create a random image\n        image = rng.integers(0, 256, size=(3, *image_size), dtype=np.uint8)\n\n        # Determine the number of elements in this image\n        n_elements = rng.integers(1, max_elements_per_image + 1)\n\n        # Generate random bounding boxes and class labels\n        boxes = rng.uniform(0, min(image_size), size=(n_elements, 4))\n        boxes[:, 2:] += boxes[:, :2]  # Ensure (xmin, ymin, xmax, ymax) format\n        boxes = np.clip(boxes, 0, min(image_size))  # Clip to image size\n        labels = rng.integers(0, n_classes, size=(n_elements,))\n\n        # Create a dictionary for the image and its targets\n        data = {\n            'image': image,\n            'targets': {\n                'boxes': boxes,\n                'labels': labels\n            }\n        }\n\n        # Split into train and test datasets\n        if i < n_test_images:\n            test_dataset.append(data)\n        else:\n            train_dataset.append(data)\n\n    return train_dataset, test_dataset\n\n# Example usage:\ntrain_data, test_data = get_fast_detection_datasets(\n    n_images=100,\n    max_elements_per_image=10,\n    n_samples_per_class=50,\n    n_classes=5,\n    seed=42,\n    image_size=(128, 128),\n    n_test_images=20\n)\n```\n\n### Explanation:\n\n1. **Random Seed**: We use `np.random.default_rng(seed)` to ensure reproducibility.\n\n2. **Assertions**: We check that the total number of samples is sufficient and that the number of test images is valid.\n\n3. **Image Generation**: Each image is generated as a random array of integers between 0 and 255, representing pixel values.\n\n4. **Bounding Boxes and Labels**: For each image, we generate a random number of bounding boxes and class labels. The bounding boxes are in the format `(xmin, ymin, xmax, ymax)`.\n\n5. **Dataset Splitting**: We split the generated data into training and testing datasets based on the specified number of test images.\n\nThis function provides a basic framework for generating synthetic datasets for object detection tasks. You can further customize it to include more complex scenarios or additional features.",
    "prompt": "Write a Python function `get_fast_detection_datasets` that generates synthetic datasets for object detection tasks. The function should accept the following inputs: `n_images` (number of total images), `max_elements_per_image` (maximum number of objects per image), `n_samples_per_class` (number of samples per class), `n_classes` (number of different classes), `seed` (random seed for reproducibility), `image_size` (size of each image), and `n_test_images` (number of images to be used for testing). The function should ensure that the number of generated images and elements per image are sufficient to cover the required samples per class and that the number of test images is within valid bounds. Use assertions to check that the total number of samples is greater than or equal to the required samples per class and that the number of test images is less than the total number of images and greater than zero. The function should return two datasets, one for training and one for testing, each containing images and corresponding detection targets.",
    "function_name": "get_fast_detection_datasets",
    "ground_truth": "tests/unit_tests_utils.py",
    "test": "tests/benchmarks/utils/test_transformations.py",
    "class": "",
    "stage": "Pre-Post processing",
    "task": "Detection",
    "data": "Image"
}