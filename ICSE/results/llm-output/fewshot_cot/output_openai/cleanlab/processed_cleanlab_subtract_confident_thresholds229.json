{
    "result": "To implement the `_subtract_confident_thresholds` function, we need to follow a series of steps to adjust the predicted probabilities by subtracting class-specific confidence thresholds and then re-normalizing them. Here's a step-by-step guide to implementing this function:\n\n```python\nimport numpy as np\n\ndef get_confident_thresholds(labels, predicted_probs, multi_label=False):\n    \"\"\"\n    Calculate class-specific confidence thresholds based on the given labels and predicted probabilities.\n    This is a placeholder function and should be implemented based on specific requirements.\n    \"\"\"\n    # Placeholder implementation: calculate mean probability for each class as a threshold\n    if multi_label:\n        thresholds = np.mean(predicted_probs, axis=0)\n    else:\n        thresholds = np.mean(predicted_probs, axis=0)\n    return thresholds\n\ndef _subtract_confident_thresholds(labels, predicted_probs, multi_label=False, thresholds=None):\n    \"\"\"\n    Adjust predicted probabilities by subtracting class-specific confidence thresholds and re-normalizing.\n\n    Args:\n        labels (np.ndarray): True labels for the data.\n        predicted_probs (np.ndarray): Predicted probabilities for each class.\n        multi_label (bool): Flag indicating if the task is multi-label.\n        thresholds (np.ndarray, optional): Pre-calculated confidence thresholds.\n\n    Returns:\n        np.ndarray: Adjusted predicted probabilities.\n    \"\"\"\n    # 1. Validate inputs\n    if thresholds is None and labels is None:\n        raise ValueError(\"Either labels or pre-calculated thresholds must be provided.\")\n\n    # 2. Calculate thresholds if not provided\n    if thresholds is None:\n        thresholds = get_confident_thresholds(labels, predicted_probs, multi_label)\n\n    # 3. Subtract thresholds from predicted probabilities\n    adjusted_probs = predicted_probs - thresholds\n\n    # 4. Ensure no negative values by shifting\n    adjusted_probs = np.maximum(adjusted_probs, 0)\n\n    # 5. Re-normalize probabilities\n    if multi_label:\n        # Normalize each sample independently for multi-label\n        row_sums = adjusted_probs.sum(axis=1, keepdims=True)\n        adjusted_probs = np.divide(adjusted_probs, row_sums, where=row_sums != 0)\n    else:\n        # Normalize across classes for single-label\n        adjusted_probs /= adjusted_probs.sum(axis=1, keepdims=True)\n\n    # 6. Return adjusted probabilities\n    return adjusted_probs\n```\n\n### Explanation:\n\n1. **Input Validation**: The function checks if either `labels` or `thresholds` are provided. If neither is available, it raises a `ValueError`.\n\n2. **Threshold Calculation**: If thresholds are not provided, the function calculates them using a placeholder `get_confident_thresholds` function. This function should be implemented based on specific requirements for calculating thresholds.\n\n3. **Threshold Subtraction**: The function subtracts the calculated or provided thresholds from the predicted probabilities.\n\n4. **Non-Negative Adjustment**: It ensures that no negative probabilities exist by using `np.maximum` to set any negative values to zero.\n\n5. **Re-normalization**: The function re-normalizes the probabilities. For multi-label tasks, it normalizes each sample independently. For single-label tasks, it normalizes across classes.\n\n6. **Return**: Finally, the function returns the adjusted probabilities as a NumPy array.",
    "prompt": "You need to implement a function named `_subtract_confident_thresholds` that adjusts predicted probabilities by subtracting class-specific confidence thresholds and then re-normalizing the probabilities. This adjustment aims to handle class imbalance in classification tasks. The function accepts labels, predicted probabilities, an optional flag for multi-label settings, and pre-calculated confidence thresholds. If confidence thresholds are not provided, they will be calculated from the labels and predicted probabilities using the `get_confident_thresholds` method. After subtracting the thresholds, the function ensures no negative values by shifting and then re-normalizing the probabilities. The function returns the adjusted predicted probabilities as a NumPy array. If neither labels nor pre-calculated thresholds are provided, a `ValueError` is raised.",
    "function_name": "_subtract_confident_thresholds",
    "ground_truth": "cleanlab/internal/label_quality_utils.py#L26",
    "test": "tests/test_rank.py",
    "class": "",
    "stage": "Inference",
    "task": "",
    "data": "Table"
}