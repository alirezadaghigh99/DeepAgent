{
    "result": "To create the `compute_confident_joint` function, we will follow a step-by-step approach similar to the examples provided. This function will estimate the confident counts of latent true vs. observed noisy labels for examples in a dataset. Here's how you can implement it:\n\n```python\nimport numpy as np\n\ndef compute_confident_joint(\n    labels,\n    pred_probs,\n    thresholds=None,\n    calibrate=False,\n    multi_label=False,\n    return_indices_of_off_diagonals=False\n):\n    \"\"\"\n    Estimate the confident counts of latent true vs observed noisy labels.\n\n    Args:\n        labels (array-like): Class labels for each example in the dataset.\n        pred_probs (array-like): Model-predicted class probabilities for each example.\n        thresholds (array-like, optional): Per-class threshold probabilities.\n        calibrate (bool): Whether to calibrate the confident joint estimate.\n        multi_label (bool, optional): If the dataset is multi-label classification.\n        return_indices_of_off_diagonals (bool, optional): Whether to return indices of examples counted in off-diagonals.\n\n    Returns:\n        np.ndarray: Array representing counts of examples for which we are confident about their given and true label.\n    \"\"\"\n    # 1. Convert inputs to numpy arrays for consistency\n    labels = np.asarray(labels)\n    pred_probs = np.asarray(pred_probs)\n\n    # 2. Determine the number of classes\n    num_classes = pred_probs.shape[1]\n\n    # 3. Initialize the confident joint matrix\n    if multi_label:\n        confident_joint = np.zeros((num_classes, 2), dtype=int)\n    else:\n        confident_joint = np.zeros((num_classes, num_classes), dtype=int)\n\n    # 4. Set default thresholds if none are provided\n    if thresholds is None:\n        thresholds = np.full(num_classes, 0.5)\n\n    # 5. Optionally calibrate the predicted probabilities\n    if calibrate:\n        # Placeholder for calibration logic\n        # This could involve scaling or adjusting pred_probs\n        pass\n\n    # 6. Determine confident predictions based on thresholds\n    confident_preds = pred_probs >= thresholds\n\n    # 7. Iterate over each example to populate the confident joint\n    for i, (label, pred_prob) in enumerate(zip(labels, pred_probs)):\n        if multi_label:\n            # For multi-label, consider each class independently\n            for class_idx in range(num_classes):\n                if confident_preds[i, class_idx]:\n                    confident_joint[class_idx, int(label[class_idx])] += 1\n        else:\n            # For single-label, find the predicted class\n            pred_class = np.argmax(pred_prob)\n            if confident_preds[i, pred_class]:\n                confident_joint[label, pred_class] += 1\n\n    # 8. Optionally return indices of off-diagonal elements\n    if return_indices_of_off_diagonals:\n        off_diagonal_indices = np.where(confident_joint != np.diag(np.diag(confident_joint)))\n        return confident_joint, off_diagonal_indices\n\n    # 9. Return the confident joint matrix\n    return confident_joint\n```\n\n### Explanation:\n\n1. **Input Conversion**: Convert `labels` and `pred_probs` to numpy arrays for consistent handling.\n2. **Class Count**: Determine the number of classes from the shape of `pred_probs`.\n3. **Confident Joint Initialization**: Initialize the confident joint matrix. Its shape depends on whether the task is multi-label.\n4. **Thresholds**: Set default thresholds if none are provided.\n5. **Calibration**: Placeholder for calibration logic if needed.\n6. **Confident Predictions**: Determine which predictions are confident based on the thresholds.\n7. **Populate Confident Joint**: Iterate over each example to update the confident joint matrix.\n8. **Off-Diagonal Indices**: Optionally return indices of off-diagonal elements if requested.\n9. **Return**: Return the confident joint matrix.\n\nThis function provides a vectorized and efficient way to compute the confident joint, accommodating both single-label and multi-label classification tasks.",
    "prompt": "Generate a Python function called compute_confident_joint that estimates the confident counts of latent true vs observed noisy labels for the examples in a dataset. The function takes in the following parameters:\n\n- labels: An array or list of class labels for each example in the dataset.\n- pred_probs: An array of model-predicted class probabilities for each example in the dataset.\n- thresholds: An optional array of per-class threshold probabilities.\n- calibrate: A boolean flag indicating whether to calibrate the confident joint estimate.\n- multi_label: An optional boolean flag indicating if the dataset is multi-label classification.\n- return_indices_of_off_diagonals: An optional boolean flag indicating whether to return indices of examples counted in off-diagonals of the confident joint.\n\nThe function returns an array representing counts of examples for which we are confident about their given and true label. If multi_label is True, the array has a different shape and represents a one-vs-rest format for the confident joint.\n\nNote that the function provides a vectorized implementation of the confident joint calculation for efficiency.",
    "function_name": "compute_confident_joint",
    "ground_truth": "cleanlab/count.py",
    "test": "tests/test_filter_count.py",
    "class": "",
    "stage": "Model Construction",
    "task": "Classification",
    "data": ""
}