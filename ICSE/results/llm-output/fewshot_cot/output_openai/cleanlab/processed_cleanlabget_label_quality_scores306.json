{
    "result": "To create the `get_label_quality_scores` function, we will follow a structured approach similar to the examples provided. This function will compute label quality scores for a multi-class classification dataset based on the given method. Here's how you can implement it:\n\n```python\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef get_label_quality_scores(labels, pred_probs, method=\"self_confidence\", adjust_pred_probs=False):\n    \"\"\"\n    Compute label quality scores for a multi-class classification dataset.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A discrete vector of noisy labels, i.e., some labels may be erroneous.\n        Format: for a dataset with K classes, labels must be in 0, 1, ..., K-1.\n\n    pred_probs : np.ndarray\n        An array of shape (N, K) of model-predicted probabilities, P(label=k|x).\n\n    method : {\"self_confidence\", \"normalized_margin\", \"confidence_weighted_entropy\"}, default=\"self_confidence\"\n        Label quality scoring method.\n\n    adjust_pred_probs : bool, optional\n        Account for class imbalance in the label-quality scoring by adjusting predicted probabilities.\n\n    Returns\n    -------\n    label_quality_scores : np.ndarray\n        Contains one score (between 0 and 1) per example.\n        Lower scores indicate more likely mislabeled examples.\n    \"\"\"\n    # 1. Validate input dimensions\n    if labels.ndim != 1 or pred_probs.ndim != 2:\n        raise ValueError(\"Invalid input dimensions. 'labels' must be 1D and 'pred_probs' must be 2D.\")\n    \n    if labels.shape[0] != pred_probs.shape[0]:\n        raise ValueError(\"Mismatch between number of labels and number of predictions.\")\n\n    # 2. Initialize scores array\n    label_quality_scores = np.zeros(labels.shape[0])\n\n    # 3. Adjust predicted probabilities for class imbalance if required\n    if adjust_pred_probs:\n        class_counts = np.bincount(labels, minlength=pred_probs.shape[1])\n        class_probs = class_counts / class_counts.sum()\n        pred_probs = pred_probs / class_probs\n        pred_probs = pred_probs / pred_probs.sum(axis=1, keepdims=True)\n\n    # 4. Compute label quality scores based on the selected method\n    for i, (label, probs) in enumerate(zip(labels, pred_probs)):\n        if method == \"self_confidence\":\n            score = probs[label]\n        elif method == \"normalized_margin\":\n            max_other_prob = np.max(np.delete(probs, label))\n            score = probs[label] - max_other_prob\n        elif method == \"confidence_weighted_entropy\":\n            score = entropy(probs) / probs[label]\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n\n        # 5. Normalize score to be between 0 and 1\n        label_quality_scores[i] = np.clip(score, 0, 1)\n\n    # 6. Return the computed label quality scores\n    return label_quality_scores\n```\n\n### Explanation:\n\n1. **Input Validation**: We first check that the input dimensions are correct. `labels` should be a 1D array, and `pred_probs` should be a 2D array with the same number of rows as `labels`.\n\n2. **Score Initialization**: We initialize an array to store the label quality scores for each data point.\n\n3. **Adjust Predicted Probabilities**: If `adjust_pred_probs` is set to `True`, we adjust the predicted probabilities to account for class imbalance.\n\n4. **Score Calculation**: We compute the label quality score for each data point based on the selected method:\n   - **Self Confidence**: Uses the probability of the given label.\n   - **Normalized Margin**: Uses the difference between the probability of the given label and the highest probability of any other label.\n   - **Confidence Weighted Entropy**: Uses the entropy of the predicted probabilities weighted by the self-confidence.\n\n5. **Normalization**: We ensure that the scores are clipped between 0 and 1.\n\n6. **Return Scores**: Finally, we return the computed label quality scores.",
    "prompt": "Generate a Python function called get_label_quality_scores that Returns a label quality score for each datapoint.\n\n    This is a function to compute label quality scores for standard (multi-class) classification datasets,\n    where lower scores indicate labels less likely to be correct.\n\n    Score is between 0 and 1.\n\n    1 - clean label (given label is likely correct).\n    0 - dirty label (given label is likely incorrect).\n\n    Parameters\n    ----------\n    labels : np.ndarray\n      A discrete vector of noisy labels, i.e. some labels may be erroneous.\n      *Format requirements*: for dataset with K classes, labels must be in 0, 1, ..., K-1.\n      Note: multi-label classification is not supported by this method, each example must belong to a single class, e.g. format: ``labels = np.ndarray([1,0,2,1,1,0...])``.\n\n    pred_probs : np.ndarray, optional\n      An array of shape ``(N, K)`` of model-predicted probabilities,\n      ``P(label=k|x)``. Each row of this matrix corresponds\n      to an example `x` and contains the model-predicted probabilities that\n      `x` belongs to each possible class, for each of the K classes. The\n      columns must be ordered such that these probabilities correspond to\n      class 0, 1, ..., K-1.\n\n      **Note**: Returned label issues are most accurate when they are computed based on out-of-sample `pred_probs` from your model.\n      To obtain out-of-sample predicted probabilities for every datapoint in your dataset, you can use :ref:`cross-validation <pred_probs_cross_val>`.\n      This is encouraged to get better results.\n\n    method : {\"self_confidence\", \"normalized_margin\", \"confidence_weighted_entropy\"}, default=\"self_confidence\"\n      Label quality scoring method.\n\n      Letting ``k = labels[i]`` and ``P = pred_probs[i]`` denote the given label and predicted class-probabilities\n      for datapoint *i*, its score can either be:\n\n      - ``'normalized_margin'``: ``P[k] - max_{k' != k}[ P[k'] ]``\n      - ``'self_confidence'``: ``P[k]``\n      - ``'confidence_weighted_entropy'``: ``entropy(P) / self_confidence``\n\n      Note: the actual label quality scores returned by this method\n      may be transformed versions of the above, in order to ensure\n      their values lie between 0-1 with lower values indicating more likely mislabeled data.\n\n      Let ``C = {0, 1, ..., K-1}`` be the set of classes specified for our classification task.\n\n      The `normalized_margin` score works better for identifying class conditional label errors,\n      i.e. examples for which another label in ``C`` is appropriate but the given label is not.\n\n      The `self_confidence` score works better for identifying alternative label issues\n      corresponding to bad examples that are: not from any of the classes in ``C``,\n      well-described by 2 or more labels in ``C``,\n      or generally just out-of-distribution (i.e. anomalous outliers).\n\n    adjust_pred_probs : bool, optional\n      Account for class imbalance in the label-quality scoring by adjusting predicted probabilities\n      via subtraction of class confident thresholds and renormalization.\n      Set this to ``True`` if you prefer to account for class-imbalance.\n      See `Northcutt et al., 2021 <https://jair.org/index.php/jair/article/view/12125>`_.\n\n    Returns\n    -------\n    label_quality_scores : np.ndarray\n      Contains one score (between 0 and 1) per example.\n      Lower scores indicate more likely mislabeled examples.",
    "function_name": "get_label_quality_scores",
    "ground_truth": "cleanlab/rank.py",
    "test": "tests/test_object_detection.py",
    "class": "",
    "stage": "Inference",
    "task": "Classification",
    "data": ""
}