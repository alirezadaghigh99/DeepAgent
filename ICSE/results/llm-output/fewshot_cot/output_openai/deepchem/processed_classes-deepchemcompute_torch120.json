{
    "result": "To create a Python function called `_create_pytorch_loss` that calculates the categorical cross-entropy loss using PyTorch, we can follow a step-by-step approach similar to the examples provided. Here's how you can implement this function:\n\n```python\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Loss\n\nclass CategoricalCrossEntropy(Loss):\n    def _create_pytorch_loss(self, probabilities: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Calculate the categorical cross-entropy loss between output probabilities and labels.\n\n        Args:\n            probabilities (torch.Tensor): Tensor of shape (batch_size, classes) or (batch_size, tasks, classes)\n                                          containing the predicted probabilities.\n            labels (torch.Tensor): Tensor of shape (batch_size,) or (batch_size, tasks) containing the true labels.\n\n        Returns:\n            torch.Tensor: Tensor of loss values.\n        \"\"\"\n        # 1. Check if the input probabilities have the correct shape\n        if probabilities.dim() not in {2, 3}:\n            raise ValueError(\"Probabilities tensor must have 2 or 3 dimensions.\")\n\n        # 2. Check if the input labels have the correct shape\n        if labels.dim() not in {1, 2}:\n            raise ValueError(\"Labels tensor must have 1 or 2 dimensions.\")\n\n        # 3. If the probabilities tensor has 3 dimensions, reshape it to 2 dimensions\n        if probabilities.dim() == 3:\n            batch_size, tasks, classes = probabilities.shape\n            probabilities = probabilities.view(batch_size * tasks, classes)\n            labels = labels.view(batch_size * tasks)\n\n        # 4. Calculate the categorical cross-entropy loss using PyTorch's F.cross_entropy\n        loss = F.cross_entropy(probabilities, labels, reduction='none')\n\n        # 5. If the original probabilities tensor had 3 dimensions, reshape the loss back to (batch_size, tasks)\n        if probabilities.dim() == 3:\n            loss = loss.view(batch_size, tasks)\n\n        # 6. Return the calculated loss tensor\n        return loss\n```\n\n### Explanation:\n\n1. **Input Validation**: We first check if the input tensors have the expected dimensions. The `probabilities` tensor should have either 2 or 3 dimensions, and the `labels` tensor should have 1 or 2 dimensions.\n\n2. **Reshape for 3D Input**: If the `probabilities` tensor has 3 dimensions (indicating multiple tasks), we reshape it to a 2D tensor to use PyTorch's `F.cross_entropy` function, which expects 2D input for probabilities.\n\n3. **Calculate Loss**: We use `F.cross_entropy` to compute the categorical cross-entropy loss. The `reduction='none'` argument ensures that the loss is calculated for each sample individually, without reducing it to a single scalar.\n\n4. **Reshape Loss for 3D Input**: If the original `probabilities` tensor was 3D, we reshape the loss tensor back to match the `(batch_size, tasks)` shape.\n\n5. **Return Loss**: Finally, we return the calculated loss tensor.",
    "prompt": "Generate a Python function called `_create_pytorch_loss` that calculates the categorical cross entropy loss between output probabilities and labels using PyTorch. The inputs are tensors of probabilities and labels with shapes `(batch_size, classes)` or `(batch_size, tasks, classes)`, and the output is a tensor of loss values.\n\n```python\nclass CategoricalCrossEntropy(Loss):\n    def _create_pytorch_loss(self):\n```",
    "function_name": "_create_pytorch_loss",
    "ground_truth": "deepchem/models/losses.py",
    "test": "deepchem/models/tests/test_losses.py::TestLosses",
    "class": "CategoricalCrossEntropy",
    "stage": "",
    "task": "Classification",
    "data": "Evaluation/metrics"
}