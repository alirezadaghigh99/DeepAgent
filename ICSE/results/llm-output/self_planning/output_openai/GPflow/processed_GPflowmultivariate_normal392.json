{
    "result": "To implement the `multivariate_normal` function as described, we need to follow the steps outlined in your plan. Here's the complete implementation:\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\ndef check_shapes(*args):\n    # This is a placeholder for the actual shape-checking decorator.\n    # In practice, you would use a library or custom implementation to enforce shape constraints.\n    def decorator(func):\n        def wrapper(*func_args, **func_kwargs):\n            # Here you would check the shapes of func_args and func_kwargs\n            return func(*func_args, **func_kwargs)\n        return wrapper\n    return decorator\n\n@check_shapes(\n    \"x: [D, broadcast N]\",\n    \"mu: [D, broadcast N]\",\n    \"L: [D, D]\",\n    \"return: [N]\",\n)\ndef multivariate_normal(x: tf.Tensor, mu: tf.Tensor, L: tf.Tensor) -> tf.Tensor:\n    \"\"\"\n    Computes the log-density of a multivariate normal.\n\n    :param x: sample(s) for which we want the density\n    :param mu: mean(s) of the normal distribution\n    :param L: Cholesky decomposition of the covariance matrix\n    :return: log densities\n    \"\"\"\n    # Compute the difference between x and mu\n    diff = x - mu\n    \n    # Solve for the standardized variable z\n    z = tf.linalg.triangular_solve(L, diff, lower=True)\n    \n    # Compute the log-determinant of the covariance matrix\n    log_det_cov = 2.0 * tf.reduce_sum(tf.math.log(tf.linalg.diag_part(L)))\n    \n    # Compute the log-density\n    D = tf.cast(tf.shape(x)[0], tf.float32)  # Dimension of the data\n    log_density = -0.5 * (tf.reduce_sum(tf.square(z), axis=0) + log_det_cov + D * tf.math.log(2.0 * np.pi))\n    \n    return log_density\n```\n\n### Explanation of the Implementation:\n\n1. **Difference Calculation**: We calculate `diff = x - mu` to find the difference between the sample and the mean.\n\n2. **Standardized Variable**: We solve the equation `Lz = diff` using `tf.linalg.triangular_solve`, which is efficient for triangular matrices like `L` (the Cholesky factor).\n\n3. **Log-Determinant**: The log-determinant of the covariance matrix is calculated as `2 * sum(log(diag(L)))`, which is a property of the Cholesky decomposition.\n\n4. **Log-Density**: The log-density is computed using the formula for the multivariate normal distribution. It includes:\n   - The quadratic form `z^T z` (computed as `tf.reduce_sum(tf.square(z), axis=0)`).\n   - The log-determinant of the covariance matrix.\n   - A normalization constant involving the dimension `D` and `2\u03c0`.\n\n5. **Shape Checking**: The `@check_shapes` decorator is a placeholder here. In practice, you would use a library or custom implementation to enforce shape constraints.\n\nThis function assumes that the input tensors are compatible with the specified shapes and that the Cholesky decomposition `L` is valid. The function returns the log densities of the samples `x` under the multivariate normal distribution defined by `mu` and `L`.",
    "prompt": "\"Write the multivariate_normal function as follow :\n@check_shapes(\n    \"x: [D, broadcast N]\",\n    \"mu: [D, broadcast N]\",\n    \"L: [D, D]\",\n    \"return: [N]\",\n)\ndef multivariate_normal(x: TensorType, mu: TensorType, L: TensorType) -> tf.Tensor:\n    \"\"\"\n    Computes the log-density of a multivariate normal.\n\n    :param x: sample(s) for which we want the density\n    :param mu: mean(s) of the normal distribution\n    :param L: Cholesky decomposition of the covariance matrix\n    :return: log densities\n    \"\"\"",
    "function_name": "multivariate_normal",
    "ground_truth": "gpflow/logdensities.py",
    "test": "tests/gpflow/test_logdensities.py::test_multivariate_normal",
    "class": "",
    "stage": "Model Construction",
    "task": "Prediction",
    "data": "tabular"
}