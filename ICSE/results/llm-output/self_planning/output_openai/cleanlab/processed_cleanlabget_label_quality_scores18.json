{
    "result": "To implement the `get_label_quality_scores` function as described, we need to follow the structured approach outlined in the plan. Here's the complete implementation:\n\n```python\nimport numpy as np\n\ndef get_label_quality_scores(labels, predictions, method=\"outre\"):\n    # Step 1: Validate Inputs\n    if not isinstance(labels, (list, np.ndarray)) or not isinstance(predictions, (list, np.ndarray)):\n        raise ValueError(\"Labels and predictions must be array-like.\")\n    \n    labels = np.array(labels)\n    predictions = np.array(predictions)\n    \n    if labels.shape != predictions.shape:\n        raise ValueError(\"Labels and predictions must have the same shape.\")\n    \n    if not isinstance(method, str):\n        raise ValueError(\"Method must be a string.\")\n    \n    # Step 2: Initialize Scoring Method\n    if method != \"outre\":\n        raise ValueError(f\"Unsupported method: {method}. Only 'outre' is supported.\")\n    \n    # Step 3: Calculate Label Quality Scores\n    # Example scoring logic for \"outre\" method\n    # Here, we assume a simple inverse error calculation for demonstration\n    errors = np.abs(labels - predictions)\n    max_error = np.max(errors) if np.max(errors) != 0 else 1  # Avoid division by zero\n    scores = 1 - (errors / max_error)\n    \n    # Step 4: Return Scores\n    return scores\n\n# Example usage\nlabels = np.array([1, 2, 3, 4])\npredictions = np.array([2, 2, 5, 4.1])\n\nlabel_quality_scores = get_label_quality_scores(labels, predictions)\nprint(label_quality_scores)\n```\n\n### Explanation of the Implementation\n\n1. **Input Validation:**\n   - We check if `labels` and `predictions` are either lists or numpy arrays. This ensures that they are array-like.\n   - We convert both inputs to numpy arrays for consistent operations.\n   - We ensure that both arrays have the same shape to prevent mismatches during calculations.\n\n2. **Method Validation:**\n   - We check if the `method` is a string and default it to \"outre\".\n   - We raise an error if any method other than \"outre\" is specified, as currently, only \"outre\" is supported.\n\n3. **Scoring Calculation:**\n   - We calculate the absolute errors between `labels` and `predictions`.\n   - We determine the maximum error to normalize the scores. This prevents division by zero by setting a default max error of 1 if the maximum error is zero.\n   - We compute the scores as `1 - (errors / max_error)`, which normalizes the errors to a range between 0 and 1, where 1 indicates no error (clean label) and 0 indicates maximum error (dirty label).\n\n4. **Return Scores:**\n   - The function returns an array of scores, with each score corresponding to the quality of the label for each example in the dataset.\n\nThis implementation provides a basic framework for calculating label quality scores using a simple inverse error method. The \"outre\" method here is a placeholder and can be replaced with a more sophisticated method if needed.",
    "prompt": "Create a Python function called get_label_quality_scores that calculates label quality scores for each example in a regression dataset. The function takes in two array-like inputs: labels (raw labels from the original dataset) and predictions (predicted labels for each example). Additionally, the function has an optional keyword argument method, which specifies the scoring method to use (default is \"outre\").\n\nThe function returns an array of label quality scores, where each score is a continuous value between 0 and 1. A score of 1 indicates a clean label (likely correct), while a score of 0 indicates a dirty label (likely incorrect).\n\nEnsure that the inputs are valid and then use the specified scoring method to calculate the label quality scores. The output is an array of scores with one score per example in the dataset.\n\nExample usage:\n```python\nimport numpy as np\nfrom cleanlab.regression.rank import get_label_quality_scores\n\nlabels = np.array([1, 2, 3, 4])\npredictions = np.array([2, 2, 5, 4.1])\n\nlabel_quality_scores = get_label_quality_scores(labels, predictions)\nprint(label_quality_scores)\n# Output: array([0.00323821, 0.33692597, 0.00191686, 0.33692597])\n```",
    "function_name": "get_label_quality_scores",
    "ground_truth": "cleanlab/regression/rank.py",
    "test": "tests/test_object_detection.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}