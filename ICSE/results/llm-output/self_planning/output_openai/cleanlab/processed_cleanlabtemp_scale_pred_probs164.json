{
    "result": "The function `temp_scale_pred_probs` is designed to adjust predicted probabilities using a technique called temperature scaling. This technique is useful for calibrating the confidence of predictions, especially in machine learning models. Let's walk through the implementation step-by-step:\n\n1. **Clipping Probabilities**: We start by clipping the predicted probabilities to avoid taking the logarithm of zero, which is undefined. We use a small positive value (`epsilon`) for this purpose.\n\n2. **Normalizing Probabilities**: We ensure that the probabilities for each set of predictions sum to 1. This is crucial because we are dealing with probability distributions.\n\n3. **Applying Logarithm**: We take the logarithm of the normalized probabilities. This transformation is necessary for the temperature scaling step, which operates in the log-probability space.\n\n4. **Temperature Scaling**: We adjust the log probabilities by dividing them by the temperature factor. A temperature less than 1 makes the predictions more confident (sharper), while a temperature greater than 1 makes them less confident (smoother).\n\n5. **Applying Softmax**: We convert the scaled log probabilities back to probabilities using the softmax function. This step ensures that the output is a valid probability distribution.\n\n6. **Returning Scaled Probabilities**: Finally, we return the adjusted probabilities as a 2D NumPy array.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef temp_scale_pred_probs(pred_probs, temp):\n    # Step 1: Clip probabilities to avoid log(0)\n    epsilon = 1e-10\n    pred_probs = np.clip(pred_probs, epsilon, 1.0)\n    \n    # Step 2: Normalize probabilities\n    pred_probs /= np.sum(pred_probs, axis=1, keepdims=True)\n    \n    # Step 3: Apply logarithm\n    log_probs = np.log(pred_probs)\n    \n    # Step 4: Temperature scaling\n    scaled_log_probs = log_probs / temp\n    \n    # Step 5: Apply softmax\n    exp_scaled_log_probs = np.exp(scaled_log_probs)\n    scaled_probs = exp_scaled_log_probs / np.sum(exp_scaled_log_probs, axis=1, keepdims=True)\n    \n    # Step 6: Return scaled probabilities\n    return scaled_probs\n```\n\n### Key Points:\n- **Clipping**: Ensures numerical stability by preventing log(0).\n- **Normalization**: Maintains the integrity of the probability distribution.\n- **Logarithm and Temperature Scaling**: Adjusts the confidence of predictions.\n- **Softmax**: Converts log probabilities back to a probability distribution.\n\nThis function is useful in scenarios where you need to calibrate the confidence of model predictions, such as in classification tasks where the model's output probabilities need to be adjusted for better interpretability or performance.",
    "prompt": "You need to implement a function named `temp_scale_pred_probs` that adjusts predicted probabilities using a temperature scaling technique. Temperature factor less than 1 sharpens the predictions, making them more confident, while a temperature greater than 1 smoothens the predictions, making them less confident. The function takes a 2D NumPy array of predicted probabilities (`pred_probs`) and a float (`temp`) representing the temperature factor. The probabilities are first clipped to avoid logarithm of zero and normalized. Then, the temperature scaling is applied using the softmax function after taking the logarithm of the probabilities. The function returns the scaled and normalized probabilities as a 2D NumPy array.",
    "function_name": "temp_scale_pred_probs",
    "ground_truth": "cleanlab/internal/multiannotator_utils.py#L353",
    "test": "tests/test_multiannotator.py",
    "class": "",
    "stage": "Pre-Post processing",
    "task": "Classification",
    "data": "Table"
}