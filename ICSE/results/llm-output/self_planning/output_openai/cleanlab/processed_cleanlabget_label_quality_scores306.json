{
    "result": "The function `get_label_quality_scores` is designed to compute label quality scores for multi-class classification datasets. Here's a detailed breakdown of the implementation:\n\n```python\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef get_label_quality_scores(labels, pred_probs, method=\"self_confidence\", adjust_pred_probs=False):\n    # Validate inputs\n    if not isinstance(labels, np.ndarray):\n        raise ValueError(\"Labels must be a numpy array.\")\n    if not isinstance(pred_probs, np.ndarray) or pred_probs.ndim != 2:\n        raise ValueError(\"pred_probs must be a 2D numpy array.\")\n    if method not in {\"self_confidence\", \"normalized_margin\", \"confidence_weighted_entropy\"}:\n        raise ValueError(\"Invalid method. Choose from 'self_confidence', 'normalized_margin', 'confidence_weighted_entropy'.\")\n    \n    # Number of examples\n    N = labels.shape[0]\n    \n    # Adjust predicted probabilities if required\n    if adjust_pred_probs:\n        class_counts = np.bincount(labels, minlength=pred_probs.shape[1])\n        class_probs = class_counts / class_counts.sum()\n        pred_probs = pred_probs - class_probs\n        pred_probs = np.clip(pred_probs, 0, None)\n        pred_probs /= pred_probs.sum(axis=1, keepdims=True)\n    \n    # Compute label quality scores\n    scores = np.zeros(N)\n    for i in range(N):\n        k = labels[i]\n        P = pred_probs[i]\n        \n        if method == \"self_confidence\":\n            scores[i] = P[k]\n        elif method == \"normalized_margin\":\n            max_other = np.max(np.delete(P, k))\n            scores[i] = P[k] - max_other\n        elif method == \"confidence_weighted_entropy\":\n            scores[i] = entropy(P) / P[k]\n    \n    # Normalize scores to be between 0 and 1\n    scores = np.clip(scores, 0, 1)\n    \n    return scores\n```\n\n### Key Components:\n\n1. **Input Validation:**\n   - Ensures `labels` is a numpy array.\n   - Ensures `pred_probs` is a 2D numpy array with shape `(N, K)`.\n   - Validates that `method` is one of the specified options.\n   - Checks that `adjust_pred_probs` is a boolean.\n\n2. **Adjust Predicted Probabilities:**\n   - If `adjust_pred_probs` is `True`, the function adjusts the predicted probabilities to account for class imbalance. This involves:\n     - Calculating class probabilities from the label distribution.\n     - Subtracting these class probabilities from the predicted probabilities.\n     - Clipping negative values to zero and renormalizing the probabilities.\n\n3. **Compute Label Quality Scores:**\n   - For each example, the function computes a score based on the specified method:\n     - **Self Confidence:** Uses the predicted probability of the given label.\n     - **Normalized Margin:** Computes the difference between the predicted probability of the given label and the highest predicted probability of any other label.\n     - **Confidence Weighted Entropy:** Computes the entropy of the predicted probabilities and divides by the self-confidence score.\n\n4. **Normalize Scores:**\n   - Ensures that the scores are clipped to be between 0 and 1, where lower scores indicate more likely mislabeled data.\n\n5. **Return Scores:**\n   - Returns a numpy array containing the label quality scores for each example.\n\nThis function is useful for identifying potentially mislabeled data in classification datasets, allowing for improved data quality and model performance.",
    "prompt": "Generate a Python function called get_label_quality_scores that Returns a label quality score for each datapoint.\n\n    This is a function to compute label quality scores for standard (multi-class) classification datasets,\n    where lower scores indicate labels less likely to be correct.\n\n    Score is between 0 and 1.\n\n    1 - clean label (given label is likely correct).\n    0 - dirty label (given label is likely incorrect).\n\n    Parameters\n    ----------\n    labels : np.ndarray\n      A discrete vector of noisy labels, i.e. some labels may be erroneous.\n      *Format requirements*: for dataset with K classes, labels must be in 0, 1, ..., K-1.\n      Note: multi-label classification is not supported by this method, each example must belong to a single class, e.g. format: ``labels = np.ndarray([1,0,2,1,1,0...])``.\n\n    pred_probs : np.ndarray, optional\n      An array of shape ``(N, K)`` of model-predicted probabilities,\n      ``P(label=k|x)``. Each row of this matrix corresponds\n      to an example `x` and contains the model-predicted probabilities that\n      `x` belongs to each possible class, for each of the K classes. The\n      columns must be ordered such that these probabilities correspond to\n      class 0, 1, ..., K-1.\n\n      **Note**: Returned label issues are most accurate when they are computed based on out-of-sample `pred_probs` from your model.\n      To obtain out-of-sample predicted probabilities for every datapoint in your dataset, you can use :ref:`cross-validation <pred_probs_cross_val>`.\n      This is encouraged to get better results.\n\n    method : {\"self_confidence\", \"normalized_margin\", \"confidence_weighted_entropy\"}, default=\"self_confidence\"\n      Label quality scoring method.\n\n      Letting ``k = labels[i]`` and ``P = pred_probs[i]`` denote the given label and predicted class-probabilities\n      for datapoint *i*, its score can either be:\n\n      - ``'normalized_margin'``: ``P[k] - max_{k' != k}[ P[k'] ]``\n      - ``'self_confidence'``: ``P[k]``\n      - ``'confidence_weighted_entropy'``: ``entropy(P) / self_confidence``\n\n      Note: the actual label quality scores returned by this method\n      may be transformed versions of the above, in order to ensure\n      their values lie between 0-1 with lower values indicating more likely mislabeled data.\n\n      Let ``C = {0, 1, ..., K-1}`` be the set of classes specified for our classification task.\n\n      The `normalized_margin` score works better for identifying class conditional label errors,\n      i.e. examples for which another label in ``C`` is appropriate but the given label is not.\n\n      The `self_confidence` score works better for identifying alternative label issues\n      corresponding to bad examples that are: not from any of the classes in ``C``,\n      well-described by 2 or more labels in ``C``,\n      or generally just out-of-distribution (i.e. anomalous outliers).\n\n    adjust_pred_probs : bool, optional\n      Account for class imbalance in the label-quality scoring by adjusting predicted probabilities\n      via subtraction of class confident thresholds and renormalization.\n      Set this to ``True`` if you prefer to account for class-imbalance.\n      See `Northcutt et al., 2021 <https://jair.org/index.php/jair/article/view/12125>`_.\n\n    Returns\n    -------\n    label_quality_scores : np.ndarray\n      Contains one score (between 0 and 1) per example.\n      Lower scores indicate more likely mislabeled examples.",
    "function_name": "get_label_quality_scores",
    "ground_truth": "cleanlab/rank.py",
    "test": "tests/test_object_detection.py",
    "class": "",
    "stage": "Inference",
    "task": "Classification",
    "data": ""
}