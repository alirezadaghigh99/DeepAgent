{
    "result": "To implement the `_subtract_confident_thresholds` function, we need to follow the structured approach outlined in your request. This function will adjust predicted probabilities by subtracting class-specific confidence thresholds, handling class imbalance in classification tasks. Let's go through the implementation step-by-step:\n\n1. **Parse and Validate Inputs:**\n   - Ensure that either `labels` or `confidence_thresholds` is provided. If neither is provided, raise a `ValueError`.\n\n2. **Calculate Confidence Thresholds (if not provided):**\n   - If `confidence_thresholds` is not provided, calculate them using a placeholder `get_confident_thresholds` method. This method should determine appropriate thresholds based on the distribution of `labels` and `predicted_probs`.\n\n3. **Adjust Predicted Probabilities:**\n   - Subtract the class-specific confidence thresholds from the predicted probabilities.\n   - Ensure no negative values exist in the adjusted probabilities by setting any negative values to zero.\n\n4. **Re-normalize Probabilities:**\n   - Re-normalize the probabilities so that they sum to 1 for each instance.\n\n5. **Return Adjusted Probabilities:**\n   - Return the adjusted and re-normalized predicted probabilities as a NumPy array.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef _subtract_confident_thresholds(labels, predicted_probs, multi_label=False, confidence_thresholds=None):\n    # Validate inputs\n    if confidence_thresholds is None and labels is None:\n        raise ValueError(\"Either labels or pre-calculated confidence thresholds must be provided.\")\n    \n    # Calculate confidence thresholds if not provided\n    if confidence_thresholds is None:\n        confidence_thresholds = get_confident_thresholds(labels, predicted_probs, multi_label)\n    \n    # Subtract thresholds from predicted probabilities\n    adjusted_probs = predicted_probs - confidence_thresholds\n    \n    # Ensure no negative values by setting negatives to zero\n    adjusted_probs = np.maximum(adjusted_probs, 0)\n    \n    # Re-normalize probabilities\n    if multi_label:\n        # For multi-label, normalize each instance independently\n        row_sums = adjusted_probs.sum(axis=1, keepdims=True)\n        adjusted_probs = np.divide(adjusted_probs, row_sums, where=row_sums != 0)\n    else:\n        # For single-label, normalize across classes\n        adjusted_probs /= adjusted_probs.sum(axis=1, keepdims=True)\n    \n    return adjusted_probs\n\ndef get_confident_thresholds(labels, predicted_probs, multi_label):\n    # Placeholder for the actual implementation of threshold calculation\n    # This function should return a NumPy array of thresholds for each class\n    # based on the provided labels and predicted probabilities.\n    # For demonstration, let's assume a simple heuristic: mean of predicted probabilities\n    if multi_label:\n        # Calculate thresholds for each class independently\n        thresholds = np.mean(predicted_probs, axis=0)\n    else:\n        # Calculate thresholds for each class\n        thresholds = np.mean(predicted_probs, axis=0)\n    \n    return thresholds\n```\n\n### Explanation:\n- **Validation:** The function checks if either `labels` or `confidence_thresholds` is provided. If not, it raises a `ValueError`.\n- **Threshold Calculation:** If `confidence_thresholds` are not provided, the function calculates them using a placeholder `get_confident_thresholds` function. This function currently uses a simple heuristic (mean of predicted probabilities) for demonstration purposes.\n- **Adjustment and Normalization:** The function subtracts the thresholds from the predicted probabilities, ensures non-negativity, and then re-normalizes the probabilities to maintain their probabilistic interpretation.\n\nThis implementation is flexible for both single-label and multi-label classification tasks. You can further refine the `get_confident_thresholds` function to suit your specific needs for threshold calculation.",
    "prompt": "You need to implement a function named `_subtract_confident_thresholds` that adjusts predicted probabilities by subtracting class-specific confidence thresholds and then re-normalizing the probabilities. This adjustment aims to handle class imbalance in classification tasks. The function accepts labels, predicted probabilities, an optional flag for multi-label settings, and pre-calculated confidence thresholds. If confidence thresholds are not provided, they will be calculated from the labels and predicted probabilities using the `get_confident_thresholds` method. After subtracting the thresholds, the function ensures no negative values by shifting and then re-normalizing the probabilities. The function returns the adjusted predicted probabilities as a NumPy array. If neither labels nor pre-calculated thresholds are provided, a `ValueError` is raised.",
    "function_name": "_subtract_confident_thresholds",
    "ground_truth": "cleanlab/internal/label_quality_utils.py#L26",
    "test": "tests/test_rank.py",
    "class": "",
    "stage": "Inference",
    "task": "",
    "data": "Table"
}