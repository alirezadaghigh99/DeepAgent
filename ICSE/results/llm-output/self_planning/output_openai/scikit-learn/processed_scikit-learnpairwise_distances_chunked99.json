{
    "result": "To implement the `pairwise_distances_chunked` function, we'll follow the outlined steps to ensure it handles large datasets efficiently by processing them in chunks. We'll use `scikit-learn`'s `pairwise_distances` for distance computation and `joblib` for parallel processing. Here's the complete implementation:\n\n```python\nimport numpy as np\nfrom sklearn.metrics import pairwise_distances\nfrom joblib import Parallel, delayed\n\ndef pairwise_distances_chunked(X, Y=None, reduce_func=None, metric='euclidean', n_jobs=1, working_memory=1024, **kwargs):\n    \"\"\"\n    Generate a distance matrix chunk by chunk with optional reduction.\n\n    Parameters:\n    - X: array-like, shape (n_samples_X, n_features)\n    - Y: array-like, shape (n_samples_Y, n_features), optional\n    - reduce_func: callable, optional\n    - metric: string or callable, default='euclidean'\n    - n_jobs: int, default=1\n    - working_memory: int, default=1024 (in MB)\n    - **kwargs: additional keyword arguments for the distance metric\n\n    Yields:\n    - Contiguous slices of the distance matrix, optionally processed by reduce_func.\n    \"\"\"\n    if Y is None:\n        Y = X\n\n    n_samples_X = X.shape[0]\n    n_samples_Y = Y.shape[0]\n\n    # Estimate the number of rows we can process in one chunk\n    bytes_per_distance = np.dtype('float64').itemsize\n    max_bytes = working_memory * (1024 ** 2)  # Convert MB to bytes\n    max_distances = max_bytes // bytes_per_distance\n    chunk_size = max(1, max_distances // n_samples_Y)\n\n    # Iterate over chunks\n    for start in range(0, n_samples_X, chunk_size):\n        end = min(start + chunk_size, n_samples_X)\n        X_chunk = X[start:end]\n\n        # Compute pairwise distances for the current chunk\n        distances = pairwise_distances(X_chunk, Y, metric=metric, n_jobs=n_jobs, **kwargs)\n\n        # Apply reduction function if provided\n        if reduce_func is not None:\n            yield reduce_func(distances)\n        else:\n            yield distances\n\n# Usage examples\n\n# Example 1: Without reduce_func\nX = np.random.rand(1000, 5)\nY = np.random.rand(1000, 5)\n\nfor chunk in pairwise_distances_chunked(X, Y, working_memory=1):\n    print(\"Chunk shape:\", chunk.shape)\n\n# Example 2: With reduce_func to find neighbors within a radius\ndef find_neighbors_within_radius(distances, radius=0.5):\n    return [np.where(row < radius)[0] for row in distances]\n\nfor neighbors in pairwise_distances_chunked(X, Y, reduce_func=lambda d: find_neighbors_within_radius(d, radius=0.5), working_memory=1):\n    print(\"Neighbors in chunk:\", neighbors)\n\n# Example 3: With reduce_func to calculate average distance\ndef average_distance(distances):\n    return np.mean(distances, axis=1)\n\nfor avg_dist in pairwise_distances_chunked(X, Y, reduce_func=average_distance, working_memory=1):\n    print(\"Average distances in chunk:\", avg_dist)\n```\n\n### Explanation:\n\n1. **Input Validation and Setup:**\n   - The function accepts `X`, `Y`, and other parameters. If `Y` is not provided, it defaults to `X`.\n   - The chunk size is determined based on the `working_memory` parameter, ensuring that the memory usage stays within limits.\n\n2. **Chunk Processing:**\n   - The function iterates over `X` in chunks, computing pairwise distances for each chunk using `pairwise_distances`.\n   - If a `reduce_func` is provided, it processes the distances with this function before yielding.\n\n3. **Usage Examples:**\n   - The first example shows how to use the function without a reduction function, yielding raw distance matrix slices.\n   - The second example demonstrates using a reduction function to find neighbors within a specified radius.\n   - The third example calculates the average distance for each chunk.\n\nThis implementation efficiently handles large datasets by processing them in chunks, making it suitable for scenarios where memory is a constraint.",
    "prompt": "Generate a Python function called pairwise_distances_chunked that generates a distance matrix chunk by chunk with optional reduction. The function takes in parameters X, Y, reduce_func, metric, n_jobs, working_memory, and additional keyword parameters. It calculates pairwise distances in working_memory-sized chunks and applies reduce_func on each chunk if provided. The function yields a contiguous slice of the distance matrix, optionally processed by reduce_func. The function also includes examples of usage without reduce_func and with reduce_func for retrieving neighbors and average distance within a radius.",
    "function_name": "pairwise_distances_chunked",
    "ground_truth": "sklearn/metrics/pairwise.py",
    "test": "sklearn/metrics/tests/test_pairwise.py::test_pairwise_distances_chunked_reduce_valid",
    "class": "",
    "stage": "Model Construction",
    "task": "",
    "data": "Table"
}