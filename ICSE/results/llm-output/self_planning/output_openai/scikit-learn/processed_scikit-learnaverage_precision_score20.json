{
    "result": "Certainly! Let's focus on implementing the `average_precision_score` function in Python, which computes the average precision (AP) from prediction scores. This function will handle binary, multilabel-indicator, and multiclass inputs, and it will include a helper function for binary classification.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\nfrom sklearn.metrics import precision_recall_curve\n\ndef _binary_uninterpolated_average_precision(y_true, y_score):\n    \"\"\"Calculate the average precision for binary classification.\"\"\"\n    precision, recall, _ = precision_recall_curve(y_true, y_score)\n    # Calculate the area under the precision-recall curve\n    return np.sum((recall[1:] - recall[:-1]) * precision[1:])\n\ndef average_precision_score(y_true, y_score, average='macro', pos_label=1, sample_weight=None):\n    \"\"\"Compute the average precision score.\"\"\"\n    y_true = np.asarray(y_true)\n    y_score = np.asarray(y_score)\n\n    if y_true.ndim == 1:\n        # Binary classification case\n        return _binary_uninterpolated_average_precision(y_true, y_score)\n    else:\n        # Multiclass or multilabel case\n        n_classes = y_true.shape[1]\n        ap_scores = []\n\n        for i in range(n_classes):\n            ap = _binary_uninterpolated_average_precision(y_true[:, i], y_score[:, i])\n            ap_scores.append(ap)\n\n        if average == 'micro':\n            # Micro-average: Calculate metrics globally by considering each element of the label indicator matrix as a label.\n            y_true = y_true.ravel()\n            y_score = y_score.ravel()\n            return _binary_uninterpolated_average_precision(y_true, y_score)\n        elif average == 'macro':\n            # Macro-average: Calculate metrics for each label, and find their unweighted mean.\n            return np.mean(ap_scores)\n        elif average == 'weighted':\n            # Weighted-average: Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label).\n            support = np.sum(y_true, axis=0)\n            return np.average(ap_scores, weights=support)\n        elif average == 'samples':\n            # Samples-average: Calculate metrics for each instance, and find their average.\n            sample_ap = [_binary_uninterpolated_average_precision(y_true[i], y_score[i]) for i in range(y_true.shape[0])]\n            return np.mean(sample_ap)\n        else:\n            raise ValueError(f\"Unknown average type: {average}\")\n\n# Example usage\ny_true = np.array([0, 1, 1, 0])\ny_score = np.array([0.1, 0.4, 0.35, 0.8])\nprint(\"Average Precision Score (binary):\", average_precision_score(y_true, y_score))\n\ny_true_multiclass = np.array([[0, 1], [1, 0], [1, 1], [0, 0]])\ny_score_multiclass = np.array([[0.1, 0.9], [0.8, 0.2], [0.7, 0.6], [0.4, 0.5]])\nprint(\"Average Precision Score (multiclass, macro):\", average_precision_score(y_true_multiclass, y_score_multiclass, average='macro'))\n```\n\n### Explanation:\n\n1. **Binary Classification**: The `_binary_uninterpolated_average_precision` function calculates the average precision for binary classification using the precision-recall curve.\n\n2. **Multiclass/Multilabel Classification**: The `average_precision_score` function handles multiclass or multilabel data by iterating over each class and computing the average precision for each. It supports different averaging methods:\n   - **Micro**: Computes metrics globally.\n   - **Macro**: Computes metrics for each class and averages them.\n   - **Weighted**: Averages metrics weighted by the number of true instances for each class.\n   - **Samples**: Computes metrics for each instance and averages them.\n\n3. **Example Usage**: The function is demonstrated with both binary and multiclass data.\n\nThis implementation provides a comprehensive approach to calculating the average precision score for various types of classification problems.",
    "prompt": "Generate a Python function called average_precision_score that computes the average precision (AP) from prediction scores. The function takes in the following parameters:\n\n- y_true: array-like of shape (n_samples,) or (n_samples, n_classes) representing true binary labels or binary label indicators.\n- y_score: array-like of shape (n_samples,) or (n_samples, n_classes) representing target scores.\n- average: {'micro', 'samples', 'weighted', 'macro'} or None, default='macro' determining the type of averaging performed on the data.\n- pos_label: int, float, bool, or str, default=1 representing the label of the positive class.\n- sample_weight: array-like of shape (n_samples,), default=None representing sample weights.\n\nThe function returns the average precision score as a float value. It also includes a helper function called _binary_uninterpolated_average_precision to calculate the average precision for binary classification. The function handles different types of input data such as binary, multilabel-indicator, and multiclass.\n\nAdditionally, the function provides examples of how to use it with input arrays for y_true and y_score.",
    "function_name": "average_precision_score",
    "ground_truth": "sklearn/metrics/_ranking.py",
    "test": "sklearn/metrics/tests/test_ranking.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}