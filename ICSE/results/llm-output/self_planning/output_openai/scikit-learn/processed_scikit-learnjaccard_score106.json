{
    "result": "To implement the `jaccard_score` function as described, we can leverage the `scikit-learn` library, which provides robust utilities for handling various input formats and computing the Jaccard similarity coefficient. Here's a detailed implementation:\n\n```python\nimport numpy as np\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import jaccard_score as sklearn_jaccard_score\n\ndef jaccard_score(y_true, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn'):\n    # Validate inputs\n    y_true = check_array(y_true, ensure_2d=False, dtype=None)\n    y_pred = check_array(y_pred, ensure_2d=False, dtype=None)\n    \n    # Binarize the labels if necessary\n    lb = LabelBinarizer()\n    if labels is not None:\n        lb.fit(labels)\n    else:\n        lb.fit(np.union1d(y_true, y_pred))\n    \n    y_true_bin = lb.transform(y_true)\n    y_pred_bin = lb.transform(y_pred)\n    \n    # Calculate Jaccard score using sklearn's implementation\n    score = sklearn_jaccard_score(y_true_bin, y_pred_bin, average=average, sample_weight=sample_weight, zero_division=zero_division)\n    \n    return score\n\n# Example usage\ny_true = [0, 1, 2, 3]\ny_pred = [0, 2, 2, 3]\nprint(jaccard_score(y_true, y_pred, average='macro'))\n```\n\n### Explanation\n\n1. **Input Validation:**\n   - We use `check_array` from `sklearn.utils` to ensure that `y_true` and `y_pred` are in a compatible format. This function checks the input arrays and ensures they are not two-dimensional, which is suitable for label arrays.\n\n2. **Label Binarization:**\n   - We use `LabelBinarizer` to convert the labels into a binary format. This is necessary for computing the Jaccard index, especially in multiclass or multilabel scenarios.\n   - If `labels` are provided, we fit the `LabelBinarizer` with these labels. Otherwise, we fit it with the union of unique labels from `y_true` and `y_pred`.\n\n3. **Jaccard Calculation:**\n   - We use `sklearn.metrics.jaccard_score` to compute the Jaccard similarity coefficient. This function handles different averaging methods (`binary`, `micro`, `macro`, `weighted`, `samples`) and respects the `zero_division` parameter to manage division by zero scenarios.\n\n4. **Averaging and Zero Division:**\n   - The `average` parameter determines how the scores are averaged across labels. The `zero_division` parameter specifies the behavior when a division by zero occurs, either returning 0, 1, or raising a warning.\n\nThis implementation is efficient and leverages `scikit-learn`'s robust handling of various input formats and averaging methods, making it suitable for a wide range of classification tasks.",
    "prompt": "Generate a Python function called jaccard_score that calculate Jaccard similarity coefficient score.\n\n    The Jaccard index or Jaccard similarity coefficient, defined as\n    the size of the intersection divided by the size of the union of two label\n    sets, is used to compare set of predicted labels for a sample to the\n    corresponding set of labels in ``y_true``.\n    Parameters\n    ----------\n    y_true : 1d array-like, or label indicator array / sparse matrix\n        Ground truth (correct) labels.\n\n    y_pred : 1d array-like, or label indicator array / sparse matrix\n        Predicted labels, as returned by a classifier.\n\n    labels : array-like of shape (n_classes,), default=None\n        The set of labels to include when `average != 'binary'`, and their\n        order if `average is None`. Labels present in the data can be\n        excluded, for example in multiclass classification to exclude a \"negative\n        class\". Labels not present in the data can be included and will be\n        \"assigned\" 0 samples. For multilabel targets, labels are column indices.\n        By default, all labels in `y_true` and `y_pred` are used in sorted order.\n\n    pos_label : int, float, bool or str, default=1\n        The class to report if `average='binary'` and the data is binary,\n        otherwise this parameter is ignored.\n        For multiclass or multilabel targets, set `labels=[pos_label]` and\n        `average != 'binary'` to report metrics for one label only.\n\n    average : {'micro', 'macro', 'samples', 'weighted', \\\n            'binary'} or None, default='binary'\n        If ``None``, the scores for each class are returned. Otherwise, this\n        determines the type of averaging performed on the data:\n\n        ``'binary'``:\n            Only report results for the class specified by ``pos_label``.\n            This is applicable only if targets (``y_{true,pred}``) are binary.\n        ``'micro'``:\n            Calculate metrics globally by counting the total true positives,\n            false negatives and false positives.\n        ``'macro'``:\n            Calculate metrics for each label, and find their unweighted\n            mean.  This does not take label imbalance into account.\n        ``'weighted'``:\n            Calculate metrics for each label, and find their average, weighted\n            by support (the number of true instances for each label). This\n            alters 'macro' to account for label imbalance.\n        ``'samples'``:\n            Calculate metrics for each instance, and find their average (only\n            meaningful for multilabel classification).\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights.\n\n    zero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n        Sets the value to return when there is a zero division, i.e. when there\n        there are no negative values in predictions and labels. If set to\n        \"warn\", this acts like 0, but a warning is also raised.\n\n    Returns\n    -------\n    score : float or ndarray of shape (n_unique_labels,), dtype=np.float64\n        The Jaccard score. When `average` is not `None`, a single scalar is\n        returned.",
    "function_name": "jaccard_score",
    "ground_truth": "sklearn/metrics/_classification.py",
    "test": "sklearn/metrics/tests/test_classification.py::test_classification_metric_pos_label_types",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "",
    "data": "Table"
}