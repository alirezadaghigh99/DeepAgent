{
    "result": "Here's a detailed implementation of the `log_loss` function, following the steps outlined in your plan. This function calculates the log loss for binary classification problems, with support for sample weights and optional normalization.\n\n```python\nimport numpy as np\n\ndef log_loss(y_true, y_pred, normalize=True, sample_weight=None, labels=None):\n    \"\"\"\n    Calculate the log loss, also known as logistic loss or cross-entropy loss.\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,) - Ground truth labels.\n    - y_pred: array-like of shape (n_samples,) - Predicted probabilities.\n    - normalize: bool, default=True - If True, return the mean loss per sample. If False, return the sum of per-sample losses.\n    - sample_weight: array-like of shape (n_samples,), default=None - Sample weights.\n    - labels: array-like, default=None - Optional labels for the classes.\n\n    Returns:\n    - float - The calculated log loss.\n\n    Notes:\n    - This function assumes binary classification.\n    - Probabilities in y_pred should be clipped to avoid log(0).\n    - If labels are provided, they should match the classes in y_true.\n\n    References:\n    - For more information on log loss, see: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n    \"\"\"\n\n    # Validate inputs\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    if y_true.shape[0] != y_pred.shape[0]:\n        raise ValueError(\"The number of samples in y_true and y_pred must be the same.\")\n    \n    if np.any((y_pred < 0) | (y_pred > 1)):\n        raise ValueError(\"y_pred must contain probabilities, i.e., values between 0 and 1.\")\n    \n    if sample_weight is not None:\n        sample_weight = np.array(sample_weight)\n        if sample_weight.shape[0] != y_true.shape[0]:\n            raise ValueError(\"sample_weight must have the same number of samples as y_true.\")\n    else:\n        sample_weight = np.ones_like(y_true, dtype=float)\n    \n    # Calculate log loss\n    epsilon = 1e-15\n    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n    log_loss_values = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n    \n    # Apply sample weights\n    log_loss_values *= sample_weight\n    \n    # Normalize or sum\n    if normalize:\n        return np.mean(log_loss_values)\n    else:\n        return np.sum(log_loss_values)\n\n# Example usage\ny_true = [0, 1, 1, 0]\ny_pred = [0.1, 0.9, 0.8, 0.2]\nprint(log_loss(y_true, y_pred))  # Output: log loss value\n```\n\n### Key Points:\n- **Input Validation**: The function checks that `y_true` and `y_pred` have the same length and that `y_pred` contains valid probabilities.\n- **Sample Weights**: If provided, sample weights are used to weight the log loss for each sample.\n- **Log Loss Calculation**: The function uses a small `epsilon` to clip probabilities, preventing issues with `log(0)`.\n- **Normalization**: The function can return either the mean or the sum of the log loss values, based on the `normalize` parameter.\n- **Documentation**: The function includes a docstring with parameter descriptions, notes, and a reference for further reading.\n\nThis implementation is suitable for binary classification problems. For multi-class problems, additional handling would be required to manage multiple classes and their respective probabilities.",
    "prompt": "Generate a Python function called log_loss that calculates the log loss, also known as logistic loss or cross-entropy loss. The function takes in the following parameters:\n\n- y_true: Ground truth labels for n_samples samples.\n- y_pred: Predicted probabilities from a classifier's predict_proba method.\n- normalize: A boolean indicating whether to return the mean loss per sample or the sum of per-sample losses.\n- sample_weight: Optional sample weights.\n- labels: Optional labels for the classes.\n\nThe function calculates the log loss based on the predicted probabilities and true labels. It returns the log loss as a float value. The function also includes examples, notes, and references for further information.",
    "function_name": "log_loss",
    "ground_truth": "sklearn/metrics/_classification.py",
    "test": "sklearn/metrics/tests/test_classification.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Classification",
    "data": "Table"
}