{
    "result": "To implement the `sample_hidden_units` function for the `RBM` class, we need to follow the steps outlined in your prompt. Here's the complete implementation:\n\n```python\nimport tensorflow as tf\n\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        # Initialize weights and biases\n        self.w = tf.Variable(tf.random.normal([visible_units, hidden_units], stddev=init_stdv), name='weights')\n        self.bh = tf.Variable(tf.zeros([hidden_units]), name='hidden_biases')\n        self.keep = keep_prob  # Dropout keep probability\n\n    def binomial_sampling(self, pr):\n        # Outputs a tensor of the same shape as pr, where each element is 1 if the corresponding\n        # probability is greater than a sampled uniform random value, and 0 otherwise.\n        random_tensor = tf.random.uniform(tf.shape(pr))\n        return tf.cast(pr > random_tensor, dtype=tf.float32)\n\n    def sample_hidden_units(self, vv):\n        # Compute the activation probabilities of the hidden units\n        phv = tf.nn.sigmoid(tf.matmul(vv, self.w) + self.bh)\n        \n        # Apply dropout regularization\n        phv_dropout = phv * self.keep\n        \n        # Sample the hidden units\n        h_ = self.binomial_sampling(phv_dropout)\n        \n        return phv, h_\n\n# Example usage:\n# rbm = RBM(possible_ratings=5, visible_units=784)\n# vv = tf.random.normal([100, 784])  # Example batch of visible units\n# phv, h_ = rbm.sample_hidden_units(vv)\n```\n\n### Explanation:\n\n1. **Weights and Biases Initialization**:\n   - The weights (`self.w`) are initialized with a normal distribution with a standard deviation of `init_stdv`.\n   - The biases (`self.bh`) are initialized to zero.\n\n2. **Compute Activation Probabilities (`phv`)**:\n   - The activation probabilities of the hidden units are computed using the sigmoid function applied to the linear combination of the visible units (`vv`), weights (`self.w`), and biases (`self.bh`).\n\n3. **Apply Dropout Regularization**:\n   - The activation probabilities are scaled by the dropout keep probability (`self.keep`) to simulate the effect of dropout during training.\n\n4. **Sample Hidden Units (`h_`)**:\n   - The `binomial_sampling` method is used to sample the hidden units based on the adjusted activation probabilities (`phv_dropout`). This method generates a random tensor and compares it to the activation probabilities to decide whether each hidden unit is activated.\n\n5. **Return the Results**:\n   - The function returns both the activation probabilities (`phv`) and the sampled hidden units (`h_`).\n\nThis implementation uses TensorFlow for tensor operations and random sampling, which is suitable for building and training neural networks like RBMs. Adjustments may be needed if using a different library or framework.",
    "prompt": "Generate a Python function `sample_hidden_units` for the class `RBM` that samples hidden units based on the visible units (`vv`). This function should return two tensors: the activation probability of the hidden units (`phv`) and the sampled hidden units (`h_`). The sampling process involves using the weights (`self.w`), biases (`self.bh`), and dropout regularization (`self.keep`). The function relies on `binomial_sampling`, which outputs a tensor where each element is `1` if the corresponding activation probability exceeds a randomly sampled threshold, and `0` otherwise.You should use tf for this.\n\nClass structure:\n\n```python\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = None  # weights between visible and hidden units\n        self.bh = None  # biases of the hidden units\n        self.keep = keep_prob  # dropout keep probability\n        pass\n\n    def binomial_sampling(self, pr):\n        # Outputs a tensor of the same shape as pr, where each element is 1 if the corresponding\n        # probability is greater than a sampled uniform random value, and 0 otherwise.\n        pass\n\n    def sample_hidden_units(self, vv):\n        # Implement the function as described in the prompt",
    "function_name": "sample_hidden_units",
    "ground_truth": "recommenders/models/rbm/rbm.py",
    "test": "tests/unit/recommenders/models/test_rbm.py",
    "class": "RBM",
    "stage": "",
    "task": "Recommendation",
    "data": "Inference"
}