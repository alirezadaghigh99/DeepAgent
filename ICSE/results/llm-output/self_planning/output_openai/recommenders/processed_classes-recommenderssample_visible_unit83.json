{
    "result": "To implement the `sample_visible_units` function for the `RBM` class in TensorFlow, we need to follow the steps outlined in your prompt. Here's the complete implementation:\n\n```python\nimport tensorflow as tf\n\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = tf.Variable(tf.random.normal([visible_units, hidden_units], stddev=init_stdv), name='weights')\n        self.bv = tf.Variable(tf.zeros([visible_units]), name='visible_bias')\n        self.v = None  # This should be set to the original input vector during training or inference\n\n    def multinomial_distribution(self, phi):\n        # Outputs a tensor representing the probability distribution over the visible units\n        return tf.nn.softmax(phi)\n\n    def multinomial_sampling(self, pr):\n        # Outputs a tensor representing sampled values of the visible units\n        return tf.random.categorical(tf.math.log(pr), num_samples=1)\n\n    def sample_visible_units(self, h):\n        # Step 1: Calculate the linear combination\n        phi = tf.matmul(h, tf.transpose(self.w)) + self.bv\n        \n        # Step 2: Compute activation probabilities\n        pvh = self.multinomial_distribution(phi)\n        \n        # Step 3: Sample visible units\n        v_ = self.multinomial_sampling(pvh)\n        \n        # Step 4: Apply mask\n        if self.v is not None:\n            mask = tf.cast(self.v > 0, tf.float32)  # Assuming self.v is set and is a binary mask\n            v_ = tf.multiply(tf.squeeze(v_), mask)\n        \n        return pvh, v_\n\n# Example usage:\n# rbm = RBM(possible_ratings=5, visible_units=100)\n# h = tf.random.normal([batch_size, hidden_units])\n# pvh, v_ = rbm.sample_visible_units(h)\n```\n\n### Explanation:\n1. **Weights and Biases Initialization**: The weights (`self.w`) and biases (`self.bv`) are initialized in the constructor. The weights are initialized with a normal distribution, and the biases are initialized to zero.\n\n2. **Multinomial Distribution and Sampling**: \n   - The `multinomial_distribution` method uses `softmax` to compute probabilities over the visible units.\n   - The `multinomial_sampling` method uses `tf.random.categorical` to sample from these probabilities. Note that `tf.random.categorical` returns indices, so we use `tf.squeeze` to adjust the shape of the sampled tensor.\n\n3. **Mask Application**: \n   - A mask is applied to ensure that only the active units in the original input (`self.v`) are considered in the reconstructed visible units (`v_`). This assumes that `self.v` is set to the original input vector during training or inference and is a binary mask indicating active units.\n\nThis implementation assumes that `self.v` is set appropriately during training or inference. Adjustments may be needed based on the specific use case and data format.",
    "prompt": "prompt = '''\n\"\"Generate a Python function `sample_visible_units` for the class `RBM` that samples the visible units given the hidden units (`h`). This function should return two tensors: the activation probability of the visible units (`pvh`) and the sampled visible units (`v_`). The function involves calculating a linear combination of the hidden units with the transpose of the weights (`self.w`) and the biases of the visible units (`self.bv`), followed by using `multinomial_distribution` to determine the conditional probability `pvh`. The visible units are then sampled using `multinomial_sampling`. A mask is applied to enforce zero values in the reconstructed vector for the inactive units as indicated by the original input (`self.v`). The code should implement in tensorflow\n\nClass structure:\n\n```python\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = None  # weights between visible and hidden units\n        self.bv = None  # biases of the visible units\n        self.v = None  # original input vector\n        pass\n\n    def multinomial_distribution(self, phi):\n        # Outputs a tensor representing the probability distribution over the visible units\n        pass\n\n    def multinomial_sampling(self, pr):\n        # Outputs a tensor representing sampled values of the visible units\n        pass\n\n    def sample_visible_units(self, h):\n        # Implement the function as described in the prompt\n```\"\n```\"\n'''\ncode = get_completion(prompt)",
    "function_name": "sample_visible_unit",
    "ground_truth": "recommenders/models/rbm/rbm.py",
    "test": "tests/unit/recommenders/models/test_rbm.py",
    "class": "RBM",
    "stage": "",
    "task": "Recommendation",
    "data": "Inference"
}