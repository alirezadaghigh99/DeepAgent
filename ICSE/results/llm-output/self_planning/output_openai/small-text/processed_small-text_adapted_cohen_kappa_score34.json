{
    "result": "To create the `_adapted_cohen_kappa_score` function, we need to handle the special case of perfect agreement to prevent division by zero. Let's go through the steps and implement the function:\n\n1. **Understand Cohen's Kappa**: Cohen's kappa measures inter-rater agreement for categorical items, accounting for chance agreement.\n\n2. **Identify the Special Case**: Perfect agreement occurs when the observed agreement equals the expected agreement, potentially leading to division by zero.\n\n3. **Function Signature**: The function will accept `y1`, `y2`, `labels`, `weights`, and `sample_weight`.\n\n4. **Calculate Observed Agreement**: Use a confusion matrix to calculate the observed agreement.\n\n5. **Calculate Expected Agreement**: Compute the expected agreement based on the label distributions.\n\n6. **Handle Perfect Agreement**: If observed and expected agreements are equal, return a kappa score of 1.0.\n\n7. **Calculate Kappa**: If not equal, proceed with the standard kappa calculation.\n\n8. **Return the Result**: Return the computed kappa score.\n\nHere's the implementation:\n\n```python\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\ndef _adapted_cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None):\n    # Create a confusion matrix\n    cm = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)\n    \n    # Calculate the observed agreement\n    total = np.sum(cm)\n    observed_agreement = np.trace(cm) / total\n    \n    # Calculate the expected agreement\n    sum_over_rows = np.sum(cm, axis=1)\n    sum_over_cols = np.sum(cm, axis=0)\n    expected_agreement = np.sum(sum_over_rows * sum_over_cols) / (total ** 2)\n    \n    # Handle perfect agreement\n    if observed_agreement == expected_agreement:\n        return 1.0\n    \n    # Calculate kappa\n    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement)\n    \n    return kappa\n\n# Example usage\ny1 = [1, 2, 3, 4]\ny2 = [1, 2, 3, 4]\nprint(_adapted_cohen_kappa_score(y1, y2))  # Output should be 1.0 for perfect agreement\n```\n\n### Explanation:\n\n- **Confusion Matrix**: We use `confusion_matrix` from `sklearn.metrics` to compute the confusion matrix, which helps in calculating observed and expected agreements.\n\n- **Observed Agreement**: Calculated as the trace of the confusion matrix divided by the total number of samples.\n\n- **Expected Agreement**: Calculated based on the marginal sums of the confusion matrix.\n\n- **Perfect Agreement Handling**: If observed and expected agreements are equal, return 1.0 to avoid division by zero.\n\n- **Kappa Calculation**: If not perfectly agreeing, compute kappa using the standard formula.\n\nThis implementation ensures that the function handles the special case of perfect agreement correctly.",
    "prompt": "Create a Python function called _adapted_cohen_kappa_score that extends Cohen's kappa by handling the special case of perfect agreement, preventing a division by zero error. \ndef _adapted_cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None):",
    "function_name": "_adapted_cohen_kappa_score",
    "ground_truth": "small_text/stopping_criteria/kappa.py",
    "test": "tests/unit/small_text/stopping_criteria/test_kappa.py::TestAdaptedCohenKappScore",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}