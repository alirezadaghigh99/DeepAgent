{
    "result": "Here's a detailed implementation of the `depth_to_3d` function, which converts depth maps to 3D points using camera intrinsics. This function includes input validation, computation of 3D points, optional normalization, and error handling.\n\n```python\nimport torch\n\ndef depth_to_3d(depth, camera_matrix, normalize=False):\n    # Validate input types\n    if not isinstance(depth, torch.Tensor):\n        raise TypeError(\"Depth must be a torch.Tensor\")\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise TypeError(\"Camera matrix must be a torch.Tensor\")\n\n    # Validate input shapes\n    if depth.ndim != 4 or depth.shape[1] != 1:\n        raise ValueError(\"Depth tensor must have shape (B, 1, H, W)\")\n    if camera_matrix.ndim != 3 or camera_matrix.shape[1:] != (3, 3):\n        raise ValueError(\"Camera matrix must have shape (B, 3, 3)\")\n\n    B, _, H, W = depth.shape\n\n    # Create a meshgrid of pixel coordinates\n    u = torch.arange(W, device=depth.device).repeat(H, 1)\n    v = torch.arange(H, device=depth.device).repeat(W, 1).t()\n    u = u.unsqueeze(0).repeat(B, 1, 1)\n    v = v.unsqueeze(0).repeat(B, 1, 1)\n\n    # Extract camera intrinsics\n    fx = camera_matrix[:, 0, 0].view(B, 1, 1)\n    fy = camera_matrix[:, 1, 1].view(B, 1, 1)\n    cx = camera_matrix[:, 0, 2].view(B, 1, 1)\n    cy = camera_matrix[:, 1, 2].view(B, 1, 1)\n\n    # Compute 3D points\n    Z = depth.squeeze(1)\n    X = (u - cx) * Z / fx\n    Y = (v - cy) * Z / fy\n\n    points_3d = torch.stack((X, Y, Z), dim=1)\n\n    # Normalize points if required\n    if normalize:\n        norm = torch.norm(points_3d, dim=1, keepdim=True)\n        points_3d = points_3d / (norm + 1e-8)  # Add epsilon to avoid division by zero\n\n    return points_3d\n\n# Example usage:\n# depth = torch.rand((2, 1, 480, 640))  # Example depth tensor\n# camera_matrix = torch.rand((2, 3, 3))  # Example camera matrix\n# points_3d = depth_to_3d(depth, camera_matrix, normalize=True)\n```\n\n### Key Points:\n1. **Input Validation**: The function checks that both `depth` and `camera_matrix` are tensors and have the correct shapes.\n2. **Meshgrid Creation**: A meshgrid of pixel coordinates `(u, v)` is created for each batch.\n3. **Camera Intrinsics Extraction**: Focal lengths and principal points are extracted from the camera matrix.\n4. **3D Point Computation**: The 3D coordinates `(X, Y, Z)` are computed using the depth values and camera intrinsics.\n5. **Normalization**: If the `normalize` flag is set, the 3D points are normalized to prevent division by zero by adding a small epsilon.\n6. **Error Handling**: The function raises appropriate errors for incorrect input types or shapes.\n\nThis implementation should work efficiently with PyTorch tensors and can be adapted for use with other tensor libraries like TensorFlow with minor modifications.",
    "prompt": "Generate a Python function called depth_to_3d that takes in a tensor representing depth values per pixel, a tensor containing camera intrinsics, and a boolean flag to normalize points. The function computes a 3D point per pixel based on the depth value and camera intrinsics. The input depth tensor should have a shape of (B, 1, H, W) and the camera_matrix tensor should have a shape of (B, 3, 3). The function returns a tensor with a 3D point per pixel of the same resolution as the input, with a shape of (B, 3, H, W). The function also includes error handling for input types and shapes.",
    "function_name": "depth_to_3d",
    "ground_truth": "kornia/geometry/depth.py",
    "test": "tests/geometry/test_depth.py::TestDepthTo3d::test_shapes_broadcast",
    "class": "",
    "stage": "Model Construction",
    "task": "",
    "data": "Image"
}