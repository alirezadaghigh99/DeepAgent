{
    "result": "To generate the `sample_visible_units` function for the `RBM` class in TensorFlow, we need to follow the steps outlined in the prompt. Here's how you can implement this function:\n\n1. **Calculate the Linear Combination**: Compute the linear combination of the hidden units with the transpose of the weights and add the biases of the visible units.\n\n2. **Compute Activation Probabilities**: Use the `multinomial_distribution` method to determine the conditional probability of the visible units.\n\n3. **Sample Visible Units**: Use the `multinomial_sampling` method to sample the visible units based on the computed probabilities.\n\n4. **Apply Mask**: Enforce zero values in the reconstructed vector for the inactive units as indicated by the original input.\n\nHere's the implementation:\n\n```python\nimport tensorflow as tf\n\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = tf.Variable(tf.random.normal([visible_units, hidden_units], stddev=init_stdv), name='weights')\n        self.bv = tf.Variable(tf.zeros([visible_units]), name='visible_bias')\n        self.v = None  # This should be set to the original input vector during training or inference\n        pass\n\n    def multinomial_distribution(self, phi):\n        # Outputs a tensor representing the probability distribution over the visible units\n        return tf.nn.softmax(phi)\n\n    def multinomial_sampling(self, pr):\n        # Outputs a tensor representing sampled values of the visible units\n        return tf.random.categorical(tf.math.log(pr), num_samples=1)\n\n    def sample_visible_units(self, h):\n        # Calculate the linear combination\n        phi_v = tf.matmul(h, tf.transpose(self.w)) + self.bv\n        \n        # Compute the activation probabilities\n        pvh = self.multinomial_distribution(phi_v)\n        \n        # Sample the visible units\n        v_ = self.multinomial_sampling(pvh)\n        \n        # Apply mask to enforce zero values for inactive units\n        mask = tf.cast(tf.not_equal(self.v, 0), dtype=tf.float32)\n        v_ = tf.multiply(v_, mask)\n        \n        return pvh, v_\n\n# Example usage:\n# rbm = RBM(possible_ratings=5, visible_units=100)\n# h = tf.random.normal([1, 500])  # Example hidden units\n# pvh, v_ = rbm.sample_visible_units(h)\n```\n\n### Explanation:\n- **Weights and Biases Initialization**: The weights `self.w` and biases `self.bv` are initialized in the constructor. The weights are initialized with a normal distribution, and the biases are initialized to zero.\n- **Multinomial Distribution**: The `multinomial_distribution` method uses the softmax function to compute the probability distribution over the visible units.\n- **Multinomial Sampling**: The `multinomial_sampling` method uses TensorFlow's `tf.random.categorical` to sample from the computed probabilities.\n- **Mask Application**: A mask is created to ensure that only the active units in the original input `self.v` are considered in the sampled visible units.\n\nThis implementation assumes that `self.v` is set to the original input vector before calling `sample_visible_units`. Adjustments may be needed based on how `self.v` is managed in your application.",
    "prompt": "prompt = '''\n\"\"Generate a Python function `sample_visible_units` for the class `RBM` that samples the visible units given the hidden units (`h`). This function should return two tensors: the activation probability of the visible units (`pvh`) and the sampled visible units (`v_`). The function involves calculating a linear combination of the hidden units with the transpose of the weights (`self.w`) and the biases of the visible units (`self.bv`), followed by using `multinomial_distribution` to determine the conditional probability `pvh`. The visible units are then sampled using `multinomial_sampling`. A mask is applied to enforce zero values in the reconstructed vector for the inactive units as indicated by the original input (`self.v`). The code should implement in tensorflow\n\nClass structure:\n\n```python\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = None  # weights between visible and hidden units\n        self.bv = None  # biases of the visible units\n        self.v = None  # original input vector\n        pass\n\n    def multinomial_distribution(self, phi):\n        # Outputs a tensor representing the probability distribution over the visible units\n        pass\n\n    def multinomial_sampling(self, pr):\n        # Outputs a tensor representing sampled values of the visible units\n        pass\n\n    def sample_visible_units(self, h):\n        # Implement the function as described in the prompt\n```\"\n```\"\n'''\ncode = get_completion(prompt)",
    "function_name": "sample_visible_unit",
    "ground_truth": "recommenders/models/rbm/rbm.py",
    "test": "tests/unit/recommenders/models/test_rbm.py",
    "class": "RBM",
    "stage": "",
    "task": "Recommendation",
    "data": "Inference"
}