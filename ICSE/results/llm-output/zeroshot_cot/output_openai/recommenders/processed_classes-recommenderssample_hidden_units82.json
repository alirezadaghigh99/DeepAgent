{
    "result": "To implement the `sample_hidden_units` function for the `RBM` class, we need to calculate the activation probabilities of the hidden units given the visible units and then sample the hidden units using these probabilities. The process involves matrix multiplication with the weights, adding biases, applying the sigmoid activation function, and then using dropout regularization. Finally, we use the `binomial_sampling` method to sample the hidden units.\n\nHere's how you can implement the `sample_hidden_units` function:\n\n```python\nimport tensorflow as tf\n\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = tf.Variable(tf.random.normal([visible_units, hidden_units], stddev=init_stdv), name='weights')\n        self.bh = tf.Variable(tf.zeros([hidden_units]), name='hidden_biases')\n        self.keep = keep_prob  # dropout keep probability\n        self.seed = seed\n        tf.random.set_seed(seed)\n\n    def binomial_sampling(self, pr):\n        # Outputs a tensor of the same shape as pr, where each element is 1 if the corresponding\n        # probability is greater than a sampled uniform random value, and 0 otherwise.\n        random_tensor = tf.random.uniform(tf.shape(pr), seed=self.seed)\n        return tf.cast(pr > random_tensor, dtype=tf.float32)\n\n    def sample_hidden_units(self, vv):\n        # Compute the activation probabilities of the hidden units\n        pre_activation = tf.matmul(vv, self.w) + self.bh\n        phv = tf.nn.sigmoid(pre_activation)\n\n        # Apply dropout regularization\n        phv_dropout = phv * self.keep\n\n        # Sample the hidden units\n        h_ = self.binomial_sampling(phv_dropout)\n\n        return phv, h_\n\n# Example usage:\n# rbm = RBM(possible_ratings=5, visible_units=784)\n# visible_units_tensor = tf.random.normal([100, 784])  # Example visible units\n# phv, h_ = rbm.sample_hidden_units(visible_units_tensor)\n```\n\n### Explanation:\n- **Weights and Biases Initialization**: The weights `self.w` and biases `self.bh` are initialized in the constructor. The weights are initialized with a normal distribution, and biases are initialized to zero.\n- **Activation Probabilities (`phv`)**: Calculated using the sigmoid function applied to the linear combination of input visible units `vv`, weights `self.w`, and biases `self.bh`.\n- **Dropout Regularization**: The activation probabilities are scaled by the dropout keep probability `self.keep`.\n- **Sampling**: The `binomial_sampling` method is used to sample the hidden units based on the dropout-scaled activation probabilities.\n\nThis function will return the activation probabilities and the sampled hidden units as tensors.",
    "prompt": "Generate a Python function `sample_hidden_units` for the class `RBM` that samples hidden units based on the visible units (`vv`). This function should return two tensors: the activation probability of the hidden units (`phv`) and the sampled hidden units (`h_`). The sampling process involves using the weights (`self.w`), biases (`self.bh`), and dropout regularization (`self.keep`). The function relies on `binomial_sampling`, which outputs a tensor where each element is `1` if the corresponding activation probability exceeds a randomly sampled threshold, and `0` otherwise.You should use tf for this.\n\nClass structure:\n\n```python\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = None  # weights between visible and hidden units\n        self.bh = None  # biases of the hidden units\n        self.keep = keep_prob  # dropout keep probability\n        pass\n\n    def binomial_sampling(self, pr):\n        # Outputs a tensor of the same shape as pr, where each element is 1 if the corresponding\n        # probability is greater than a sampled uniform random value, and 0 otherwise.\n        pass\n\n    def sample_hidden_units(self, vv):\n        # Implement the function as described in the prompt",
    "function_name": "sample_hidden_units",
    "ground_truth": "recommenders/models/rbm/rbm.py",
    "test": "tests/unit/recommenders/models/test_rbm.py",
    "class": "RBM",
    "stage": "",
    "task": "Recommendation",
    "data": "Inference"
}