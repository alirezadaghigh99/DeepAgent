{
    "result": "To create the `find_predicted_neq_given` function, we'll break down the task into several steps. We'll start by implementing input validation, then handle the multi-label case, and finally, implement the main logic for identifying label issues. Let's go through these steps:\n\n### Step 1: Input Validation\n\nFirst, we need to ensure that the inputs are valid. We'll check that `labels` and `pred_probs` are either lists or numpy arrays, and that they have compatible shapes.\n\n```python\nimport numpy as np\n\ndef validate_inputs(labels, pred_probs, multi_label):\n    if not isinstance(labels, (np.ndarray, list)):\n        raise ValueError(\"Labels must be a numpy array or a list.\")\n    if not isinstance(pred_probs, np.ndarray):\n        raise ValueError(\"Prediction probabilities must be a numpy array.\")\n    if not isinstance(multi_label, bool):\n        raise ValueError(\"multi_label must be a boolean.\")\n    if len(labels) != len(pred_probs):\n        raise ValueError(\"Labels and prediction probabilities must have the same length.\")\n```\n\n### Step 2: Multi-label Handling\n\nFor the multi-label case, we'll define a helper function `_find_predicted_neq_given_multilabel`. This function will handle the logic specific to multi-label classification.\n\n```python\ndef _find_predicted_neq_given_multilabel(labels, pred_probs):\n    # Assuming labels are binary (0 or 1) for each class\n    threshold = 0.5\n    label_issues = []\n    for i in range(len(labels)):\n        predicted_labels = (pred_probs[i] >= threshold).astype(int)\n        label_issues.append(not np.array_equal(labels[i], predicted_labels))\n    return np.array(label_issues)\n```\n\n### Step 3: Main Logic for Single-label Case\n\nFor the single-label case, we'll compare the predicted class with the given label and check if the prediction confidence is high.\n\n```python\ndef find_predicted_neq_given(labels, pred_probs, multi_label=False):\n    # Validate inputs\n    validate_inputs(labels, pred_probs, multi_label)\n    \n    if multi_label:\n        return _find_predicted_neq_given_multilabel(labels, pred_probs)\n    \n    # Single-label case\n    label_issues = []\n    for i in range(len(labels)):\n        predicted_class = np.argmax(pred_probs[i])\n        confidence = np.max(pred_probs[i])\n        if predicted_class != labels[i] and confidence > 0.8:  # Assuming 0.8 as a high confidence threshold\n            label_issues.append(True)\n        else:\n            label_issues.append(False)\n    \n    return np.array(label_issues)\n```\n\n### Explanation\n\n- **Input Validation**: We ensure that the inputs are of the correct type and compatible in terms of length.\n- **Multi-label Handling**: We use a threshold (0.5) to determine predicted labels and compare them with the given labels.\n- **Single-label Logic**: We identify label issues by checking if the predicted class differs from the given label and if the prediction confidence is high (greater than 0.8).\n\nThis function should now be able to identify label issues in both single-label and multi-label datasets.",
    "prompt": "Generate a Python function called find_predicted_neq_given that implements a simple baseline approach to identify label issues. The function takes in three parameters: labels (np.ndarray or list), pred_probs (np.ndarray), and multi_label (bool, optional). The function returns a boolean mask for the entire dataset where True represents a label issue and False represents an accurately labeled example with high confidence. The function includes input validation and handles cases where multi_label is True by calling a helper function _find_predicted_neq_given_multilabel.",
    "function_name": "find_predicted_neq_given",
    "ground_truth": "cleanlab/filter.py",
    "test": "tests/test_filter_count.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Classification",
    "data": "Table"
}