{
    "result": "To implement the `temp_scale_pred_probs` function, we will follow these steps:\n\n1. **Clip the Predicted Probabilities**: To avoid taking the logarithm of zero, we will clip the predicted probabilities to a small positive value.\n\n2. **Normalize the Probabilities**: Ensure that the probabilities sum to 1 for each set of predictions.\n\n3. **Apply Temperature Scaling**: Use the temperature scaling technique by adjusting the logits (logarithm of probabilities) and then applying the softmax function.\n\n4. **Return the Scaled Probabilities**: The final output should be a 2D NumPy array of scaled probabilities.\n\nLet's implement this step by step:\n\n```python\nimport numpy as np\n\ndef temp_scale_pred_probs(pred_probs, temp):\n    # Step 1: Clip the predicted probabilities to avoid log(0)\n    epsilon = 1e-12\n    pred_probs = np.clip(pred_probs, epsilon, 1.0)\n    \n    # Step 2: Normalize the probabilities\n    pred_probs /= np.sum(pred_probs, axis=1, keepdims=True)\n    \n    # Step 3: Apply temperature scaling\n    # Take the log of the probabilities\n    log_probs = np.log(pred_probs)\n    \n    # Scale the log probabilities by the temperature\n    scaled_log_probs = log_probs / temp\n    \n    # Apply the softmax function to get the scaled probabilities\n    exp_scaled_log_probs = np.exp(scaled_log_probs)\n    scaled_probs = exp_scaled_log_probs / np.sum(exp_scaled_log_probs, axis=1, keepdims=True)\n    \n    # Step 4: Return the scaled probabilities\n    return scaled_probs\n```\n\n### Explanation:\n\n- **Clipping**: We use `np.clip` to ensure that the probabilities are not exactly zero, which would cause issues when taking the logarithm.\n\n- **Normalization**: We normalize the probabilities to ensure they sum to 1 for each set of predictions. This is done by dividing each probability by the sum of probabilities in its row.\n\n- **Temperature Scaling**: \n  - We first take the logarithm of the probabilities to get the logits.\n  - We then divide these logits by the temperature factor `temp`.\n  - Finally, we apply the softmax function to these scaled logits to get the adjusted probabilities.\n\n- **Softmax Function**: The softmax function is applied by exponentiating the scaled logits and normalizing them to sum to 1.\n\nThis function will adjust the confidence of the predicted probabilities based on the temperature factor provided.",
    "prompt": "You need to implement a function named `temp_scale_pred_probs` that adjusts predicted probabilities using a temperature scaling technique. Temperature factor less than 1 sharpens the predictions, making them more confident, while a temperature greater than 1 smoothens the predictions, making them less confident. The function takes a 2D NumPy array of predicted probabilities (`pred_probs`) and a float (`temp`) representing the temperature factor. The probabilities are first clipped to avoid logarithm of zero and normalized. Then, the temperature scaling is applied using the softmax function after taking the logarithm of the probabilities. The function returns the scaled and normalized probabilities as a 2D NumPy array.",
    "function_name": "temp_scale_pred_probs",
    "ground_truth": "cleanlab/internal/multiannotator_utils.py#L353",
    "test": "tests/test_multiannotator.py",
    "class": "",
    "stage": "Pre-Post processing",
    "task": "Classification",
    "data": "Table"
}