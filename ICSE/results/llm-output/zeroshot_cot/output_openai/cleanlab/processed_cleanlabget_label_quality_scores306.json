{
    "result": "To generate the `get_label_quality_scores` function, we will break down the task into several steps. We'll start by defining the function signature and then implement the logic for each scoring method. Finally, we'll handle optional adjustments for class imbalance and ensure the scores are between 0 and 1.\n\n### Step 1: Define the Function Signature\n\nWe'll start by defining the function signature with the necessary parameters.\n\n```python\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef get_label_quality_scores(labels, pred_probs, method=\"self_confidence\", adjust_pred_probs=False):\n    \"\"\"\n    Compute label quality scores for a multi-class classification dataset.\n\n    Parameters\n    ----------\n    labels : np.ndarray\n        A discrete vector of noisy labels.\n    pred_probs : np.ndarray\n        An array of shape (N, K) of model-predicted probabilities.\n    method : {\"self_confidence\", \"normalized_margin\", \"confidence_weighted_entropy\"}, default=\"self_confidence\"\n        Label quality scoring method.\n    adjust_pred_probs : bool, optional\n        Adjust predicted probabilities for class imbalance.\n\n    Returns\n    -------\n    label_quality_scores : np.ndarray\n        Contains one score (between 0 and 1) per example.\n    \"\"\"\n    # Ensure labels and pred_probs are numpy arrays\n    labels = np.asarray(labels)\n    pred_probs = np.asarray(pred_probs)\n    \n    # Check that pred_probs shape matches the number of labels\n    assert pred_probs.shape[0] == labels.shape[0], \"Number of predictions must match number of labels.\"\n    \n    # Number of classes\n    num_classes = pred_probs.shape[1]\n    \n    # Initialize scores\n    scores = np.zeros(labels.shape[0])\n    \n    # Adjust predicted probabilities if required\n    if adjust_pred_probs:\n        class_counts = np.bincount(labels, minlength=num_classes)\n        class_probs = class_counts / labels.size\n        pred_probs = pred_probs - class_probs\n        pred_probs = np.clip(pred_probs, 0, None)\n        pred_probs = pred_probs / pred_probs.sum(axis=1, keepdims=True)\n    \n    # Compute scores based on the selected method\n    for i, (label, probs) in enumerate(zip(labels, pred_probs)):\n        if method == \"self_confidence\":\n            scores[i] = probs[label]\n        elif method == \"normalized_margin\":\n            max_other_prob = np.max(np.delete(probs, label))\n            scores[i] = probs[label] - max_other_prob\n        elif method == \"confidence_weighted_entropy\":\n            scores[i] = entropy(probs) / probs[label]\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n    \n    # Normalize scores to be between 0 and 1\n    scores = np.clip(scores, 0, 1)\n    \n    return scores\n```\n\n### Explanation\n\n1. **Function Signature**: The function takes in `labels`, `pred_probs`, `method`, and `adjust_pred_probs` as parameters.\n\n2. **Input Validation**: We ensure that `labels` and `pred_probs` are numpy arrays and that their dimensions match.\n\n3. **Adjust Predicted Probabilities**: If `adjust_pred_probs` is `True`, we adjust the predicted probabilities to account for class imbalance.\n\n4. **Score Calculation**: We iterate over each data point and calculate the score based on the selected method:\n   - **Self Confidence**: Directly uses the predicted probability of the given label.\n   - **Normalized Margin**: Computes the difference between the predicted probability of the given label and the highest probability of any other label.\n   - **Confidence Weighted Entropy**: Computes the entropy of the predicted probabilities divided by the self-confidence score.\n\n5. **Normalization**: We ensure that the scores are clipped between 0 and 1.\n\nThis function should provide a robust way to compute label quality scores for multi-class classification datasets.",
    "prompt": "Generate a Python function called get_label_quality_scores that Returns a label quality score for each datapoint.\n\n    This is a function to compute label quality scores for standard (multi-class) classification datasets,\n    where lower scores indicate labels less likely to be correct.\n\n    Score is between 0 and 1.\n\n    1 - clean label (given label is likely correct).\n    0 - dirty label (given label is likely incorrect).\n\n    Parameters\n    ----------\n    labels : np.ndarray\n      A discrete vector of noisy labels, i.e. some labels may be erroneous.\n      *Format requirements*: for dataset with K classes, labels must be in 0, 1, ..., K-1.\n      Note: multi-label classification is not supported by this method, each example must belong to a single class, e.g. format: ``labels = np.ndarray([1,0,2,1,1,0...])``.\n\n    pred_probs : np.ndarray, optional\n      An array of shape ``(N, K)`` of model-predicted probabilities,\n      ``P(label=k|x)``. Each row of this matrix corresponds\n      to an example `x` and contains the model-predicted probabilities that\n      `x` belongs to each possible class, for each of the K classes. The\n      columns must be ordered such that these probabilities correspond to\n      class 0, 1, ..., K-1.\n\n      **Note**: Returned label issues are most accurate when they are computed based on out-of-sample `pred_probs` from your model.\n      To obtain out-of-sample predicted probabilities for every datapoint in your dataset, you can use :ref:`cross-validation <pred_probs_cross_val>`.\n      This is encouraged to get better results.\n\n    method : {\"self_confidence\", \"normalized_margin\", \"confidence_weighted_entropy\"}, default=\"self_confidence\"\n      Label quality scoring method.\n\n      Letting ``k = labels[i]`` and ``P = pred_probs[i]`` denote the given label and predicted class-probabilities\n      for datapoint *i*, its score can either be:\n\n      - ``'normalized_margin'``: ``P[k] - max_{k' != k}[ P[k'] ]``\n      - ``'self_confidence'``: ``P[k]``\n      - ``'confidence_weighted_entropy'``: ``entropy(P) / self_confidence``\n\n      Note: the actual label quality scores returned by this method\n      may be transformed versions of the above, in order to ensure\n      their values lie between 0-1 with lower values indicating more likely mislabeled data.\n\n      Let ``C = {0, 1, ..., K-1}`` be the set of classes specified for our classification task.\n\n      The `normalized_margin` score works better for identifying class conditional label errors,\n      i.e. examples for which another label in ``C`` is appropriate but the given label is not.\n\n      The `self_confidence` score works better for identifying alternative label issues\n      corresponding to bad examples that are: not from any of the classes in ``C``,\n      well-described by 2 or more labels in ``C``,\n      or generally just out-of-distribution (i.e. anomalous outliers).\n\n    adjust_pred_probs : bool, optional\n      Account for class imbalance in the label-quality scoring by adjusting predicted probabilities\n      via subtraction of class confident thresholds and renormalization.\n      Set this to ``True`` if you prefer to account for class-imbalance.\n      See `Northcutt et al., 2021 <https://jair.org/index.php/jair/article/view/12125>`_.\n\n    Returns\n    -------\n    label_quality_scores : np.ndarray\n      Contains one score (between 0 and 1) per example.\n      Lower scores indicate more likely mislabeled examples.",
    "function_name": "get_label_quality_scores",
    "ground_truth": "cleanlab/rank.py",
    "test": "tests/test_object_detection.py",
    "class": "",
    "stage": "Inference",
    "task": "Classification",
    "data": ""
}