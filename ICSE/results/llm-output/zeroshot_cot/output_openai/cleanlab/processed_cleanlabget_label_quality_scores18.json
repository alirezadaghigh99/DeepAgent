{
    "result": "To create the `get_label_quality_scores` function, we need to follow these steps:\n\n1. **Validate Inputs**: Ensure that the inputs `labels` and `predictions` are valid, i.e., they are array-like and have the same length.\n\n2. **Define Scoring Method**: Implement the default scoring method (\"outre\") and allow for future extension to other methods.\n\n3. **Calculate Scores**: Compute the label quality scores based on the specified method.\n\n4. **Return Scores**: Return an array of scores, with each score between 0 and 1.\n\nLet's implement this step by step:\n\n### Step 1: Validate Inputs\n\nWe need to ensure that `labels` and `predictions` are array-like and have the same length. We can use NumPy to handle array-like inputs.\n\n### Step 2: Define Scoring Method\n\nFor this example, we'll implement a simple scoring method called \"outre\". This method will calculate scores based on the absolute difference between the labels and predictions, normalized to a range between 0 and 1.\n\n### Step 3: Calculate Scores\n\nWe'll calculate the scores using the specified method.\n\n### Step 4: Return Scores\n\nFinally, we'll return the computed scores.\n\nHere's the complete implementation:\n\n```python\nimport numpy as np\n\ndef get_label_quality_scores(labels, predictions, method=\"outre\"):\n    # Convert inputs to numpy arrays\n    labels = np.asarray(labels)\n    predictions = np.asarray(predictions)\n    \n    # Validate inputs\n    if labels.shape != predictions.shape:\n        raise ValueError(\"Labels and predictions must have the same shape.\")\n    \n    if method == \"outre\":\n        # Calculate absolute differences\n        differences = np.abs(labels - predictions)\n        \n        # Normalize differences to a range between 0 and 1\n        max_diff = np.max(differences)\n        if max_diff == 0:\n            # If max_diff is 0, all predictions are perfect, return scores of 1\n            return np.ones_like(differences)\n        \n        scores = 1 - (differences / max_diff)\n        \n        # Ensure scores are between 0 and 1\n        scores = np.clip(scores, 0, 1)\n        \n        return scores\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n# Example usage\nlabels = np.array([1, 2, 3, 4])\npredictions = np.array([2, 2, 5, 4.1])\n\nlabel_quality_scores = get_label_quality_scores(labels, predictions)\nprint(label_quality_scores)\n```\n\n### Explanation\n\n- **Input Validation**: We check if `labels` and `predictions` have the same shape.\n- **Scoring Method**: The \"outre\" method calculates the absolute difference between each label and prediction, normalizes it, and inverts it to get a score between 0 and 1.\n- **Normalization**: We normalize the differences by dividing by the maximum difference to ensure scores are between 0 and 1.\n- **Output**: The function returns an array of scores, with each score indicating the quality of the corresponding label.",
    "prompt": "Create a Python function called get_label_quality_scores that calculates label quality scores for each example in a regression dataset. The function takes in two array-like inputs: labels (raw labels from the original dataset) and predictions (predicted labels for each example). Additionally, the function has an optional keyword argument method, which specifies the scoring method to use (default is \"outre\").\n\nThe function returns an array of label quality scores, where each score is a continuous value between 0 and 1. A score of 1 indicates a clean label (likely correct), while a score of 0 indicates a dirty label (likely incorrect).\n\nEnsure that the inputs are valid and then use the specified scoring method to calculate the label quality scores. The output is an array of scores with one score per example in the dataset.\n\nExample usage:\n```python\nimport numpy as np\nfrom cleanlab.regression.rank import get_label_quality_scores\n\nlabels = np.array([1, 2, 3, 4])\npredictions = np.array([2, 2, 5, 4.1])\n\nlabel_quality_scores = get_label_quality_scores(labels, predictions)\nprint(label_quality_scores)\n# Output: array([0.00323821, 0.33692597, 0.00191686, 0.33692597])\n```",
    "function_name": "get_label_quality_scores",
    "ground_truth": "cleanlab/regression/rank.py",
    "test": "tests/test_object_detection.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}