{
    "result": "To implement the `_subtract_confident_thresholds` function, we need to follow a series of steps. Let's break down the process:\n\n1. **Input Validation**: Ensure that either labels or pre-calculated thresholds are provided. If neither is provided, raise a `ValueError`.\n\n2. **Calculate or Use Provided Thresholds**: If thresholds are not provided, calculate them using the `get_confident_thresholds` method. This method should determine class-specific thresholds based on the labels and predicted probabilities.\n\n3. **Subtract Thresholds**: For each class, subtract the corresponding threshold from the predicted probabilities.\n\n4. **Ensure Non-Negative Values**: After subtraction, ensure that no probability is negative. This can be done by shifting the probabilities so that the minimum value is zero.\n\n5. **Re-normalize Probabilities**: After adjusting the probabilities, re-normalize them so that they sum to 1 for each instance.\n\n6. **Return Adjusted Probabilities**: Finally, return the adjusted probabilities as a NumPy array.\n\nLet's start coding this step by step:\n\n```python\nimport numpy as np\n\ndef get_confident_thresholds(labels, predicted_probs):\n    # Placeholder for the actual implementation of threshold calculation\n    # This function should return a list or array of thresholds for each class\n    # For simplicity, let's assume it returns a fixed threshold for each class\n    num_classes = predicted_probs.shape[1]\n    return np.full(num_classes, 0.1)  # Example: fixed threshold of 0.1 for each class\n\ndef _subtract_confident_thresholds(labels=None, predicted_probs=None, multi_label=False, thresholds=None):\n    if predicted_probs is None:\n        raise ValueError(\"Predicted probabilities must be provided.\")\n    \n    if thresholds is None:\n        if labels is None:\n            raise ValueError(\"Either labels or pre-calculated thresholds must be provided.\")\n        # Calculate thresholds if not provided\n        thresholds = get_confident_thresholds(labels, predicted_probs)\n    \n    # Subtract thresholds from predicted probabilities\n    adjusted_probs = predicted_probs - thresholds\n    \n    # Ensure no negative values by shifting\n    adjusted_probs = np.maximum(adjusted_probs, 0)\n    \n    # Re-normalize probabilities\n    if multi_label:\n        # For multi-label, normalize each instance separately\n        row_sums = adjusted_probs.sum(axis=1, keepdims=True)\n        # Avoid division by zero\n        row_sums[row_sums == 0] = 1\n        adjusted_probs = adjusted_probs / row_sums\n    else:\n        # For single-label, normalize across classes\n        adjusted_probs = adjusted_probs / adjusted_probs.sum(axis=1, keepdims=True)\n    \n    return adjusted_probs\n\n# Example usage\npredicted_probs = np.array([[0.3, 0.4, 0.3], [0.1, 0.7, 0.2]])\nlabels = np.array([1, 0])  # Example labels\nadjusted_probs = _subtract_confident_thresholds(labels=labels, predicted_probs=predicted_probs)\nprint(adjusted_probs)\n```\n\n### Explanation:\n\n- **get_confident_thresholds**: This is a placeholder function that should be replaced with the actual logic to calculate class-specific thresholds. For now, it returns a fixed threshold for simplicity.\n\n- **_subtract_confident_thresholds**: This function performs the main task of adjusting the predicted probabilities. It handles both single-label and multi-label scenarios.\n\n- **Normalization**: The probabilities are re-normalized to ensure they sum to 1 for each instance, which is crucial for valid probability distributions.\n\nThis code provides a basic framework for the function. You can further refine the `get_confident_thresholds` method to calculate thresholds based on your specific requirements.",
    "prompt": "You need to implement a function named `_subtract_confident_thresholds` that adjusts predicted probabilities by subtracting class-specific confidence thresholds and then re-normalizing the probabilities. This adjustment aims to handle class imbalance in classification tasks. The function accepts labels, predicted probabilities, an optional flag for multi-label settings, and pre-calculated confidence thresholds. If confidence thresholds are not provided, they will be calculated from the labels and predicted probabilities using the `get_confident_thresholds` method. After subtracting the thresholds, the function ensures no negative values by shifting and then re-normalizing the probabilities. The function returns the adjusted predicted probabilities as a NumPy array. If neither labels nor pre-calculated thresholds are provided, a `ValueError` is raised.",
    "function_name": "_subtract_confident_thresholds",
    "ground_truth": "cleanlab/internal/label_quality_utils.py#L26",
    "test": "tests/test_rank.py",
    "class": "",
    "stage": "Inference",
    "task": "",
    "data": "Table"
}