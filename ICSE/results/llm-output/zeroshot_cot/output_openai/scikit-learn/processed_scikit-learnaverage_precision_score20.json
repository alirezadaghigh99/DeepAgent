{
    "result": "To create the `average_precision_score` function, we need to handle different types of input data, such as binary, multilabel-indicator, and multiclass. We'll also implement a helper function `_binary_uninterpolated_average_precision` to calculate the average precision for binary classification. Let's break down the implementation step by step.\n\n### Step 1: Implement the Helper Function\n\nFirst, we'll implement the `_binary_uninterpolated_average_precision` function. This function will calculate the average precision for binary classification.\n\n```python\nimport numpy as np\n\ndef _binary_uninterpolated_average_precision(y_true, y_score, sample_weight=None):\n    # Sort scores and corresponding truth values\n    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n    y_score = y_score[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n    if sample_weight is not None:\n        sample_weight = sample_weight[desc_score_indices]\n    else:\n        sample_weight = np.ones_like(y_true)\n\n    # Calculate precision and recall\n    distinct_value_indices = np.where(np.diff(y_score))[0]\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n    tps = np.cumsum(y_true * sample_weight)[threshold_idxs]\n    fps = np.cumsum((1 - y_true) * sample_weight)[threshold_idxs]\n\n    precisions = tps / (tps + fps)\n    recalls = tps / tps[-1]\n\n    # Calculate average precision\n    average_precision = np.sum((recalls[1:] - recalls[:-1]) * precisions[:-1])\n    return average_precision\n```\n\n### Step 2: Implement the Main Function\n\nNext, we'll implement the `average_precision_score` function. This function will handle different types of input data and use the helper function for binary classification.\n\n```python\ndef average_precision_score(y_true, y_score, average='macro', pos_label=1, sample_weight=None):\n    y_true = np.asarray(y_true)\n    y_score = np.asarray(y_score)\n\n    if y_true.ndim == 1:\n        # Binary classification case\n        return _binary_uninterpolated_average_precision(y_true == pos_label, y_score, sample_weight)\n    elif y_true.ndim == 2:\n        # Multilabel-indicator case\n        n_classes = y_true.shape[1]\n        ap_scores = np.zeros(n_classes)\n        for i in range(n_classes):\n            ap_scores[i] = _binary_uninterpolated_average_precision(y_true[:, i], y_score[:, i], sample_weight)\n        \n        if average == 'macro':\n            return np.mean(ap_scores)\n        elif average == 'weighted':\n            weights = np.sum(y_true, axis=0)\n            return np.average(ap_scores, weights=weights)\n        elif average == 'samples':\n            return np.mean([_binary_uninterpolated_average_precision(y_true[i], y_score[i], sample_weight) for i in range(y_true.shape[0])])\n        elif average == 'micro':\n            y_true = y_true.ravel()\n            y_score = y_score.ravel()\n            return _binary_uninterpolated_average_precision(y_true, y_score, sample_weight)\n        else:\n            raise ValueError(\"average has to be one of ['micro', 'samples', 'weighted', 'macro']\")\n    else:\n        raise ValueError(\"y_true should be 1d or 2d array\")\n\n# Example usage\ny_true = [0, 0, 1, 1]\ny_scores = [0.1, 0.4, 0.35, 0.8]\nprint(\"Average Precision Score (binary):\", average_precision_score(y_true, y_scores))\n\ny_true_multilabel = [[0, 1], [1, 0], [1, 1], [0, 0]]\ny_scores_multilabel = [[0.5, 0.2], [0.3, 0.8], [0.9, 0.4], [0.1, 0.6]]\nprint(\"Average Precision Score (multilabel, macro):\", average_precision_score(y_true_multilabel, y_scores_multilabel, average='macro'))\n```\n\n### Explanation\n\n- **Helper Function**: `_binary_uninterpolated_average_precision` calculates the average precision for binary classification by sorting the scores, calculating precision and recall, and then computing the average precision.\n- **Main Function**: `average_precision_score` handles different input types (binary, multilabel) and averaging methods (`macro`, `weighted`, `samples`, `micro`).\n- **Example Usage**: Demonstrates how to use the function with binary and multilabel data.\n\nThis implementation should cover the requirements for calculating the average precision score for different types of input data.",
    "prompt": "Generate a Python function called average_precision_score that computes the average precision (AP) from prediction scores. The function takes in the following parameters:\n\n- y_true: array-like of shape (n_samples,) or (n_samples, n_classes) representing true binary labels or binary label indicators.\n- y_score: array-like of shape (n_samples,) or (n_samples, n_classes) representing target scores.\n- average: {'micro', 'samples', 'weighted', 'macro'} or None, default='macro' determining the type of averaging performed on the data.\n- pos_label: int, float, bool, or str, default=1 representing the label of the positive class.\n- sample_weight: array-like of shape (n_samples,), default=None representing sample weights.\n\nThe function returns the average precision score as a float value. It also includes a helper function called _binary_uninterpolated_average_precision to calculate the average precision for binary classification. The function handles different types of input data such as binary, multilabel-indicator, and multiclass.\n\nAdditionally, the function provides examples of how to use it with input arrays for y_true and y_score.",
    "function_name": "average_precision_score",
    "ground_truth": "sklearn/metrics/_ranking.py",
    "test": "sklearn/metrics/tests/test_ranking.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}