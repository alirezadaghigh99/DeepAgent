{
    "result": "To implement the `ndcg_score` function, we need to follow these steps:\n\n1. **Calculate DCG (Discounted Cumulative Gain):** For each sample, sort the true scores based on the predicted scores, apply a logarithmic discount, and sum them up.\n\n2. **Calculate IDCG (Ideal Discounted Cumulative Gain):** For each sample, sort the true scores in descending order (ideal ranking), apply the same logarithmic discount, and sum them up.\n\n3. **Compute NDCG (Normalized DCG):** Divide the DCG by the IDCG for each sample to get a score between 0 and 1.\n\n4. **Handle optional parameters:** Consider the top `k` scores if specified, apply sample weights if provided, and handle ties in scores if `ignore_ties` is set to `False`.\n\nHere's the implementation of the `ndcg_score` function:\n\n```python\nimport numpy as np\n\ndef dcg_score(y_true, y_score, k=None, ignore_ties=False):\n    order = np.argsort(y_score)[::-1]\n    y_true_sorted = np.take_along_axis(y_true, order, axis=0)\n    \n    if k is not None:\n        y_true_sorted = y_true_sorted[:k]\n    \n    gains = y_true_sorted\n    discounts = np.log2(np.arange(len(y_true_sorted)) + 2)\n    \n    return np.sum(gains / discounts)\n\ndef ndcg_score(y_true, y_score, k=None, sample_weight=None, ignore_ties=False):\n    y_true = np.asarray(y_true)\n    y_score = np.asarray(y_score)\n    \n    if sample_weight is None:\n        sample_weight = np.ones(y_true.shape[0])\n    else:\n        sample_weight = np.asarray(sample_weight)\n    \n    scores = []\n    for i in range(y_true.shape[0]):\n        actual_dcg = dcg_score(y_true[i], y_score[i], k, ignore_ties)\n        ideal_dcg = dcg_score(y_true[i], y_true[i], k, ignore_ties)\n        \n        if ideal_dcg == 0:\n            score = 0\n        else:\n            score = actual_dcg / ideal_dcg\n        \n        scores.append(score * sample_weight[i])\n    \n    return np.sum(scores) / np.sum(sample_weight)\n\n# Example usage:\ny_true = np.array([[3, 2, 3, 0, 1, 2]])\ny_score = np.array([[0.1, 0.4, 0.35, 0.8, 0.7, 0.2]])\nprint(ndcg_score(y_true, y_score, k=5))\n```\n\n### Explanation:\n- **DCG Calculation:** The `dcg_score` function sorts the true scores based on the predicted scores, applies a logarithmic discount, and sums them up.\n- **NDCG Calculation:** The `ndcg_score` function calculates the DCG for both the predicted and ideal rankings, then normalizes the DCG by dividing by the IDCG.\n- **Handling Parameters:** The function considers the top `k` scores if specified, applies sample weights, and handles ties based on the `ignore_ties` parameter.\n\nThis implementation should work for multilabel classification tasks and provide a normalized score between 0 and 1.",
    "prompt": "Generate a Python function called ndcg_score that Compute Normalized Discounted Cumulative Gain.\n\n    Sum the true scores ranked in the order induced by the predicted scores,\n    after applying a logarithmic discount. Then divide by the best possible\n    score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n    0 and 1.\n\n    This ranking metric returns a high value if true labels are ranked high by\n    ``y_score``.\n\n    Parameters\n    ----------\n    y_true : array-like of shape (n_samples, n_labels)\n        True targets of multilabel classification, or true scores of entities\n        to be ranked. Negative values in `y_true` may result in an output\n        that is not between 0 and 1.\n\n    y_score : array-like of shape (n_samples, n_labels)\n        Target scores, can either be probability estimates, confidence values,\n        or non-thresholded measure of decisions (as returned by\n        \"decision_function\" on some classifiers).\n\n    k : int, default=None\n        Only consider the highest k scores in the ranking. If `None`, use all\n        outputs.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If `None`, all samples are given the same weight.\n\n    ignore_ties : bool, default=False\n        Assume that there are no ties in y_score (which is likely to be the\n        case if y_score is continuous) for efficiency gains.\n\n    Returns\n    -------\n    normalized_discounted_cumulative_gain : float in [0., 1.]\n        The averaged NDCG scores for all samples.",
    "function_name": "ndcg_score",
    "ground_truth": "sklearn/metrics/_ranking.py",
    "test": "sklearn/metrics/tests/test_ranking.py:test_ndcg_toy_examples",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "",
    "data": "Table"
}