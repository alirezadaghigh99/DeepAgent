{
    "result": "To implement the `binary_focal_loss_with_logits` function, we need to follow these steps:\n\n1. **Convert logits to probabilities**: Use the sigmoid function to convert logits to probabilities.\n2. **Compute the focal loss**: Use the formula provided to compute the focal loss for each element.\n3. **Apply class weights and positive weights**: If provided, apply these weights to the computed loss.\n4. **Apply reduction**: Depending on the `reduction` parameter, compute the final loss value.\n\nHere's the implementation:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\ndef binary_focal_loss_with_logits(pred, target, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None, weight=None):\n    \"\"\"\n    Compute the binary focal loss between `pred` and the ground truth `target`.\n\n    Args:\n        pred (Tensor): Logits tensor with shape (N, C, *) where C = number of classes.\n        target (Tensor): Labels tensor with the same shape as pred (N, C, *).\n        alpha (float): Weighting factor alpha in [0, 1].\n        gamma (float): Focusing parameter gamma >= 0.\n        reduction (str): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n        pos_weight (Tensor, optional): A weight of positive examples with shape (num_of_classes,).\n        weight (Tensor, optional): Weights for classes with shape (num_of_classes,).\n\n    Returns:\n        Tensor: The computed loss.\n    \"\"\"\n    # Convert logits to probabilities\n    prob = torch.sigmoid(pred)\n    \n    # Compute the focal loss components\n    p_t = prob * target + (1 - prob) * (1 - target)\n    alpha_t = alpha * target + (1 - alpha) * (1 - target)\n    \n    # Compute the focal loss\n    focal_loss = -alpha_t * (1 - p_t) ** gamma * torch.log(p_t + 1e-12)\n    \n    # Apply class weights if provided\n    if weight is not None:\n        focal_loss = focal_loss * weight.unsqueeze(0).unsqueeze(-1)\n    \n    # Apply positive weights if provided\n    if pos_weight is not None:\n        focal_loss = focal_loss * (pos_weight * target + (1 - target))\n    \n    # Apply reduction\n    if reduction == 'mean':\n        return focal_loss.mean()\n    elif reduction == 'sum':\n        return focal_loss.sum()\n    else:\n        return focal_loss\n\n# Example usage\nif __name__ == \"__main__\":\n    C = 3  # num_classes\n    pred = torch.randn(1, C, 5, requires_grad=True)\n    target = torch.randint(2, (1, C, 5))\n    kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n    output = binary_focal_loss_with_logits(pred, target, **kwargs)\n    output.backward()\n```\n\n### Explanation:\n\n- **Sigmoid Function**: Converts logits to probabilities.\n- **Focal Loss Calculation**: Uses the formula provided, with `alpha_t` and `p_t` calculated based on the target.\n- **Weights**: If `weight` or `pos_weight` is provided, they are applied to the loss.\n- **Reduction**: The loss is reduced according to the specified method (`'none'`, `'mean'`, or `'sum'`).\n\nThis function should work for binary classification tasks with multiple classes, where each class is treated independently.",
    "prompt": "Generate a Python function called binary_focal_loss_with_logits for Criterion that computes Binary Focal loss.\n\nFocal loss is computed as follows:\n\n    .. math::\n\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n\n    where:\n       - :math:`p_t` is the model's estimated probability for each class.\n\n    Args:\n        pred: logits tensor with shape :math:`(N, C, *)` where C = number of classes.\n        target: labels tensor with the same shape as pred :math:`(N, C, *)`\n          where each value is between 0 and 1.\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.\n        reduction: Specifies the reduction to apply to the\n          output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n          will be applied, ``'mean'``: the sum of the output will be divided by\n          the number of elements in the output, ``'sum'``: the output will be\n          summed.\n        pos_weight: a weight of positive examples with shape :math:`(num\\_of\\_classes,)`.\n          It is possible to trade off recall and precision by adding weights to positive examples.\n        weight: weights for classes with shape :math:`(num\\_of\\_classes,)`.\n\n    Returns:\n        the computed loss.\n\n    Examples:\n        >>> C = 3  # num_classes\n        >>> pred = torch.randn(1, C, 5, requires_grad=True)\n        >>> target = torch.randint(2, (1, C, 5))\n        >>> kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n        >>> output = binary_focal_loss_with_logits(pred, target, **kwargs)\n        >>> output.backward()",
    "function_name": "binary_focal_loss_with_logits",
    "ground_truth": "kornia/losses/focal.py",
    "test": "tests/losses/test_focal_loss.py::TestBinaryFocalLossWithLogits",
    "class": "",
    "stage": "Model Construction",
    "task": "Classification",
    "data": "Image"
}