{
    "result": "To implement the `sample_hidden_units` function for the `RBM` class, we need to calculate the activation probabilities of the hidden units given the visible units and then sample the hidden units based on these probabilities. The process involves using the weights, biases, and dropout regularization. We'll use TensorFlow for tensor operations and random sampling.\n\nHere's how you can implement the `sample_hidden_units` function:\n\n```python\nimport tensorflow as tf\n\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = tf.Variable(tf.random.normal([visible_units, hidden_units], stddev=init_stdv), name='weights')\n        self.bh = tf.Variable(tf.zeros([hidden_units]), name='hidden_biases')\n        self.keep = keep_prob  # dropout keep probability\n        self.seed = seed\n        tf.random.set_seed(seed)\n\n    def binomial_sampling(self, pr):\n        # Outputs a tensor of the same shape as pr, where each element is 1 if the corresponding\n        # probability is greater than a sampled uniform random value, and 0 otherwise.\n        random_tensor = tf.random.uniform(tf.shape(pr), seed=self.seed)\n        return tf.cast(pr > random_tensor, dtype=tf.float32)\n\n    def sample_hidden_units(self, vv):\n        # Compute the activation probabilities of the hidden units\n        pre_activation = tf.matmul(vv, self.w) + self.bh\n        phv = tf.nn.sigmoid(pre_activation)\n\n        # Apply dropout regularization\n        phv_dropout = phv * self.keep\n\n        # Sample the hidden units\n        h_ = self.binomial_sampling(phv_dropout)\n\n        return phv, h_\n\n# Example usage:\n# rbm = RBM(possible_ratings=5, visible_units=784)\n# visible_units_tensor = tf.random.normal([100, 784])  # Example visible units\n# phv, h_ = rbm.sample_hidden_units(visible_units_tensor)\n```\n\n### Explanation:\n\n1. **Weights and Biases Initialization**: \n   - `self.w` is initialized as a random normal tensor with a standard deviation of `init_stdv`.\n   - `self.bh` is initialized as a zero tensor for the hidden biases.\n\n2. **Activation Probabilities (`phv`)**:\n   - Compute the pre-activation by multiplying the visible units `vv` with the weights `self.w` and adding the hidden biases `self.bh`.\n   - Apply the sigmoid function to get the activation probabilities `phv`.\n\n3. **Dropout Regularization**:\n   - Apply dropout by multiplying the activation probabilities `phv` with the dropout keep probability `self.keep`.\n\n4. **Sampling Hidden Units (`h_`)**:\n   - Use the `binomial_sampling` method to sample the hidden units based on the dropout-adjusted activation probabilities `phv_dropout`.\n\nThis function returns the activation probabilities and the sampled hidden units, which can be used in further computations within the RBM.",
    "prompt": "Generate a Python function `sample_hidden_units` for the class `RBM` that samples hidden units based on the visible units (`vv`). This function should return two tensors: the activation probability of the hidden units (`phv`) and the sampled hidden units (`h_`). The sampling process involves using the weights (`self.w`), biases (`self.bh`), and dropout regularization (`self.keep`). The function relies on `binomial_sampling`, which outputs a tensor where each element is `1` if the corresponding activation probability exceeds a randomly sampled threshold, and `0` otherwise.You should use tf for this.\n\nClass structure:\n\n```python\nclass RBM:\n    def __init__(self, possible_ratings, visible_units, hidden_units=500, keep_prob=0.7, init_stdv=0.1, learning_rate=0.004, minibatch_size=100, training_epoch=20, display_epoch=10, sampling_protocol=[50, 70, 80, 90, 100], debug=False, with_metrics=False, seed=42):\n        self.w = None  # weights between visible and hidden units\n        self.bh = None  # biases of the hidden units\n        self.keep = keep_prob  # dropout keep probability\n        pass\n\n    def binomial_sampling(self, pr):\n        # Outputs a tensor of the same shape as pr, where each element is 1 if the corresponding\n        # probability is greater than a sampled uniform random value, and 0 otherwise.\n        pass\n\n    def sample_hidden_units(self, vv):\n        # Implement the function as described in the prompt",
    "function_name": "sample_hidden_units",
    "ground_truth": "recommenders/models/rbm/rbm.py",
    "test": "tests/unit/recommenders/models/test_rbm.py",
    "class": "RBM",
    "stage": "",
    "task": "Recommendation",
    "data": "Inference"
}