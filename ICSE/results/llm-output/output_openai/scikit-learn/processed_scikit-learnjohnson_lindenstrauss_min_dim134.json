{
    "result": "The Johnson-Lindenstrauss lemma is a result in mathematics that provides a way to reduce the dimensionality of data while approximately preserving the pairwise distances between points. This is particularly useful in machine learning and data science for dimensionality reduction techniques like random projection.\n\nThe lemma states that for any \\(0 < \\epsilon < 1\\) and a set of \\(n\\) points in high-dimensional space, there exists a mapping to a lower-dimensional space of dimension \\(k\\) such that the distances between the points are preserved within a factor of \\(1 \\pm \\epsilon\\). The minimum dimension \\(k\\) can be calculated using the formula:\n\n\\[ k \\geq \\frac{4 \\ln(n)}{\\epsilon^2 / 2 - \\epsilon^3 / 3} \\]\n\nHere's a Python function that implements this calculation:\n\n```python\nimport numpy as np\n\ndef johnson_lindenstrauss_min_dim(n_samples, eps):\n    \"\"\"\n    Calculate the minimum number of components required for a random projection\n    to ensure an eps-embedding with good probability, according to the\n    Johnson-Lindenstrauss lemma.\n\n    Parameters:\n    n_samples (int or array-like): The number of samples.\n    eps (float or array-like): The maximum distortion rate (0 < eps < 1).\n\n    Returns:\n    int or np.ndarray: The minimal number of components required.\n    \"\"\"\n    n_samples = np.atleast_1d(n_samples)\n    eps = np.atleast_1d(eps)\n\n    if np.any(eps <= 0) or np.any(eps >= 1):\n        raise ValueError(\"eps must be between 0 and 1 (exclusive).\")\n\n    # Calculate the minimum number of components\n    min_dim = (4 * np.log(n_samples)) / (eps**2 / 2 - eps**3 / 3)\n    \n    return np.ceil(min_dim).astype(int)\n\n# Example usage:\n# For a single number of samples and distortion rate\nn_samples = 1000\neps = 0.1\nmin_components = johnson_lindenstrauss_min_dim(n_samples, eps)\nprint(f\"Minimum components for {n_samples} samples with eps={eps}: {min_components}\")\n\n# For multiple numbers of samples and a single distortion rate\nn_samples_array = [100, 1000, 10000]\neps = 0.1\nmin_components_array = johnson_lindenstrauss_min_dim(n_samples_array, eps)\nprint(f\"Minimum components for {n_samples_array} samples with eps={eps}: {min_components_array}\")\n\n# For a single number of samples and multiple distortion rates\nn_samples = 1000\neps_array = [0.1, 0.2, 0.3]\nmin_components_array = johnson_lindenstrauss_min_dim(n_samples, eps_array)\nprint(f\"Minimum components for {n_samples} samples with eps={eps_array}: {min_components_array}\")\n```\n\n### Explanation:\n- The function `johnson_lindenstrauss_min_dim` takes `n_samples` and `eps` as inputs, which can be either single values or arrays.\n- It checks that `eps` is within the valid range (0, 1).\n- It calculates the minimum number of dimensions required using the formula derived from the Johnson-Lindenstrauss lemma.\n- The result is rounded up to the nearest integer using `np.ceil` to ensure that the number of dimensions is sufficient.\n- The function can handle both scalar and array inputs, making it versatile for different use cases.",
    "prompt": "Generate a Python function called johnson_lindenstrauss_min_dim that calculates the minimum number of components needed to guarantee a distortion rate within a specified range for a random projection. The function takes in the number of samples (n_samples) as an integer or array-like input and the maximum distortion rate (eps) as a float or array-like input. It returns the minimal number of components required to ensure an eps-embedding with good probability for the given number of samples. The function also includes references to the Johnson-Lindenstrauss lemma and provides examples of usage.",
    "function_name": "johnson_lindenstrauss_min_dim",
    "ground_truth": "sklearn/random_projection.py",
    "test": "sklearn/tests/test_random_projection.py",
    "class": "",
    "stage": "Pre-Post processing",
    "task": "",
    "data": "Table"
}