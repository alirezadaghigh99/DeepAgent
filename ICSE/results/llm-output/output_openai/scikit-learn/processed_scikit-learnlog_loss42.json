{
    "result": "Here's a Python function called `log_loss` that calculates the log loss, also known as logistic loss or cross-entropy loss. This function includes parameters for ground truth labels, predicted probabilities, normalization, sample weights, and optional class labels. Additionally, I've included examples, notes, and references for further information.\n\n```python\nimport numpy as np\n\ndef log_loss(y_true, y_pred, normalize=True, sample_weight=None, labels=None):\n    \"\"\"\n    Calculate the log loss, also known as logistic loss or cross-entropy loss.\n\n    Parameters:\n    - y_true: array-like of shape (n_samples,) - Ground truth labels for n_samples samples.\n    - y_pred: array-like of shape (n_samples, n_classes) - Predicted probabilities from a classifier's predict_proba method.\n    - normalize: bool, default=True - Whether to return the mean loss per sample or the sum of per-sample losses.\n    - sample_weight: array-like of shape (n_samples,), default=None - Optional sample weights.\n    - labels: array-like, default=None - Optional labels for the classes.\n\n    Returns:\n    - log_loss: float - The calculated log loss.\n\n    Examples:\n    >>> y_true = [0, 1, 1]\n    >>> y_pred = [[0.9, 0.1], [0.2, 0.8], [0.3, 0.7]]\n    >>> log_loss(y_true, y_pred)\n    0.1738073366910675\n\n    Notes:\n    - Log loss is a performance metric for evaluating the predictions of classifiers that output probability values.\n    - It is particularly useful for models that predict probabilities of class membership.\n\n    References:\n    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html\n    - https://en.wikipedia.org/wiki/Cross_entropy\n    \"\"\"\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n\n    if labels is not None:\n        labels = np.array(labels)\n        y_true = np.searchsorted(labels, y_true)\n\n    if sample_weight is None:\n        sample_weight = np.ones(y_true.shape[0])\n    else:\n        sample_weight = np.array(sample_weight)\n\n    # Clip y_pred to prevent log(0)\n    eps = np.finfo(float).eps\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n\n    # Calculate log loss\n    n_samples = y_true.shape[0]\n    n_classes = y_pred.shape[1]\n\n    # Create a one-hot encoded matrix of y_true\n    y_true_one_hot = np.zeros((n_samples, n_classes))\n    y_true_one_hot[np.arange(n_samples), y_true] = 1\n\n    # Calculate the log loss\n    loss = -np.sum(y_true_one_hot * np.log(y_pred) * sample_weight[:, np.newaxis])\n\n    if normalize:\n        return loss / np.sum(sample_weight)\n    else:\n        return loss\n\n# Example usage\nif __name__ == \"__main__\":\n    y_true = [0, 1, 1]\n    y_pred = [[0.9, 0.1], [0.2, 0.8], [0.3, 0.7]]\n    print(log_loss(y_true, y_pred))  # Output: 0.1738073366910675\n```\n\n### Notes:\n- The function uses `np.clip` to ensure that the predicted probabilities are within a valid range to avoid taking the logarithm of zero.\n- The function supports optional sample weights, which can be useful if some samples are more important than others.\n- The `labels` parameter allows for specifying the class labels explicitly, which can be useful if the classes are not zero-indexed or are not in a contiguous range.\n\n### References:\n- [Scikit-learn Log Loss Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)\n- [Wikipedia: Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy)",
    "prompt": "Generate a Python function called log_loss that calculates the log loss, also known as logistic loss or cross-entropy loss. The function takes in the following parameters:\n\n- y_true: Ground truth labels for n_samples samples.\n- y_pred: Predicted probabilities from a classifier's predict_proba method.\n- normalize: A boolean indicating whether to return the mean loss per sample or the sum of per-sample losses.\n- sample_weight: Optional sample weights.\n- labels: Optional labels for the classes.\n\nThe function calculates the log loss based on the predicted probabilities and true labels. It returns the log loss as a float value. The function also includes examples, notes, and references for further information.",
    "function_name": "log_loss",
    "ground_truth": "sklearn/metrics/_classification.py",
    "test": "sklearn/metrics/tests/test_classification.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Classification",
    "data": "Table"
}