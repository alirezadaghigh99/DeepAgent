{
    "result": "To implement the `average_precision_score` function, we need to handle different types of input data, such as binary, multilabel-indicator, and multiclass. We'll also implement a helper function `_binary_uninterpolated_average_precision` to calculate the average precision for binary classification. Here's how you can implement this in Python:\n\n```python\nimport numpy as np\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.utils import check_array, check_consistent_length\nfrom sklearn.utils.multiclass import type_of_target\n\ndef _binary_uninterpolated_average_precision(y_true, y_score, sample_weight=None):\n    # Sort scores and corresponding truth values\n    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n    y_score = y_score[desc_score_indices]\n    y_true = y_true[desc_score_indices]\n    if sample_weight is not None:\n        sample_weight = sample_weight[desc_score_indices]\n    else:\n        sample_weight = np.ones_like(y_true)\n\n    # Accumulate true positives and false positives\n    distinct_value_indices = np.where(np.diff(y_score))[0]\n    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n\n    tps = np.cumsum(y_true * sample_weight)[threshold_idxs]\n    fps = np.cumsum((1 - y_true) * sample_weight)[threshold_idxs]\n\n    # Calculate precision and recall\n    precision = tps / (tps + fps)\n    recall = tps / tps[-1]\n\n    # Calculate average precision\n    average_precision = np.sum(np.diff(np.r_[0, recall]) * precision)\n    return average_precision\n\ndef average_precision_score(y_true, y_score, average='macro', pos_label=1, sample_weight=None):\n    y_true = check_array(y_true, ensure_2d=False)\n    y_score = check_array(y_score, ensure_2d=False)\n    check_consistent_length(y_true, y_score, sample_weight)\n\n    y_type = type_of_target(y_true)\n    if y_type == \"binary\":\n        return _binary_uninterpolated_average_precision(y_true == pos_label, y_score, sample_weight)\n    elif y_type == \"multiclass\":\n        y_true = label_binarize(y_true, classes=np.unique(y_true))\n        return average_precision_score(y_true, y_score, average=average, pos_label=pos_label, sample_weight=sample_weight)\n    elif y_type == \"multilabel-indicator\":\n        n_classes = y_true.shape[1]\n        ap_scores = []\n        for i in range(n_classes):\n            ap = _binary_uninterpolated_average_precision(y_true[:, i], y_score[:, i], sample_weight)\n            ap_scores.append(ap)\n        if average == 'macro':\n            return np.mean(ap_scores)\n        elif average == 'weighted':\n            weights = np.sum(y_true, axis=0)\n            return np.average(ap_scores, weights=weights)\n        elif average == 'samples':\n            return np.mean([_binary_uninterpolated_average_precision(y_true[i], y_score[i], sample_weight) for i in range(y_true.shape[0])])\n        elif average == 'micro':\n            y_true = y_true.ravel()\n            y_score = y_score.ravel()\n            return _binary_uninterpolated_average_precision(y_true, y_score, sample_weight)\n        else:\n            raise ValueError(\"average has to be one of ['micro', 'samples', 'weighted', 'macro']\")\n    else:\n        raise ValueError(f\"Unsupported target type: {y_type}\")\n\n# Example usage\ny_true = np.array([0, 1, 1, 0])\ny_score = np.array([0.1, 0.4, 0.35, 0.8])\nprint(\"Average Precision Score (binary):\", average_precision_score(y_true, y_score))\n\ny_true_multiclass = np.array([0, 1, 2, 2])\ny_score_multiclass = np.array([[0.1, 0.9, 0.0], [0.2, 0.8, 0.0], [0.0, 0.1, 0.9], [0.0, 0.2, 0.8]])\nprint(\"Average Precision Score (multiclass):\", average_precision_score(y_true_multiclass, y_score_multiclass, average='macro'))\n```\n\n### Explanation:\n- **_binary_uninterpolated_average_precision**: This helper function calculates the average precision for binary classification by sorting the scores, calculating true positives and false positives, and then computing precision and recall.\n- **average_precision_score**: This function handles different types of input data (binary, multiclass, multilabel-indicator) and computes the average precision score based on the specified averaging method.\n- **Example Usage**: The examples demonstrate how to use the function with binary and multiclass data.",
    "prompt": "Generate a Python function called average_precision_score that computes the average precision (AP) from prediction scores. The function takes in the following parameters:\n\n- y_true: array-like of shape (n_samples,) or (n_samples, n_classes) representing true binary labels or binary label indicators.\n- y_score: array-like of shape (n_samples,) or (n_samples, n_classes) representing target scores.\n- average: {'micro', 'samples', 'weighted', 'macro'} or None, default='macro' determining the type of averaging performed on the data.\n- pos_label: int, float, bool, or str, default=1 representing the label of the positive class.\n- sample_weight: array-like of shape (n_samples,), default=None representing sample weights.\n\nThe function returns the average precision score as a float value. It also includes a helper function called _binary_uninterpolated_average_precision to calculate the average precision for binary classification. The function handles different types of input data such as binary, multilabel-indicator, and multiclass.\n\nAdditionally, the function provides examples of how to use it with input arrays for y_true and y_score.",
    "function_name": "average_precision_score",
    "ground_truth": "sklearn/metrics/_ranking.py",
    "test": "sklearn/metrics/tests/test_ranking.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}