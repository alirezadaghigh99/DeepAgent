{
    "result": "To create the `depth_to_3d` function, we need to transform depth values into 3D coordinates using the camera intrinsics. The camera intrinsics matrix is used to map pixel coordinates to camera coordinates. The function will also include error handling to ensure the inputs are of the correct type and shape. Here's how you can implement this function:\n\n```python\nimport torch\n\ndef depth_to_3d(depth_tensor, camera_matrix, normalize_points=False):\n    # Check input types\n    if not isinstance(depth_tensor, torch.Tensor):\n        raise TypeError(\"depth_tensor must be a torch.Tensor\")\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise TypeError(\"camera_matrix must be a torch.Tensor\")\n    \n    # Check input shapes\n    if depth_tensor.ndim != 4 or depth_tensor.shape[1] != 1:\n        raise ValueError(\"depth_tensor must have shape (B, 1, H, W)\")\n    if camera_matrix.ndim != 3 or camera_matrix.shape[1:] != (3, 3):\n        raise ValueError(\"camera_matrix must have shape (B, 3, 3)\")\n    \n    B, _, H, W = depth_tensor.shape\n    \n    # Create a mesh grid of pixel coordinates\n    y, x = torch.meshgrid(torch.arange(H, device=depth_tensor.device), \n                          torch.arange(W, device=depth_tensor.device), \n                          indexing='ij')\n    x = x.expand(B, -1, -1)\n    y = y.expand(B, -1, -1)\n    \n    # Flatten the grid\n    x_flat = x.reshape(B, -1)\n    y_flat = y.reshape(B, -1)\n    \n    # Get depth values and flatten them\n    depth_flat = depth_tensor.reshape(B, -1)\n    \n    # Get camera intrinsics\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n    \n    # Compute 3D points\n    X = (x_flat - cx.unsqueeze(1)) * depth_flat / fx.unsqueeze(1)\n    Y = (y_flat - cy.unsqueeze(1)) * depth_flat / fy.unsqueeze(1)\n    Z = depth_flat\n    \n    # Stack to get 3D points\n    points_3d = torch.stack((X, Y, Z), dim=1)\n    \n    # Reshape to (B, 3, H, W)\n    points_3d = points_3d.reshape(B, 3, H, W)\n    \n    if normalize_points:\n        # Normalize the 3D points\n        norm = torch.norm(points_3d, dim=1, keepdim=True)\n        points_3d = points_3d / (norm + 1e-8)  # Add epsilon to avoid division by zero\n    \n    return points_3d\n\n# Example usage:\n# depth_tensor = torch.rand((2, 1, 480, 640))\n# camera_matrix = torch.rand((2, 3, 3))\n# points_3d = depth_to_3d(depth_tensor, camera_matrix, normalize_points=True)\n```\n\n### Explanation:\n- **Input Validation**: The function checks if the inputs are `torch.Tensor` and have the correct shapes.\n- **Meshgrid Creation**: A meshgrid of pixel coordinates is created for the height and width of the image.\n- **Depth and Intrinsics**: The depth values are flattened, and the camera intrinsics are extracted.\n- **3D Point Calculation**: Using the camera intrinsics, the 3D coordinates are calculated for each pixel.\n- **Normalization**: If `normalize_points` is `True`, the 3D points are normalized to unit vectors.\n- **Output**: The function returns a tensor of shape `(B, 3, H, W)` representing the 3D points for each pixel.",
    "prompt": "Generate a Python function called depth_to_3d that takes in a tensor representing depth values per pixel, a tensor containing camera intrinsics, and a boolean flag to normalize points. The function computes a 3D point per pixel based on the depth value and camera intrinsics. The input depth tensor should have a shape of (B, 1, H, W) and the camera_matrix tensor should have a shape of (B, 3, 3). The function returns a tensor with a 3D point per pixel of the same resolution as the input, with a shape of (B, 3, H, W). The function also includes error handling for input types and shapes.",
    "function_name": "depth_to_3d",
    "ground_truth": "kornia/geometry/depth.py",
    "test": "tests/geometry/test_depth.py::TestDepthTo3d::test_shapes_broadcast",
    "class": "",
    "stage": "Model Construction",
    "task": "",
    "data": "Image"
}