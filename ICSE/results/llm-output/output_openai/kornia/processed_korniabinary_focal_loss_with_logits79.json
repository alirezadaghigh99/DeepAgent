{
    "result": "To implement the `binary_focal_loss_with_logits` function, we need to compute the focal loss using the provided formula. The function will take into account the logits, target labels, and additional parameters such as `alpha`, `gamma`, `reduction`, `pos_weight`, and `weight`. Here's how you can implement this function in Python using PyTorch:\n\n```python\nimport torch\nimport torch.nn.functional as F\n\ndef binary_focal_loss_with_logits(pred, target, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None, weight=None):\n    \"\"\"\n    Compute the binary focal loss with logits.\n\n    Args:\n        pred (Tensor): Logits tensor with shape (N, C, *) where C = number of classes.\n        target (Tensor): Labels tensor with the same shape as pred (N, C, *) where each value is between 0 and 1.\n        alpha (float): Weighting factor alpha in [0, 1].\n        gamma (float): Focusing parameter gamma >= 0.\n        reduction (str): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n        pos_weight (Tensor, optional): A weight of positive examples with shape (num_of_classes,).\n        weight (Tensor, optional): Weights for classes with shape (num_of_classes,).\n\n    Returns:\n        Tensor: The computed loss.\n    \"\"\"\n    # Compute the sigmoid of the predictions\n    p = torch.sigmoid(pred)\n    \n    # Compute the focal loss components\n    ce_loss = F.binary_cross_entropy_with_logits(pred, target, reduction='none', pos_weight=pos_weight)\n    p_t = p * target + (1 - p) * (1 - target)  # p_t is the model's estimated probability for each class\n    focal_loss = alpha * (1 - p_t) ** gamma * ce_loss\n\n    # Apply class weights if provided\n    if weight is not None:\n        focal_loss = focal_loss * weight\n\n    # Apply the specified reduction\n    if reduction == 'mean':\n        return focal_loss.mean()\n    elif reduction == 'sum':\n        return focal_loss.sum()\n    else:\n        return focal_loss\n\n# Example usage\nif __name__ == \"__main__\":\n    C = 3  # num_classes\n    pred = torch.randn(1, C, 5, requires_grad=True)\n    target = torch.randint(2, (1, C, 5)).float()\n    kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n    output = binary_focal_loss_with_logits(pred, target, **kwargs)\n    output.backward()\n```\n\n### Explanation:\n- **Sigmoid Activation**: We apply the sigmoid function to the logits to get the probabilities.\n- **Cross-Entropy Loss**: We use `binary_cross_entropy_with_logits` to compute the base cross-entropy loss.\n- **Focal Loss Calculation**: We adjust the cross-entropy loss with the focal loss formula, incorporating the `alpha` and `gamma` parameters.\n- **Class Weights**: If class weights are provided, they are applied to the focal loss.\n- **Reduction**: The function supports three types of reduction: `'none'`, `'mean'`, and `'sum'`.\n\nThis implementation should work for binary classification tasks with multiple classes, where each class is treated independently.",
    "prompt": "Generate a Python function called binary_focal_loss_with_logits for Criterion that computes Binary Focal loss.\n\nFocal loss is computed as follows:\n\n    .. math::\n\n        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n\n    where:\n       - :math:`p_t` is the model's estimated probability for each class.\n\n    Args:\n        pred: logits tensor with shape :math:`(N, C, *)` where C = number of classes.\n        target: labels tensor with the same shape as pred :math:`(N, C, *)`\n          where each value is between 0 and 1.\n        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n        gamma: Focusing parameter :math:`\\gamma >= 0`.\n        reduction: Specifies the reduction to apply to the\n          output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n          will be applied, ``'mean'``: the sum of the output will be divided by\n          the number of elements in the output, ``'sum'``: the output will be\n          summed.\n        pos_weight: a weight of positive examples with shape :math:`(num\\_of\\_classes,)`.\n          It is possible to trade off recall and precision by adding weights to positive examples.\n        weight: weights for classes with shape :math:`(num\\_of\\_classes,)`.\n\n    Returns:\n        the computed loss.\n\n    Examples:\n        >>> C = 3  # num_classes\n        >>> pred = torch.randn(1, C, 5, requires_grad=True)\n        >>> target = torch.randint(2, (1, C, 5))\n        >>> kwargs = {\"alpha\": 0.25, \"gamma\": 2.0, \"reduction\": 'mean'}\n        >>> output = binary_focal_loss_with_logits(pred, target, **kwargs)\n        >>> output.backward()",
    "function_name": "binary_focal_loss_with_logits",
    "ground_truth": "kornia/losses/focal.py",
    "test": "tests/losses/test_focal_loss.py::TestBinaryFocalLossWithLogits",
    "class": "",
    "stage": "Model Construction",
    "task": "Classification",
    "data": "Image"
}