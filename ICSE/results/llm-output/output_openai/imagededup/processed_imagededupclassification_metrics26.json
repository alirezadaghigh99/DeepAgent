{
    "result": "To calculate precision, recall, and F1 score for each class, we need to compare the ground truth dictionary with the retrieved dictionary. Here's a Python function that performs these calculations:\n\n```python\ndef calculate_metrics(ground_truth, retrieved):\n    def calculate_class_metrics(tp, fp, fn):\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        return precision, recall, f1_score\n\n    # Initialize counts for true positives, false positives, and false negatives\n    tp_1, fp_1, fn_1 = 0, 0, 0  # For class 1 (duplicates)\n    tp_0, fp_0, fn_0 = 0, 0, 0  # For class 0 (non-duplicates)\n\n    # Iterate over each file in the ground truth\n    for file, true_duplicates in ground_truth.items():\n        retrieved_duplicates = retrieved.get(file, [])\n\n        # Calculate true positives, false positives, and false negatives for class 1\n        true_positives = set(true_duplicates) & set(retrieved_duplicates)\n        false_positives = set(retrieved_duplicates) - set(true_duplicates)\n        false_negatives = set(true_duplicates) - set(retrieved_duplicates)\n\n        tp_1 += len(true_positives)\n        fp_1 += len(false_positives)\n        fn_1 += len(false_negatives)\n\n        # Calculate true negatives and false negatives for class 0\n        all_files = set(ground_truth.keys()) | set(retrieved.keys())\n        non_duplicates = all_files - set(true_duplicates) - {file}\n        retrieved_non_duplicates = all_files - set(retrieved_duplicates) - {file}\n\n        true_negatives = non_duplicates & retrieved_non_duplicates\n        false_negatives_0 = non_duplicates - retrieved_non_duplicates\n        false_positives_0 = retrieved_non_duplicates - non_duplicates\n\n        tp_0 += len(true_negatives)\n        fn_0 += len(false_negatives_0)\n        fp_0 += len(false_positives_0)\n\n    # Calculate metrics for class 1\n    precision_1, recall_1, f1_score_1 = calculate_class_metrics(tp_1, fp_1, fn_1)\n\n    # Calculate metrics for class 0\n    precision_0, recall_0, f1_score_0 = calculate_class_metrics(tp_0, fp_0, fn_0)\n\n    return {\n        'class_1': {\n            'precision': precision_1,\n            'recall': recall_1,\n            'f1_score': f1_score_1\n        },\n        'class_0': {\n            'precision': precision_0,\n            'recall': recall_0,\n            'f1_score': f1_score_0\n        }\n    }\n\n# Example usage:\nground_truth = {\n    'file1': ['file2', 'file3'],\n    'file4': ['file5']\n}\n\nretrieved = {\n    'file1': ['file2'],\n    'file4': ['file5', 'file6']\n}\n\nmetrics = calculate_metrics(ground_truth, retrieved)\nprint(metrics)\n```\n\nThis function calculates the precision, recall, and F1 score for both classes (duplicates and non-duplicates) by comparing the ground truth and retrieved dictionaries. It handles the calculation of true positives, false positives, and false negatives for each class and then computes the metrics accordingly.",
    "prompt": "Generate a python function that Given ground truth dictionary and retrieved dictionary, return per class precision, recall and f1 score. Class 1 is\n    assigned to duplicate file pairs while class 0 is for non-duplicate file pairs.\n\n    Args:\n        ground_truth: A dictionary representing ground truth with filenames as key and a list of duplicate filenames\n        as value.\n        retrieved: A dictionary representing retrieved duplicates with filenames as key and a list of retrieved\n        duplicate filenames as value.\n\n    Returns:\n        Dictionary of precision, recall and f1 score for both classes.",
    "function_name": "classification_metrics",
    "ground_truth": "imagededup/handlers/metrics/classification.py#L69",
    "test": "tests/test_evaluator.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Classification",
    "data": "image"
}