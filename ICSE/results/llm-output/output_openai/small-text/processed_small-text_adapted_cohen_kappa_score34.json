{
    "result": "To create a Python function called `_adapted_cohen_kappa_score` that extends Cohen's kappa by handling the special case of perfect agreement and preventing a division by zero error, we can start by understanding the standard implementation of Cohen's kappa. Cohen's kappa is a statistic that measures inter-annotator agreement for categorical items. It is generally thought to be a more robust measure than simple percent agreement calculation, as it takes into account the agreement occurring by chance.\n\nHere's how you can implement the `_adapted_cohen_kappa_score` function:\n\n```python\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\ndef _adapted_cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None):\n    # Check if y1 and y2 are of the same length\n    if len(y1) != len(y2):\n        raise ValueError(\"The length of y1 and y2 must be the same.\")\n\n    # Compute the confusion matrix\n    cm = confusion_matrix(y1, y2, labels=labels, sample_weight=sample_weight)\n    \n    # Number of classes\n    n_classes = cm.shape[0]\n    \n    # Observed agreement\n    observed_agreement = np.trace(cm)\n    \n    # Expected agreement\n    sum0 = np.sum(cm, axis=0)\n    sum1 = np.sum(cm, axis=1)\n    expected_agreement = np.dot(sum0, sum1) / np.sum(cm)\n    \n    # Total number of samples\n    total_samples = np.sum(cm)\n    \n    # Handle the special case of perfect agreement\n    if observed_agreement == total_samples:\n        return 1.0\n    \n    # Calculate kappa\n    kappa = (observed_agreement - expected_agreement) / (total_samples - expected_agreement)\n    \n    return kappa\n\n# Example usage\ny1 = [0, 1, 2, 2, 0]\ny2 = [0, 2, 2, 2, 0]\nprint(_adapted_cohen_kappa_score(y1, y2))\n```\n\n### Explanation:\n\n1. **Input Validation**: The function first checks if the input lists `y1` and `y2` are of the same length. If not, it raises a `ValueError`.\n\n2. **Confusion Matrix**: It computes the confusion matrix using `sklearn.metrics.confusion_matrix`, which is essential for calculating observed and expected agreements.\n\n3. **Observed Agreement**: This is simply the trace of the confusion matrix, which represents the number of times the annotators agreed.\n\n4. **Expected Agreement**: This is calculated based on the marginal sums of the confusion matrix, representing the agreement expected by chance.\n\n5. **Perfect Agreement Handling**: If the observed agreement equals the total number of samples, it means there is perfect agreement, and the function returns `1.0` directly to avoid division by zero.\n\n6. **Kappa Calculation**: Finally, it calculates the kappa score using the formula, ensuring that the denominator is not zero due to the perfect agreement check.\n\nThis function should handle the special case of perfect agreement gracefully and prevent division by zero errors.",
    "prompt": "Create a Python function called _adapted_cohen_kappa_score that extends Cohen's kappa by handling the special case of perfect agreement, preventing a division by zero error. \ndef _adapted_cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None):",
    "function_name": "_adapted_cohen_kappa_score",
    "ground_truth": "small_text/stopping_criteria/kappa.py",
    "test": "tests/unit/small_text/stopping_criteria/test_kappa.py::TestAdaptedCohenKappScore",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}