{
    "result": "To implement the `_subtract_confident_thresholds` function, we need to follow these steps:\n\n1. **Input Validation**: Ensure that either labels or pre-calculated thresholds are provided. If neither is provided, raise a `ValueError`.\n\n2. **Calculate or Use Provided Thresholds**: If thresholds are not provided, calculate them using the `get_confident_thresholds` method. This method should determine class-specific thresholds based on the labels and predicted probabilities.\n\n3. **Adjust Probabilities**: Subtract the class-specific thresholds from the predicted probabilities. Ensure that no negative values result from this subtraction by shifting the probabilities.\n\n4. **Re-normalize Probabilities**: After adjusting the probabilities, re-normalize them so that they sum to 1 for each instance.\n\n5. **Return Adjusted Probabilities**: Return the adjusted and re-normalized probabilities as a NumPy array.\n\nHere's how you can implement this function:\n\n```python\nimport numpy as np\n\ndef get_confident_thresholds(labels, predicted_probs):\n    # Placeholder for the actual implementation of threshold calculation\n    # This should return a list or array of thresholds for each class\n    # For simplicity, let's assume it returns an array of zeros\n    num_classes = predicted_probs.shape[1]\n    return np.zeros(num_classes)\n\ndef _subtract_confident_thresholds(labels=None, predicted_probs=None, multi_label=False, thresholds=None):\n    if thresholds is None:\n        if labels is None or predicted_probs is None:\n            raise ValueError(\"Either labels and predicted_probs or pre-calculated thresholds must be provided.\")\n        thresholds = get_confident_thresholds(labels, predicted_probs)\n    \n    # Subtract thresholds from predicted probabilities\n    adjusted_probs = predicted_probs - thresholds\n    \n    # Ensure no negative values by shifting\n    adjusted_probs = np.maximum(adjusted_probs, 0)\n    \n    # Re-normalize probabilities\n    if multi_label:\n        # For multi-label, normalize each instance separately\n        row_sums = adjusted_probs.sum(axis=1, keepdims=True)\n        # Avoid division by zero\n        row_sums[row_sums == 0] = 1\n        adjusted_probs = adjusted_probs / row_sums\n    else:\n        # For single-label, normalize across classes\n        adjusted_probs = adjusted_probs / adjusted_probs.sum(axis=1, keepdims=True)\n    \n    return adjusted_probs\n\n# Example usage:\n# labels = np.array([...])  # Your labels here\n# predicted_probs = np.array([...])  # Your predicted probabilities here\n# adjusted_probs = _subtract_confident_thresholds(labels=labels, predicted_probs=predicted_probs)\n```\n\n### Key Points:\n- **Threshold Calculation**: The `get_confident_thresholds` function is a placeholder and should be implemented to calculate meaningful thresholds based on the data.\n- **Normalization**: The normalization step ensures that the adjusted probabilities remain valid probability distributions.\n- **Multi-label vs Single-label**: The function handles both multi-label and single-label scenarios, adjusting the normalization process accordingly.",
    "prompt": "You need to implement a function named `_subtract_confident_thresholds` that adjusts predicted probabilities by subtracting class-specific confidence thresholds and then re-normalizing the probabilities. This adjustment aims to handle class imbalance in classification tasks. The function accepts labels, predicted probabilities, an optional flag for multi-label settings, and pre-calculated confidence thresholds. If confidence thresholds are not provided, they will be calculated from the labels and predicted probabilities using the `get_confident_thresholds` method. After subtracting the thresholds, the function ensures no negative values by shifting and then re-normalizing the probabilities. The function returns the adjusted predicted probabilities as a NumPy array. If neither labels nor pre-calculated thresholds are provided, a `ValueError` is raised.",
    "function_name": "_subtract_confident_thresholds",
    "ground_truth": "cleanlab/internal/label_quality_utils.py#L26",
    "test": "tests/test_rank.py",
    "class": "",
    "stage": "Inference",
    "task": "",
    "data": "Table"
}