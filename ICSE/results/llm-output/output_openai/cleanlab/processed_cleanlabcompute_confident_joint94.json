{
    "result": "To implement the `compute_confident_joint` function, we need to calculate the confident joint matrix, which represents the counts of examples for which we are confident about their given and true labels. This involves comparing the predicted probabilities against specified thresholds to determine confidence. Here's a vectorized implementation of the function:\n\n```python\nimport numpy as np\n\ndef compute_confident_joint(labels, pred_probs, thresholds=None, calibrate=False, multi_label=False, return_indices_of_off_diagonals=False):\n    \"\"\"\n    Estimates the confident counts of latent true vs observed noisy labels.\n\n    Parameters:\n    - labels: An array or list of class labels for each example in the dataset.\n    - pred_probs: An array of model-predicted class probabilities for each example in the dataset.\n    - thresholds: An optional array of per-class threshold probabilities.\n    - calibrate: A boolean flag indicating whether to calibrate the confident joint estimate.\n    - multi_label: An optional boolean flag indicating if the dataset is multi-label classification.\n    - return_indices_of_off_diagonals: An optional boolean flag indicating whether to return indices of examples counted in off-diagonals of the confident joint.\n\n    Returns:\n    - A numpy array representing counts of examples for which we are confident about their given and true label.\n    \"\"\"\n    labels = np.array(labels)\n    pred_probs = np.array(pred_probs)\n    \n    if thresholds is None:\n        thresholds = np.ones(pred_probs.shape[1]) * 0.5  # Default threshold of 0.5 for binary/multi-class\n\n    num_classes = pred_probs.shape[1]\n    confident_joint = np.zeros((num_classes, num_classes), dtype=int)\n    off_diagonal_indices = []\n\n    if multi_label:\n        # For multi-label, we need a one-vs-rest approach\n        for i in range(num_classes):\n            confident_indices = pred_probs[:, i] >= thresholds[i]\n            for j in range(num_classes):\n                if i != j:\n                    confident_joint[i, j] = np.sum((labels[:, j] == 1) & confident_indices)\n                    if return_indices_of_off_diagonals:\n                        off_diagonal_indices.extend(np.where((labels[:, j] == 1) & confident_indices)[0])\n    else:\n        # For single-label, we compare predicted class with true class\n        predicted_labels = np.argmax(pred_probs, axis=1)\n        for i in range(num_classes):\n            confident_indices = pred_probs[:, i] >= thresholds[i]\n            for j in range(num_classes):\n                confident_joint[i, j] = np.sum((labels == j) & (predicted_labels == i) & confident_indices)\n                if return_indices_of_off_diagonals and i != j:\n                    off_diagonal_indices.extend(np.where((labels == j) & (predicted_labels == i) & confident_indices)[0])\n\n    if calibrate:\n        # Placeholder for calibration logic, if needed\n        pass\n\n    if return_indices_of_off_diagonals:\n        return confident_joint, np.array(off_diagonal_indices)\n    else:\n        return confident_joint\n\n# Example usage:\n# labels = [0, 1, 2, 1, 0]\n# pred_probs = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.2, 0.7], [0.3, 0.6, 0.1], [0.9, 0.05, 0.05]]\n# thresholds = [0.5, 0.5, 0.5]\n# compute_confident_joint(labels, pred_probs, thresholds)\n```\n\n### Key Points:\n- **Thresholds**: If not provided, a default threshold of 0.5 is used for each class.\n- **Multi-label vs Single-label**: The function handles both multi-label and single-label classification scenarios.\n- **Calibration**: A placeholder for calibration logic is included, but the actual calibration process would depend on specific requirements.\n- **Off-diagonal Indices**: If requested, the function returns indices of examples that contribute to off-diagonal entries in the confident joint matrix, which can be useful for identifying misclassified examples.",
    "prompt": "Generate a Python function called compute_confident_joint that estimates the confident counts of latent true vs observed noisy labels for the examples in a dataset. The function takes in the following parameters:\n\n- labels: An array or list of class labels for each example in the dataset.\n- pred_probs: An array of model-predicted class probabilities for each example in the dataset.\n- thresholds: An optional array of per-class threshold probabilities.\n- calibrate: A boolean flag indicating whether to calibrate the confident joint estimate.\n- multi_label: An optional boolean flag indicating if the dataset is multi-label classification.\n- return_indices_of_off_diagonals: An optional boolean flag indicating whether to return indices of examples counted in off-diagonals of the confident joint.\n\nThe function returns an array representing counts of examples for which we are confident about their given and true label. If multi_label is True, the array has a different shape and represents a one-vs-rest format for the confident joint.\n\nNote that the function provides a vectorized implementation of the confident joint calculation for efficiency.",
    "function_name": "compute_confident_joint",
    "ground_truth": "cleanlab/count.py",
    "test": "tests/test_filter_count.py",
    "class": "",
    "stage": "Model Construction",
    "task": "Classification",
    "data": ""
}