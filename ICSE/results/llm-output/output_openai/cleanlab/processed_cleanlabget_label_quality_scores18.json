{
    "result": "To create the `get_label_quality_scores` function, we need to implement a method to calculate label quality scores for a regression dataset. The function will take in two arrays: `labels` and `predictions`, and an optional `method` argument to specify the scoring method. We'll implement a simple scoring method called \"outre\" as the default.\n\nThe \"outre\" method will calculate the label quality score based on the absolute error between the predicted and actual labels. The score will be inversely proportional to the error, normalized to a range between 0 and 1.\n\nHere's how you can implement this function:\n\n```python\nimport numpy as np\n\ndef get_label_quality_scores(labels, predictions, method=\"outre\"):\n    # Ensure inputs are numpy arrays\n    labels = np.asarray(labels)\n    predictions = np.asarray(predictions)\n    \n    # Validate inputs\n    if labels.shape != predictions.shape:\n        raise ValueError(\"The shape of labels and predictions must be the same.\")\n    \n    if method == \"outre\":\n        # Calculate absolute errors\n        errors = np.abs(labels - predictions)\n        \n        # Normalize errors to a range between 0 and 1\n        max_error = np.max(errors)\n        if max_error == 0:\n            # If max_error is 0, all predictions are perfect\n            return np.ones_like(errors)\n        \n        normalized_errors = errors / max_error\n        \n        # Calculate quality scores as 1 - normalized error\n        quality_scores = 1 - normalized_errors\n        \n        return quality_scores\n    else:\n        raise ValueError(f\"Unknown method: {method}\")\n\n# Example usage\nlabels = np.array([1, 2, 3, 4])\npredictions = np.array([2, 2, 5, 4.1])\n\nlabel_quality_scores = get_label_quality_scores(labels, predictions)\nprint(label_quality_scores)\n```\n\n### Explanation:\n- **Input Validation**: The function first checks if the `labels` and `predictions` arrays have the same shape. If not, it raises a `ValueError`.\n- **Error Calculation**: It calculates the absolute error between each pair of label and prediction.\n- **Normalization**: The errors are normalized by dividing by the maximum error to ensure they are between 0 and 1.\n- **Score Calculation**: The quality score for each label is calculated as `1 - normalized_error`, where a lower error results in a higher score.\n- **Output**: The function returns an array of quality scores, with each score corresponding to a label-prediction pair.\n\nThis implementation provides a simple way to assess the quality of labels based on prediction errors, with the flexibility to extend or modify the scoring method as needed.",
    "prompt": "Create a Python function called get_label_quality_scores that calculates label quality scores for each example in a regression dataset. The function takes in two array-like inputs: labels (raw labels from the original dataset) and predictions (predicted labels for each example). Additionally, the function has an optional keyword argument method, which specifies the scoring method to use (default is \"outre\").\n\nThe function returns an array of label quality scores, where each score is a continuous value between 0 and 1. A score of 1 indicates a clean label (likely correct), while a score of 0 indicates a dirty label (likely incorrect).\n\nEnsure that the inputs are valid and then use the specified scoring method to calculate the label quality scores. The output is an array of scores with one score per example in the dataset.\n\nExample usage:\n```python\nimport numpy as np\nfrom cleanlab.regression.rank import get_label_quality_scores\n\nlabels = np.array([1, 2, 3, 4])\npredictions = np.array([2, 2, 5, 4.1])\n\nlabel_quality_scores = get_label_quality_scores(labels, predictions)\nprint(label_quality_scores)\n# Output: array([0.00323821, 0.33692597, 0.00191686, 0.33692597])\n```",
    "function_name": "get_label_quality_scores",
    "ground_truth": "cleanlab/regression/rank.py",
    "test": "tests/test_object_detection.py",
    "class": "",
    "stage": "Evaluating&Metrics",
    "task": "Regression",
    "data": "Table"
}