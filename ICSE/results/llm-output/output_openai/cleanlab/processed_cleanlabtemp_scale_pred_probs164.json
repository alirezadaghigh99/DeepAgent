{
    "result": "To implement the `temp_scale_pred_probs` function, we need to adjust the predicted probabilities using temperature scaling. This involves a few key steps:\n\n1. **Clipping Probabilities**: To avoid issues with logarithms of zero, we clip the probabilities to a small positive value.\n\n2. **Normalization**: Ensure that the probabilities sum to 1 for each set of predictions.\n\n3. **Temperature Scaling**: Apply the temperature scaling using the softmax function. This involves taking the logarithm of the probabilities, dividing by the temperature, and then applying the exponential function followed by normalization.\n\nHere's how you can implement this:\n\n```python\nimport numpy as np\n\ndef temp_scale_pred_probs(pred_probs, temp):\n    # Clip probabilities to avoid log(0)\n    epsilon = 1e-12\n    pred_probs = np.clip(pred_probs, epsilon, 1.0)\n    \n    # Normalize probabilities to ensure they sum to 1\n    pred_probs /= np.sum(pred_probs, axis=1, keepdims=True)\n    \n    # Apply temperature scaling\n    log_probs = np.log(pred_probs)\n    scaled_log_probs = log_probs / temp\n    \n    # Apply softmax to get scaled probabilities\n    exp_scaled_log_probs = np.exp(scaled_log_probs)\n    scaled_probs = exp_scaled_log_probs / np.sum(exp_scaled_log_probs, axis=1, keepdims=True)\n    \n    return scaled_probs\n\n# Example usage:\n# pred_probs = np.array([[0.2, 0.3, 0.5], [0.1, 0.1, 0.8]])\n# temp = 0.5\n# scaled_probs = temp_scale_pred_probs(pred_probs, temp)\n# print(scaled_probs)\n```\n\n### Explanation:\n\n- **Clipping**: We use `np.clip` to ensure that all probabilities are at least `epsilon` (a very small number) to prevent taking the logarithm of zero.\n\n- **Normalization**: We divide each row by its sum to ensure that the probabilities in each row sum to 1.\n\n- **Logarithm and Temperature Scaling**: We take the logarithm of the probabilities, divide by the temperature, and then exponentiate. This is the core of the temperature scaling process.\n\n- **Softmax**: After exponentiating the scaled log probabilities, we normalize them again to ensure they sum to 1, effectively applying the softmax function.\n\nThis function will adjust the confidence of the predictions based on the temperature parameter, making them sharper or smoother as required.",
    "prompt": "You need to implement a function named `temp_scale_pred_probs` that adjusts predicted probabilities using a temperature scaling technique. Temperature factor less than 1 sharpens the predictions, making them more confident, while a temperature greater than 1 smoothens the predictions, making them less confident. The function takes a 2D NumPy array of predicted probabilities (`pred_probs`) and a float (`temp`) representing the temperature factor. The probabilities are first clipped to avoid logarithm of zero and normalized. Then, the temperature scaling is applied using the softmax function after taking the logarithm of the probabilities. The function returns the scaled and normalized probabilities as a 2D NumPy array.",
    "function_name": "temp_scale_pred_probs",
    "ground_truth": "cleanlab/internal/multiannotator_utils.py#L353",
    "test": "tests/test_multiannotator.py",
    "class": "",
    "stage": "Pre-Post processing",
    "task": "Classification",
    "data": "Table"
}